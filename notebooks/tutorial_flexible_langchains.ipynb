{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_thoughtsource_100() got an unexpected keyword argument 'generate_mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/robertpraas/Desktop/ThoughtSource/notebooks/tutorial_flexible_langchains.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robertpraas/Desktop/ThoughtSource/notebooks/tutorial_flexible_langchains.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m\"\"\"Example dataset with one sample for two datasets\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robertpraas/Desktop/ThoughtSource/notebooks/tutorial_flexible_langchains.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcot\u001b[39;00m \u001b[39mimport\u001b[39;00m Collection\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/robertpraas/Desktop/ThoughtSource/notebooks/tutorial_flexible_langchains.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m coll \u001b[39m=\u001b[39m Collection\u001b[39m.\u001b[39;49mload_thoughtsource_100(names\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mworldtree\u001b[39;49m\u001b[39m'\u001b[39;49m],load_pregenerated_cots\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,generate_mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrecache\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39m#random_sample=False?\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robertpraas/Desktop/ThoughtSource/notebooks/tutorial_flexible_langchains.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m coll \u001b[39m=\u001b[39m coll\u001b[39m.\u001b[39mselect(split\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m, number_samples\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robertpraas/Desktop/ThoughtSource/notebooks/tutorial_flexible_langchains.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m coll\u001b[39m.\u001b[39mselect_generated_cots(cot_trigger \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mkojima-01\u001b[39m\u001b[39m\"\u001b[39m, api_service\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcohere\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: load_thoughtsource_100() got an unexpected keyword argument 'generate_mode'"
     ]
    }
   ],
   "source": [
    "\"\"\"Example dataset with one sample for two datasets\"\"\"\n",
    "\n",
    "from cot import Collection\n",
    "coll = Collection.load_thoughtsource_100(names=['worldtree'],load_pregenerated_cots=True,generate_mode=\"recache\") #random_sample=False?\n",
    "coll = coll.select(split=\"all\", number_samples=1)\n",
    "coll.select_generated_cots(cot_trigger = \"kojima-01\", api_service='cohere') #have one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading aqua...\n",
      "Downloading and preparing dataset aqua_dataset/thoughtsource to /Users/robertpraas/.cache/huggingface/datasets/aqua_dataset/thoughtsource/1.0.0/1e513577b1b5ccfcf97069f9f660ccdc6ebf4987495b5fc95402f12cd9c83a99...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8252d265514eb4ba92c34126a41efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43259b543834e2e859ed4eefeb060c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/15.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28622ae63ebd44a8b0d95af2bdcb3186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/43.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47575c81d1d644458c5938f5fc716da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/45.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce305dc1385442baa5248f820d7e08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605c39f6b6ed4da384aae76ed94cdcd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb2f21c8dfa4fb2916965c20a0cd72f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2f14bfbc85240ab826944ba4d8be550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset aqua_dataset downloaded and prepared to /Users/robertpraas/.cache/huggingface/datasets/aqua_dataset/thoughtsource/1.0.0/1e513577b1b5ccfcf97069f9f660ccdc6ebf4987495b5fc95402f12cd9c83a99. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f7888159304a669ba96685fb055c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading asdiv...\n",
      "Downloading and preparing dataset asdiv_dataset/thoughtsource to /Users/robertpraas/.cache/huggingface/datasets/asdiv_dataset/thoughtsource/1.0.0/8d75df8ce5c6294c738da1b417c2493b2cc632e11b202a0823c93971fb8101fb...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e8c98a97e144cd84a0a36a391402ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66288ba57deb416d8fe515695239786b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/178k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83890390c574c68ba66cc553c0f7e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/601 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c40cb4164168455cb9fe412f56016ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab54a9308e41465582e50fbe58c2ca30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/577 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1dc1053dbd54e0fbb7307d9ec299733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/568 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6cbf3e48a8740a298f8d5bc68dc1fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/631 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c29541811114df4af2e6d44e9f770f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7011d036cbe44a595e92b84a9d2b2e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset asdiv_dataset downloaded and prepared to /Users/robertpraas/.cache/huggingface/datasets/asdiv_dataset/thoughtsource/1.0.0/8d75df8ce5c6294c738da1b417c2493b2cc632e11b202a0823c93971fb8101fb. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5685ccfda4584063ac8a44a5c4882ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading commonsense_qa...\n",
      "Downloading and preparing dataset commonsense_qa_dataset/thoughtsource to /Users/robertpraas/.cache/huggingface/datasets/commonsense_qa_dataset/thoughtsource/1.0.0/cd427a3573ffb55b035f31bc28bbe62e3eec600ea9bd3186f90bee36862c029a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "595deb64a0d94f9481f342b10a1d83dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb5139a3a79419a8a603f3375617f15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568c18b456fa45bebbec7432f106132f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 kojima cots mapped.\n",
      "0 wei cots mapped.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e518ad4690984cf9bd41423d0ea1d6d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 kojima cots mapped.\n",
      "0 wei cots mapped.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5f64530b5e4c42ac498edbfcc4a84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1217 kojima cots mapped.\n",
      "1220 wei cots mapped.\n",
      "Dataset commonsense_qa_dataset downloaded and prepared to /Users/robertpraas/.cache/huggingface/datasets/commonsense_qa_dataset/thoughtsource/1.0.0/cd427a3573ffb55b035f31bc28bbe62e3eec600ea9bd3186f90bee36862c029a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc93cc04186a4858b6bf464dfa0009e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading entailment_bank...\n",
      "Downloading and preparing dataset entailment_bank_dataset/thoughtsource to /Users/robertpraas/.cache/huggingface/datasets/entailment_bank_dataset/thoughtsource/1.0.0/484622edfc1dec21806b69ef91c8b8b8a83f70ea20ba98bd4ca89caa73566656...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d36a4b68c44e868d19374998afc107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/7.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1388f1e39ff4fa7922f4a35f02e6e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49fe275598de4228bec0f5289e98b9cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537793b44d3643af979695782956748b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset entailment_bank_dataset downloaded and prepared to /Users/robertpraas/.cache/huggingface/datasets/entailment_bank_dataset/thoughtsource/1.0.0/484622edfc1dec21806b69ef91c8b8b8a83f70ea20ba98bd4ca89caa73566656. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de6846bbb5242549426732e498329af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gsm8k...\n",
      "Downloading and preparing dataset gsm8k_dataset/thoughtsource to /Users/robertpraas/.cache/huggingface/datasets/gsm8k_dataset/thoughtsource/1.0.0/75d8b11f06245c6b0e586227209915eefbbd845907359e859c4d09107045f1ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6626490f769240ac883f52cee860c9af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d51175d35c4d8d8f127a28fcc33a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d61d4b963ed48f68fbe8bf5d9eaefbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/242k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba96f7b189534cd695bcad3c10c7c085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52243940b1ee4a34a22452a0083d49d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c2148e89d24438ca8fa9f42d89f0799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset gsm8k_dataset downloaded and prepared to /Users/robertpraas/.cache/huggingface/datasets/gsm8k_dataset/thoughtsource/1.0.0/75d8b11f06245c6b0e586227209915eefbbd845907359e859c4d09107045f1ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d619d9c3f4ce44339c925a999d642e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mawps...\n",
      "Downloading and preparing dataset mawps_dataset/thoughtsource to /Users/robertpraas/.cache/huggingface/datasets/mawps_dataset/thoughtsource/1.0.0/c1f05fb89b5a20a4d6b7129e7692a12abdc1ebf17f623d0598bd80d48d00ff77...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c90558e3b7a445a86287c55ab024fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504ddf3efd20453aaf2aa6a0a0a86a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/109k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de6db7060814751be8ccbbe5ebdd8c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/27.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f217c31bc9c8479ca756989c3b79a40a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14422c1b7a24a6dab641852883ab8c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset mawps_dataset downloaded and prepared to /Users/robertpraas/.cache/huggingface/datasets/mawps_dataset/thoughtsource/1.0.0/c1f05fb89b5a20a4d6b7129e7692a12abdc1ebf17f623d0598bd80d48d00ff77. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea19bfd184e4b0e94c788be78ede085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading med_qa...\n",
      "Downloading and preparing dataset med_qa_dataset/thoughtsource to /Users/robertpraas/.cache/huggingface/datasets/med_qa_dataset/thoughtsource/1.0.0/80feaf4e24940034debbd30deb9eeac25df0c6d1dfe2d8fb0cb497c9f549a35c...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a9fe0d0410447fac54d6af314230cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f84cc73147749b1bafc0c735b58ca8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ffe6d500e64028be7fba954ab27782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e5cb904d3e54dc783efc1a5e76e80e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing Lievin CoTs: 100%|██████████| 6365/6365 [00:01<00:00, 3288.51it/s]\n",
      "Preparing Lievin CoTs v1: 100%|██████████| 1273/1273 [00:01<00:00, 888.72it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046fc30ec56249f28874523069eb626c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset med_qa_dataset downloaded and prepared to /Users/robertpraas/.cache/huggingface/datasets/med_qa_dataset/thoughtsource/1.0.0/80feaf4e24940034debbd30deb9eeac25df0c6d1dfe2d8fb0cb497c9f549a35c. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c875dabf8034548a599db9fbdbe8a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading med_qa_open...\n",
      "Downloading and preparing dataset med_qa_open_dataset/thoughtsource to /Users/robertpraas/.cache/huggingface/datasets/med_qa_open_dataset/thoughtsource/1.0.0/180844ba7fec4f04cdf5594bf707ecaf3b8f809799e1a3d5adcd42a2eded9d15...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd03d606d069447bbbd046dddf4b422f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb37e1c221e44968737bed7d9a863eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/450k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5085fc80744a45b7b5139e8ba769acd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/463k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68875ecc97314149975be35d8ff17b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8587355e9df04cd4950092ac4d7c4fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4573d15700134543bde9bc5b2f2abd72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ab1af3b4b1439d8d0049e9934c4e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c397a1c43e42b2a9dd579c4ddd9a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset med_qa_open_dataset downloaded and prepared to /Users/robertpraas/.cache/huggingface/datasets/med_qa_open_dataset/thoughtsource/1.0.0/180844ba7fec4f04cdf5594bf707ecaf3b8f809799e1a3d5adcd42a2eded9d15. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e4aed9ddc9462095a8e489d91ef1d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading medmc_qa...\n",
      "Downloading and preparing dataset med_mcqa_dataset/thoughtsource to /Users/robertpraas/.cache/huggingface/datasets/med_mcqa_dataset/thoughtsource/1.0.0/c1ebfc6e86f8317b0891f26c2fec24ec6f3bfc269c44d7d3f0cb1d77553a925b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe6f4253e00439085e588c8a79edf85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b1021edded42cea94ab814bc673e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07dc8c5f6a0e42b698ba4c1a24ab1da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb3a79ba9ab4e48957036efad1dd923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022258546c364cfd87806991a84c6162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing Lievin CoTs: 100%|██████████| 5000/5000 [00:01<00:00, 2705.49it/s]\n",
      "Preparing Lievin CoTs v1: 100%|██████████| 1000/1000 [00:01<00:00, 910.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset med_mcqa_dataset downloaded and prepared to /Users/robertpraas/.cache/huggingface/datasets/med_mcqa_dataset/thoughtsource/1.0.0/c1ebfc6e86f8317b0891f26c2fec24ec6f3bfc269c44d7d3f0cb1d77553a925b. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3e5cdfbe6a40b3afe636c50fea53b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading _init_...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/robertpraas/Desktop/ThoughtSource/notebooks/tutorial_flexible_langchains.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/robertpraas/Desktop/ThoughtSource/notebooks/tutorial_flexible_langchains.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test \u001b[39m=\u001b[39m Collection(\u001b[39m\"\u001b[39;49m\u001b[39mall\u001b[39;49m\u001b[39m\"\u001b[39;49m, generate_mode \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mrecache\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/ThoughtSource/libs/cot/cot/dataloader.py:103\u001b[0m, in \u001b[0;36mCollection.__init__\u001b[0;34m(self, names, verbose, generate_mode, source, load_pregenerated_cots)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache \u001b[39m=\u001b[39m {}\n\u001b[1;32m    102\u001b[0m \u001b[39mif\u001b[39;00m names \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_datasets()\n\u001b[1;32m    104\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(names, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_datasets(names)\n",
      "File \u001b[0;32m~/Desktop/ThoughtSource/libs/cot/cot/dataloader.py:191\u001b[0m, in \u001b[0;36mCollection.load_datasets\u001b[0;34m(self, names)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoading \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    190\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n\u001b[0;32m--> 191\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache[name] \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39;49mload_dataset(\n\u001b[1;32m    192\u001b[0m         \u001b[39mstr\u001b[39;49m(script), name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msource\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_source \u001b[39melse\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39mthoughtsource\u001b[39;49m\u001b[39m\"\u001b[39;49m, download_mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload_mode\n\u001b[1;32m    193\u001b[0m     )\n\u001b[1;32m    194\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[39mwith\u001b[39;00m suppress_stdout_stderr():\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/load.py:1664\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[1;32m   1661\u001b[0m ignore_verifications \u001b[39m=\u001b[39m ignore_verifications \u001b[39mor\u001b[39;00m save_infos\n\u001b[1;32m   1663\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1664\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   1665\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m   1666\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1667\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1668\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1669\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1670\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1671\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1672\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1673\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1674\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1675\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   1676\u001b[0m )\n\u001b[1;32m   1678\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1679\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/load.py:1516\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(error_msg)\n\u001b[1;32m   1515\u001b[0m \u001b[39m# Instantiate the dataset builder\u001b[39;00m\n\u001b[0;32m-> 1516\u001b[0m builder_instance: DatasetBuilder \u001b[39m=\u001b[39m builder_cls(\n\u001b[1;32m   1517\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1518\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1519\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1520\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1521\u001b[0m     \u001b[39mhash\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mhash\u001b[39;49m,\n\u001b[1;32m   1522\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1523\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1524\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbuilder_kwargs,\n\u001b[1;32m   1525\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   1526\u001b[0m )\n\u001b[1;32m   1528\u001b[0m \u001b[39mreturn\u001b[39;00m builder_instance\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "test = Collection(\"all\", generate_mode = 'recache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll = Collection(\"worldtree\", load_pregenerated_cots=True, generate_mode=\"recache\")\n",
    "coll = coll.select(split=\"all\", number_samples=1)\n",
    "coll.select_generated_cots(cot_trigger = \"kojima-01\", api_service='cohere') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'worldtree_test_788',\n",
       " 'ref_id': '',\n",
       " 'question': 'When light hits a mirror, most of the light is',\n",
       " 'type': 'multiplechoice',\n",
       " 'choices': ['refracted.', 'reflected.', 'absorbed.', 'transmitted.'],\n",
       " 'context': '',\n",
       " 'cot': ['A mirror reflects light.',\n",
       "  'Reflection is when a wave bounces off a surface and travels in the opposite direction relative to the angle of incidence.',\n",
       "  'Light is a kind of wave.',\n",
       "  'When light hits a reflective object , that light bounces off that object.',\n",
       "  'A mirror is a kind of reflective object.'],\n",
       " 'answer': ['reflected.'],\n",
       " 'generated_cot': [],\n",
       " 'feedback': []}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll['worldtree']['test'][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI \n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CoT Chain\"\"\"\n",
    "\n",
    "llm = ChatOpenAI(temperature=.0,model_name=\"gpt-3.5-turbo\") #ADA #for chat: gpt-3.5-turbo\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "template = \"\"\"{instruction}\n",
    "\n",
    "Question: {question}\n",
    "Answer_choices: {answer_choices}\n",
    "\n",
    "{cot_trigger}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(input_variables=[\"instruction\",\"question\",\"answer_choices\",\"cot_trigger\"], template=template)\n",
    "cot_chain = LLMChain(llm=llm, prompt=prompt_template,output_key=\"cot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"answer extraction chain\"\"\"\n",
    "\n",
    "extraction_template = \"\"\"{instruction}\n",
    "\n",
    "Question: {question}\n",
    "Answer_choices: {answer_choices}\n",
    "\n",
    "Cot: {cot_trigger}{cot}\n",
    "{answer_extraction}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(input_variables=[\"instruction\",\"question\",\"answer_choices\",\"cot_trigger\",\"cot\",\"answer_extraction\"], template=extraction_template)\n",
    "answer_chain = LLMChain(llm=llm, prompt=prompt_template,output_key=\"predicted_answer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CoT-Ans_extraction chain\"\"\"\n",
    "\n",
    "from langchain.chains import SequentialChain\n",
    "overall_chain = SequentialChain(chains=[cot_chain, answer_chain],\n",
    "                                input_variables=[\"instruction\",\"question\",\"answer_choices\",\"cot_trigger\",\"answer_extraction\"],\n",
    "                                output_variables=[\"cot\", \"predicted_answer\"],\n",
    "                                verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating worldtree...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Generating worldtree...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Generating worldtree...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Generate CoT with use of TS-schema\"\"\"\n",
    "#compare with config used before; what about max tokens?\n",
    "#config contains what the chain needs\n",
    "input_dict = {\n",
    "    \"instruction\": \"Be faithful and a little hopeful\",\n",
    "    \"cot_trigger\": \"Answer: Let's think step by step.\",\n",
    "    \"answer_extraction\": \"Therefore, among A through D, the answer is\",\n",
    "    'model':\"gpt-3.5-turbo\",\n",
    "    'temperature': 0,\n",
    "    'max_tokens': 800 \n",
    "}\n",
    "coll.generate_extract_flexible(chain=overall_chain,input_dict=input_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'f319c095-1dd9-477c-9050-459331cdd6a2',\n",
       " 'fragments_version': '0.01',\n",
       " 'instruction': 'Be faithful and a little hopeful',\n",
       " 'cot_trigger': \"Answer: Let's think step by step.\",\n",
       " 'cot_trigger_template': '',\n",
       " 'prompt_text': '',\n",
       " 'cot': 'When light hits a mirror, it bounces back off the surface of the mirror. This process is called reflection. Therefore, the correct answer is B) reflected.',\n",
       " 'answers': [{'id': '093d4eda-5ef4-48f7-8de9-51248d02a6e8',\n",
       "   'answer_extraction': 'Therefore, among A through D, the answer is',\n",
       "   'answer_extraction_template': '',\n",
       "   'answer_extraction_text': '',\n",
       "   'answer': 'B) reflected.',\n",
       "   'correct_answer': None}],\n",
       " 'author': '',\n",
       " 'date': '2023/05/07 10:30:53',\n",
       " 'api_service': '',\n",
       " 'model': \"{'name': 'gpt-3.5-turbo', 'temperature': 0, 'max_tokens': 800}\",\n",
       " 'comment': 'generated and extracted',\n",
       " 'annotations': []}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll['worldtree']['test'][0]['generated_cot'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_hard = Collection.from_json('/Users/robertpraas/Desktop/ThoughtSource/notebooks/internal_documentation/paper_2/ts_hard_v1.json')\n",
    "ts_hard.unload_datasets([\"med_qa\",\"medmc_qa\",\"commonsense_qa\",\"strategy_qa\",\"worldtree\"])\n",
    "#extract = ts_hard.extract_flexible(chain=answer_chain,input_dict=input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type Collection is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/robertpraas/Desktop/ThoughtSource/notebooks/tutorial_flexible_langchains.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/robertpraas/Desktop/ThoughtSource/notebooks/tutorial_flexible_langchains.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test \u001b[39m=\u001b[39m Collection\u001b[39m.\u001b[39;49mto_Collection(ts_hard,\u001b[39m\"\u001b[39;49m\u001b[39mworldtree\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mfile_test\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/ThoughtSource/libs/cot/cot/dataloader.py:593\u001b[0m, in \u001b[0;36mCollection.to_Collection\u001b[0;34m(chain_output, dataset_name, split, file_name)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39m#create and collect a json to make collection\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfile_name\u001b[39m}\u001b[39;00m\u001b[39m.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m outfile:\n\u001b[0;32m--> 593\u001b[0m     json\u001b[39m.\u001b[39;49mdump(ts_set, outfile)\n\u001b[1;32m    594\u001b[0m collect \u001b[39m=\u001b[39m Collection\u001b[39m.\u001b[39mfrom_json(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfile_name\u001b[39m}\u001b[39;00m\u001b[39m.json\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    596\u001b[0m \u001b[39mreturn\u001b[39;00m collect\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/json/__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m     iterable \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(skipkeys\u001b[39m=\u001b[39mskipkeys, ensure_ascii\u001b[39m=\u001b[39mensure_ascii,\n\u001b[1;32m    174\u001b[0m         check_circular\u001b[39m=\u001b[39mcheck_circular, allow_nan\u001b[39m=\u001b[39mallow_nan, indent\u001b[39m=\u001b[39mindent,\n\u001b[1;32m    175\u001b[0m         separators\u001b[39m=\u001b[39mseparators,\n\u001b[1;32m    176\u001b[0m         default\u001b[39m=\u001b[39mdefault, sort_keys\u001b[39m=\u001b[39msort_keys, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\u001b[39m.\u001b[39miterencode(obj)\n\u001b[1;32m    177\u001b[0m \u001b[39m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[39m# a debuggability cost\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m    180\u001b[0m     fp\u001b[39m.\u001b[39mwrite(chunk)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/json/encoder.py:431\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[39myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    430\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(o, \u001b[39mdict\u001b[39m):\n\u001b[0;32m--> 431\u001b[0m     \u001b[39myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[1;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     \u001b[39mif\u001b[39;00m markers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[39m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[39myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m newline_indent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[39m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[39myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m newline_indent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/json/encoder.py:438\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCircular reference detected\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    437\u001b[0m     markers[markerid] \u001b[39m=\u001b[39m o\n\u001b[0;32m--> 438\u001b[0m o \u001b[39m=\u001b[39m _default(o)\n\u001b[1;32m    439\u001b[0m \u001b[39myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[1;32m    440\u001b[0m \u001b[39mif\u001b[39;00m markers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault\u001b[39m(\u001b[39mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m     \u001b[39m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mObject of type \u001b[39m\u001b[39m{\u001b[39;00mo\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mis not JSON serializable\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type Collection is not JSON serializable"
     ]
    }
   ],
   "source": [
    "test = Collection.to_Collection(ts_hard,\"worldtree\",'test','file_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/robertpraas/Desktop/ThoughtSource/notebooks/tutorial_flexible_langchains.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/robertpraas/Desktop/ThoughtSource/notebooks/tutorial_flexible_langchains.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m coll\u001b[39m.\u001b[39;49mevaluate()\n",
      "File \u001b[0;32m~/Desktop/ThoughtSource/libs/cot/cot/dataloader.py:605\u001b[0m, in \u001b[0;36mCollection.evaluate\u001b[0;34m(self, name, split, overwrite, warn)\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache:\n\u001b[1;32m    603\u001b[0m         \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache[name]:\n\u001b[1;32m    604\u001b[0m             \u001b[39m# print(f\"Evaluating {name}...\")\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m             \u001b[39mself\u001b[39m[name][split], evaluation \u001b[39m=\u001b[39m evaluate(\u001b[39mself\u001b[39;49m[name][split], overwrite\u001b[39m=\u001b[39;49moverwrite, warn\u001b[39m=\u001b[39;49mwarn)\n\u001b[1;32m    606\u001b[0m             evaluations_dict[name][split] \u001b[39m=\u001b[39m evaluation\n\u001b[1;32m    607\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/ThoughtSource/libs/cot/cot/evaluate.py:23\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, overwrite, warn, config)\u001b[0m\n\u001b[1;32m     20\u001b[0m type_ \u001b[39m=\u001b[39m dataset[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     22\u001b[0m \u001b[39m# evaluate each sample\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m     24\u001b[0m     _evaluate,\n\u001b[1;32m     25\u001b[0m     fn_kwargs\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mtype_\u001b[39m\u001b[39m\"\u001b[39m: type_, \u001b[39m\"\u001b[39m\u001b[39moverwrite\u001b[39m\u001b[39m\"\u001b[39m: overwrite, \u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m: warn},\n\u001b[1;32m     26\u001b[0m     features\u001b[39m=\u001b[39mdataset\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mfeatures,\n\u001b[1;32m     27\u001b[0m     \u001b[39m# deleting the cache is necessary in generate if you call it multiple times\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[39m# not clear if it is needed here, but it doesn't hurt\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     load_from_cache_file\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m keys \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m     33\u001b[0m model_names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "coll.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate or extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.llms.openai import OpenAIChat\n",
    "\n",
    "from langchain.llms import OpenAI \n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain.chains.llm import LLMChain\n",
    "from cot import Collection\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating worldtree...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Extract script: Assumes there are CoTs in the dataset already\"\"\"\n",
    "\n",
    "input_dict = {\n",
    "    'instruction': None,\n",
    "    \"answer_extraction\": \"Be faithful and a little hopeful\",\n",
    "    \"cot_trigger\": \"Answer: Let's think step by step.\",\n",
    "    'api_service': \"chat_openai\",\n",
    "    'model':\"gpt-3.5-turbo\",\n",
    "    'temperature': 0,\n",
    "    'max_tokens': 800 \n",
    "}\n",
    "coll.extract_flexible(chain=answer_chain,input_dict=input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll['strategy_qa']['train'][0]['generated_cot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '6b3d0b3b-bc87-4ecb-a271-ee7d181cc2a2',\n",
       " 'fragments_version': '0.01',\n",
       " 'instruction': None,\n",
       " 'cot_trigger': \"Answer: Let's think step by step.\",\n",
       " 'cot_trigger_template': '',\n",
       " 'prompt_text': '',\n",
       " 'cot': \"\\n1. The mass of Earth causes a gravitational attraction between Earth and you.\\n2. The rotation of Earth causes centrifugal force, which is a pseudo-force that appears to be a reaction force in a rotating reference frame.\\n3. The revolution of Earth around the Sun causes Earth to be pulled by the Sun's gravity.\\n4. Weather patterns on Earth do not cause gravity.\\nThe correct answer is A) the mass of Earth.\",\n",
       " 'answers': [{'id': '65a672ce-5b05-4c74-aaac-5a05051c66a2',\n",
       "   'answer_extraction': 'Be faithful and a little hopeful',\n",
       "   'answer_extraction_template': '',\n",
       "   'answer_extraction_text': '',\n",
       "   'answer': 'as you approach each question, and use logical reasoning to eliminate incorrect answer choices.',\n",
       "   'correct_answer': None}],\n",
       " 'author': '',\n",
       " 'date': '2023/04/12 11:06:02',\n",
       " 'api_service': 'chat_openai',\n",
       " 'model': \"{'name': 'gpt-3.5-turbo', 'temperature': 0, 'max_tokens': 800}\",\n",
       " 'comment': 'answer_extraction cot',\n",
       " 'annotations': []}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll['worldtree']['test'][0]['generated_cot'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating worldtree...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Generate only\"\"\"\n",
    "input_dict = {\n",
    "    \"instruction\": \"Be faithful and a little hopeful\",\n",
    "    \"cot_trigger\": \"Answer: Let's think step by step.\",\n",
    "    'api_service': \"chat_openai\",\n",
    "    'model': \"gpt-3.5-turbo\",\n",
    "    \"answer_extraction\": \"Therefore, among A through D, the answer is\",\n",
    "    'temperature': 0,\n",
    "    'max_tokens': 800  \n",
    "}\n",
    "\n",
    "coll.generate_flexible(chain=cot_chain,input_dict=input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll['worldtree']['test'][0]['generated_cot'][2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=.0,model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "reflect_template = \"\"\"\n",
    "    Question: {question}\n",
    "    Answer_choices: {answer_choices}\n",
    "\n",
    "    Cot: {cot_trigger}{cot}\n",
    "    {answer_extraction}\n",
    "    Answer: {answer}\n",
    "    \n",
    "    {reflection_prompt}\n",
    "    \"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"question\",\"answer_choices\",\"cot_trigger\",\"cot\",\"answer_extraction\",'answer','reflection_prompt'], template=reflect_template)\n",
    "reflect_chain = LLMChain(llm=llm, prompt=prompt_template,output_key=\"reflection\")\n",
    "\n",
    "extraction_template = \"\"\"{instruction}\n",
    "\n",
    "    Question: {question}\n",
    "    Answer_choices: {answer_choices}\n",
    "\n",
    "    Cot: {cot_trigger}{cot}\n",
    "    {answer_extraction}{answer}\n",
    "    {reflection_prompt}{reflection}\n",
    "\n",
    "    {reflect_answer_extraction}\n",
    "    \"\"\"\n",
    "    #Get reflection\n",
    "prompt_template = PromptTemplate(input_variables=[\"instruction\",\"question\",\"answer_choices\",\"cot_trigger\",\"cot\",\"answer_extraction\",'answer','reflection_prompt','reflection','reflect_answer_extraction'], template=extraction_template)\n",
    "reflect_answer_chain = LLMChain(llm=llm, prompt=prompt_template,output_key=\"reflection_answer\")\n",
    "\n",
    "    # This is the overall chain where we run these two chains in sequence.\n",
    "from langchain.chains import SequentialChain\n",
    "reflect_overall_chain = SequentialChain(chains=[reflect_chain, reflect_answer_chain],input_variables=[\"instruction\",\"question\",\"answer_choices\",\"cot_trigger\",\"answer_extraction\",'cot','answer','reflection_prompt','reflect_answer_extraction'],\n",
    "        output_variables=[\"reflection\", \"reflection_answer\"],\n",
    "        verbose=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating worldtree...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#check for what is already in item\n",
    "input_dict = {\n",
    "    'cot_trigger':\"\", \n",
    "    'answer':\"\", \n",
    "    'answer_extraction': \"\", \n",
    "    'cot': \"\", \n",
    "    'instruction': \"\",\n",
    "    'api_service': \"chat_openai\", \n",
    "    'model': \"gpt-3.5-turbo\",\n",
    "    'reflection_prompt':\"Double check this\",\n",
    "    'reflect_answer_extraction':'Based on the reflection, what is the definite answer?',\n",
    "    'temperature': 0,\n",
    "    'max_tokens': 800 \n",
    "\n",
    "}\n",
    "coll.metareason_flexible(chain=reflect_overall_chain,input_dict=input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'ddbcca69-1ded-4593-a80f-03f3644190db',\n",
       " 'fragments_version': '0.01',\n",
       " 'instruction': '',\n",
       " 'cot_trigger': 'Double check this',\n",
       " 'cot_trigger_template': '',\n",
       " 'prompt_text': '',\n",
       " 'cot': 'Confirmed, the correct answer is A) the mass of Earth.',\n",
       " 'answers': [{'id': 'ea0f47f5-c8f8-41ee-ada0-f59618e8748f',\n",
       "   'answer_extraction': 'Based on the reflection, what is the definite answer?',\n",
       "   'answer_extraction_template': '',\n",
       "   'answer_extraction_text': 'self_reflection',\n",
       "   'answer': 'The definite answer is A) the mass of Earth.',\n",
       "   'correct_answer': None}],\n",
       " 'author': '',\n",
       " 'date': '2023/04/12 11:09:03',\n",
       " 'api_service': 'chat_openai',\n",
       " 'model': \"{'name': 'gpt-3.5-turbo', 'temperature': 0, 'max_tokens': 800}\",\n",
       " 'comment': 'self_reflection cot',\n",
       " 'annotations': []}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll['worldtree']['test'][0]['generated_cot'][1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/robertpraas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "596f335bc7d949fa8676578db54fc2f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dba05b2b8444a87898d624756961926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4cbe9a0e23748f6a85ff07924a1b824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f11c3da40d49b9ba696a4cac958c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79e70986cd541c8860a3c6b06d4bf39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4be95380ce42f7b2dab2e280bf01dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from cot import Collection\n",
    "col = Collection.load_thoughtsource_100(load_pregenerated_cots=True)\n",
    "col.select_generated_cots(author='thoughtsource', cot_trigger='kojima-01')\n",
    "dataset = col['worldtree']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading worldtree...\n"
     ]
    }
   ],
   "source": [
    "worldtree = Collection([\"worldtree\"], verbose=False)\n",
    "worldtree_1 = worldtree.select(split=\"train\", number_samples=1, random_samples=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1577',\n",
       " 'ref_id': '',\n",
       " 'question': 'A parent and a child share several characteristics. Both individuals are tall, have curly hair, are good cooks, and have freckles. Which of these characteristics is a learned behavior?',\n",
       " 'type': 'multiplechoice',\n",
       " 'choices': ['being tall',\n",
       "  'having curly hair',\n",
       "  'being a good cook',\n",
       "  'having freckles'],\n",
       " 'context': '',\n",
       " 'cot': ['Skills are learned characteristics.',\n",
       "  'A behavior is a kind of characteristic.',\n",
       "  'Cooking is a kind of skill for preparing food.'],\n",
       " 'answer': ['being a good cook'],\n",
       " 'generated_cot': [],\n",
       " 'feedback': []}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worldtree_1['worldtree']['train'][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflect"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = {\n",
    "    \"instruction\": \"Be faithful and a little hopeful\",\n",
    "    \"cot_trigger\": \"Answer: Let's think step by step.\",\n",
    "    \"answer_extraction\": \"Therefore, among A through D, the answer is\" \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating worldtree...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "processed_example:\n",
      "{'id': '1577', 'ref_id': '', 'question': 'A parent and a child share several characteristics. Both individuals are tall, have curly hair, are good cooks, and have freckles. Which of these characteristics is a learned behavior?', 'type': 'multiplechoice', 'choices': ['being tall', 'having curly hair', 'being a good cook', 'having freckles'], 'context': '', 'cot': ['Skills are learned characteristics.', 'A behavior is a kind of characteristic.', 'Cooking is a kind of skill for preparing food.'], 'answer': ['being a good cook'], 'generated_cot': [{'id': '2db4a9e4-301c-4197-8796-b40931fd7d6f', 'fragments_version': '0.01', 'instruction': 'Be faithful and a little hopeful', 'cot_trigger': \"Answer: Let's think step by step.\", 'cot_trigger_template': '', 'prompt_text': '', 'cot': '\\n1. A) is a learned behavior.\\n\\n2. B) is a learned behavior.\\n\\n3. C) is a learned behavior.\\n\\n4. D) is a learned behavior.\\n\\n5. E) is a learned behavior.\\n\\n6. F) is a learned behavior.\\n\\n7. G) is a learned behavior.\\n\\n8. H) is a learned behavior.\\n\\n9. I) is a learned behavior.\\n\\n10. J) is a learned behavior.\\n\\n11. K) is a learned behavior.\\n\\n12. L) is a learned behavior.\\n\\n13. M) is a learned behavior.\\n\\n14. N) is a learned behavior.\\n\\n15. O) is a learned behavior.\\n\\n16. P) is a learned behavior.\\n\\n17. Q) is a learned behavior.\\n\\n18. R) is a learned behavior.\\n\\n19. S) is a learned behavior.\\n\\n20. T) is a learned behavior.\\n\\n21. U) is a learned behavior.\\n\\n22. V) is a learned behavior.\\n\\n23. W) is a learned behavior.\\n\\n24.', 'answers': [{'id': '926544be-60ef-435f-9692-994aa8f738b3', 'answer_extraction': 'Therefore, among A through D, the answer is', 'answer_extraction_template': '', 'answer_extraction_text': '', 'answer': '\\nA) is a learned behavior.\\n\\nB) is a learned behavior.\\n\\nC) is a learned behavior.\\n\\nD) is a learned behavior.\\n\\nE) is a learned behavior.\\n\\nF) is a learned behavior.\\n\\nG) is a learned behavior.\\n\\nH) is a learned behavior.\\n\\nI) is a learned behavior.\\n\\nJ) is a learned behavior.\\n\\nK) is a learned behavior.\\n\\nL) is a learned behavior.\\n\\nM) is a learned behavior.\\n\\nN) is a learned behavior.\\n\\nO) is a learned behavior.\\n\\nP) is a learned behavior.\\n\\nQ) is a learned behavior.\\n\\nR) is a learned behavior.\\n\\nS) is a learned behavior.\\n\\nT) is a learned behavior.\\n\\nU) is a learned behavior.\\n\\nV) is a learned behavior.\\n\\nW) is a learned behavior.\\n\\nTherefore, among A through F, the answer is\\n\\nA) is a learned behavior.\\n\\nB) is a learned behavior.\\n\\nC) is a learned behavior.\\n\\nD) is a learned behavior.\\n\\n', 'correct_answer': None}], 'author': '', 'date': '2023/03/14 07:38:05', 'api_service': '', 'model': \"{'name': '', 'temperature': 0, 'max_tokens': 800}\", 'comment': '', 'annotations': []}], 'feedback': []}\n"
     ]
    }
   ],
   "source": [
    "generate_test = worldtree_1.generate_extract_flexibly(chain=overall_chain,input_dict=input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'id': '2db4a9e4-301c-4197-8796-b40931fd7d6f',\n",
       "   'fragments_version': '0.01',\n",
       "   'instruction': 'Be faithful and a little hopeful',\n",
       "   'cot_trigger': \"Answer: Let's think step by step.\",\n",
       "   'cot_trigger_template': '',\n",
       "   'prompt_text': '',\n",
       "   'cot': '\\n1. A) is a learned behavior.\\n\\n2. B) is a learned behavior.\\n\\n3. C) is a learned behavior.\\n\\n4. D) is a learned behavior.\\n\\n5. E) is a learned behavior.\\n\\n6. F) is a learned behavior.\\n\\n7. G) is a learned behavior.\\n\\n8. H) is a learned behavior.\\n\\n9. I) is a learned behavior.\\n\\n10. J) is a learned behavior.\\n\\n11. K) is a learned behavior.\\n\\n12. L) is a learned behavior.\\n\\n13. M) is a learned behavior.\\n\\n14. N) is a learned behavior.\\n\\n15. O) is a learned behavior.\\n\\n16. P) is a learned behavior.\\n\\n17. Q) is a learned behavior.\\n\\n18. R) is a learned behavior.\\n\\n19. S) is a learned behavior.\\n\\n20. T) is a learned behavior.\\n\\n21. U) is a learned behavior.\\n\\n22. V) is a learned behavior.\\n\\n23. W) is a learned behavior.\\n\\n24.',\n",
       "   'answers': [{'id': '926544be-60ef-435f-9692-994aa8f738b3',\n",
       "     'answer_extraction': 'Therefore, among A through D, the answer is',\n",
       "     'answer_extraction_template': '',\n",
       "     'answer_extraction_text': '',\n",
       "     'answer': '\\nA) is a learned behavior.\\n\\nB) is a learned behavior.\\n\\nC) is a learned behavior.\\n\\nD) is a learned behavior.\\n\\nE) is a learned behavior.\\n\\nF) is a learned behavior.\\n\\nG) is a learned behavior.\\n\\nH) is a learned behavior.\\n\\nI) is a learned behavior.\\n\\nJ) is a learned behavior.\\n\\nK) is a learned behavior.\\n\\nL) is a learned behavior.\\n\\nM) is a learned behavior.\\n\\nN) is a learned behavior.\\n\\nO) is a learned behavior.\\n\\nP) is a learned behavior.\\n\\nQ) is a learned behavior.\\n\\nR) is a learned behavior.\\n\\nS) is a learned behavior.\\n\\nT) is a learned behavior.\\n\\nU) is a learned behavior.\\n\\nV) is a learned behavior.\\n\\nW) is a learned behavior.\\n\\nTherefore, among A through F, the answer is\\n\\nA) is a learned behavior.\\n\\nB) is a learned behavior.\\n\\nC) is a learned behavior.\\n\\nD) is a learned behavior.\\n\\n',\n",
       "     'correct_answer': None}],\n",
       "   'author': '',\n",
       "   'date': '2023/03/14 07:38:05',\n",
       "   'api_service': '',\n",
       "   'model': \"{'name': '', 'temperature': 0, 'max_tokens': 800}\",\n",
       "   'comment': '',\n",
       "   'annotations': []}]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Force langchain into TS structure \n",
    "worldtree_new = {'worldtree':{'train':generate_test}}\n",
    "\n",
    "#create and collect a json to make collection\n",
    "with open(\"sample.json\", \"w\") as outfile:\n",
    "    json.dump(worldtree_new, outfile)\n",
    "collect = Collection.from_json('sample.json')\n",
    "\n",
    "collect['worldtree']['train']['generated_cot']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating worldtree...\n",
      "processed_example:\n",
      "{'id': '1577', 'ref_id': '', 'question': 'A parent and a child share several characteristics. Both individuals are tall, have curly hair, are good cooks, and have freckles. Which of these characteristics is a learned behavior?', 'type': 'multiplechoice', 'choices': ['being tall', 'having curly hair', 'being a good cook', 'having freckles'], 'context': '', 'cot': ['Skills are learned characteristics.', 'A behavior is a kind of characteristic.', 'Cooking is a kind of skill for preparing food.'], 'answer': ['being a good cook'], 'generated_cot': [{'id': '7d977578-5359-49d9-bd26-c373911bf3c9', 'fragments_version': '0.01', 'instruction': 'Be faithful and a little hopeful', 'cot_trigger': \"Answer: Let's think step by step.\", 'cot_trigger_template': '', 'prompt_text': '', 'cot': '\\n1. A) is a learned behavior.\\n\\n2. B) is a learned behavior.\\n\\n3. C) is a learned behavior.\\n\\n4. D) is a learned behavior.\\n\\n5. E) is a learned behavior.\\n\\n6. F) is a learned behavior.\\n\\n7. G) is a learned behavior.\\n\\n8. H) is a learned behavior.\\n\\n9. I) is a learned behavior.\\n\\n10. J) is a learned behavior.\\n\\n11. K) is a learned behavior.\\n\\n12. L) is a learned behavior.\\n\\n13. M) is a learned behavior.\\n\\n14. N) is a learned behavior.\\n\\n15. O) is a learned behavior.\\n\\n16. P) is a learned behavior.\\n\\n17. Q) is a learned behavior.\\n\\n18. R) is a learned behavior.\\n\\n19. S) is a learned behavior.\\n\\n20. T) is a learned behavior.\\n\\n21. U) is a learned behavior.\\n\\n22. V) is a learned behavior.\\n\\n23. W) is a learned behavior.\\n\\n24.', 'answers': [], 'author': '', 'date': '2023/03/14 08:00:42', 'api_service': '', 'model': \"{'name': '', 'temperature': 0, 'max_tokens': 800}\", 'comment': '', 'annotations': []}], 'feedback': []}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': '1577',\n",
       "  'ref_id': '',\n",
       "  'question': 'A parent and a child share several characteristics. Both individuals are tall, have curly hair, are good cooks, and have freckles. Which of these characteristics is a learned behavior?',\n",
       "  'type': 'multiplechoice',\n",
       "  'choices': ['being tall',\n",
       "   'having curly hair',\n",
       "   'being a good cook',\n",
       "   'having freckles'],\n",
       "  'context': '',\n",
       "  'cot': ['Skills are learned characteristics.',\n",
       "   'A behavior is a kind of characteristic.',\n",
       "   'Cooking is a kind of skill for preparing food.'],\n",
       "  'answer': ['being a good cook'],\n",
       "  'generated_cot': [{'id': '7d977578-5359-49d9-bd26-c373911bf3c9',\n",
       "    'fragments_version': '0.01',\n",
       "    'instruction': 'Be faithful and a little hopeful',\n",
       "    'cot_trigger': \"Answer: Let's think step by step.\",\n",
       "    'cot_trigger_template': '',\n",
       "    'prompt_text': '',\n",
       "    'cot': '\\n1. A) is a learned behavior.\\n\\n2. B) is a learned behavior.\\n\\n3. C) is a learned behavior.\\n\\n4. D) is a learned behavior.\\n\\n5. E) is a learned behavior.\\n\\n6. F) is a learned behavior.\\n\\n7. G) is a learned behavior.\\n\\n8. H) is a learned behavior.\\n\\n9. I) is a learned behavior.\\n\\n10. J) is a learned behavior.\\n\\n11. K) is a learned behavior.\\n\\n12. L) is a learned behavior.\\n\\n13. M) is a learned behavior.\\n\\n14. N) is a learned behavior.\\n\\n15. O) is a learned behavior.\\n\\n16. P) is a learned behavior.\\n\\n17. Q) is a learned behavior.\\n\\n18. R) is a learned behavior.\\n\\n19. S) is a learned behavior.\\n\\n20. T) is a learned behavior.\\n\\n21. U) is a learned behavior.\\n\\n22. V) is a learned behavior.\\n\\n23. W) is a learned behavior.\\n\\n24.',\n",
       "    'answers': [],\n",
       "    'author': '',\n",
       "    'date': '2023/03/14 08:00:42',\n",
       "    'api_service': '',\n",
       "    'model': \"{'name': '', 'temperature': 0, 'max_tokens': 800}\",\n",
       "    'comment': '',\n",
       "    'annotations': []}],\n",
       "  'feedback': []}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_only = worldtree_1.generate_flexible(chain=cot_chain,input_dict=input_dict)\n",
    "generate_only"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Force langchain into TS structure -- from geneate\n",
    "worldtree_new = {'worldtree':{'train':generate_only}}\n",
    "\n",
    "#create and collect a json to make collection\n",
    "with open(\"sample.json\", \"w\") as outfile:\n",
    "    json.dump(worldtree_new, outfile)\n",
    "collect = Collection.from_json('sample.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating worldtree...\n",
      "processed_example:\n",
      "{'id': '1577', 'ref_id': '', 'question': 'A parent and a child share several characteristics. Both individuals are tall, have curly hair, are good cooks, and have freckles. Which of these characteristics is a learned behavior?', 'type': 'multiplechoice', 'choices': ['being tall', 'having curly hair', 'being a good cook', 'having freckles'], 'context': '', 'cot': ['Skills are learned characteristics.', 'A behavior is a kind of characteristic.', 'Cooking is a kind of skill for preparing food.'], 'answer': ['being a good cook'], 'generated_cot': [{'id': '7d977578-5359-49d9-bd26-c373911bf3c9', 'fragments_version': '0.01', 'instruction': 'Be faithful and a little hopeful', 'cot_trigger': \"Answer: Let's think step by step.\", 'cot_trigger_template': '', 'prompt_text': '', 'cot': '\\n1. A) is a learned behavior.\\n\\n2. B) is a learned behavior.\\n\\n3. C) is a learned behavior.\\n\\n4. D) is a learned behavior.\\n\\n5. E) is a learned behavior.\\n\\n6. F) is a learned behavior.\\n\\n7. G) is a learned behavior.\\n\\n8. H) is a learned behavior.\\n\\n9. I) is a learned behavior.\\n\\n10. J) is a learned behavior.\\n\\n11. K) is a learned behavior.\\n\\n12. L) is a learned behavior.\\n\\n13. M) is a learned behavior.\\n\\n14. N) is a learned behavior.\\n\\n15. O) is a learned behavior.\\n\\n16. P) is a learned behavior.\\n\\n17. Q) is a learned behavior.\\n\\n18. R) is a learned behavior.\\n\\n19. S) is a learned behavior.\\n\\n20. T) is a learned behavior.\\n\\n21. U) is a learned behavior.\\n\\n22. V) is a learned behavior.\\n\\n23. W) is a learned behavior.\\n\\n24.', 'answers': [{'id': 'e1ecd70e-51f4-4ade-a153-7110946750b9', 'answer_extraction': 'Therefore, among A through D, the answer is', 'answer_extraction_template': '', 'answer_extraction_text': '', 'answer': '\\nA) is a learned behavior.\\n\\nB) is a learned behavior.\\n\\nC) is a learned behavior.\\n\\nD) is a learned behavior.\\n\\nE) is a learned behavior.\\n\\nF) is a learned behavior.\\n\\nG) is a learned behavior.\\n\\nH) is a learned behavior.\\n\\nI) is a learned behavior.\\n\\nJ) is a learned behavior.\\n\\nK) is a learned behavior.\\n\\nL) is a learned behavior.\\n\\nM) is a learned behavior.\\n\\nN) is a learned behavior.\\n\\nO) is a learned behavior.\\n\\nP) is a learned behavior.\\n\\nQ) is a learned behavior.\\n\\nR) is a learned behavior.\\n\\nS) is a learned behavior.\\n\\nT) is a learned behavior.\\n\\nU) is a learned behavior.\\n\\nV) is a learned behavior.\\n\\nW) is a learned behavior.\\n\\nTherefore, among A through F, the answer is\\n\\nA) is a learned behavior.\\n\\nB) is a learned behavior.\\n\\nC) is a learned behavior.\\n\\nD) is a learned behavior.\\n\\n', 'correct_answer': None}], 'author': '', 'date': '2023/03/14 08:00:42', 'api_service': '', 'model': \"{'name': '', 'temperature': 0, 'max_tokens': 800}\", 'comment': '', 'annotations': []}, {'id': '654f07a6-6104-495d-b328-e8c067af2322', 'fragments_version': '0.01', 'instruction': 'Be faithful and a little hopeful', 'cot_trigger': \"Answer: Let's think step by step.\", 'cot_trigger_template': '', 'prompt_text': '', 'cot': '\\n1. A) is a learned behavior.\\n\\n2. B) is a learned behavior.\\n\\n3. C) is a learned behavior.\\n\\n4. D) is a learned behavior.\\n\\n5. E) is a learned behavior.\\n\\n6. F) is a learned behavior.\\n\\n7. G) is a learned behavior.\\n\\n8. H) is a learned behavior.\\n\\n9. I) is a learned behavior.\\n\\n10. J) is a learned behavior.\\n\\n11. K) is a learned behavior.\\n\\n12. L) is a learned behavior.\\n\\n13. M) is a learned behavior.\\n\\n14. N) is a learned behavior.\\n\\n15. O) is a learned behavior.\\n\\n16. P) is a learned behavior.\\n\\n17. Q) is a learned behavior.\\n\\n18. R) is a learned behavior.\\n\\n19. S) is a learned behavior.\\n\\n20. T) is a learned behavior.\\n\\n21. U) is a learned behavior.\\n\\n22. V) is a learned behavior.\\n\\n23. W) is a learned behavior.\\n\\n24.', 'answers': [], 'author': '', 'date': '2023/03/14 08:03:54', 'api_service': '', 'model': \"{'name': '', 'temperature': 0, 'max_tokens': 800}\", 'comment': 'self_reflection cot', 'annotations': []}], 'feedback': []}\n"
     ]
    }
   ],
   "source": [
    "extract = collect.extract_flexible(chain=answer_chain,input_dict=input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll = Collection.load_thoughtsource_100(names='worldtree',load_pregenerated_cots=True)\n",
    "coll = coll.select(split=\"all\", number_samples=1)\n",
    "coll.select_generated_cots(author='thoughtsource',cot_trigger = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
