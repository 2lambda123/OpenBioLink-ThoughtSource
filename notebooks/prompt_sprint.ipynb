{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from cot import Collection\n",
    "from cot.generate import FRAGMENTS\n",
    "from rich.pretty import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number_examples = 100\n",
    "# dataset_name_splits = [(\"worldtree\", \"test\"), (\"open_book_qa\", \"test\")]\n",
    "# api_service_model_time = [(\"cohere\", \"command-xlarge-nightly\", 0), (\"huggingface_hub\", \"google/flan-t5-xl\", 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_extraction(type,choices): # collection[dataset_name][split][0][\"choices\"]\n",
    "    if type == \"bool\": \n",
    "        return \"kojima-yes-no\"\n",
    "    elif type == \"multiplechoice\":\n",
    "        n_choices = len(choices)\n",
    "        if n_choices == 3: answer_extraction_key = 'kojima-A-C'\n",
    "        elif n_choices == 4: answer_extraction_key = 'kojima-A-D'\n",
    "        elif n_choices == 5: answer_extraction_key = 'kojima-A-E'\n",
    "        elif n_choices == 6: answer_extraction_key = 'kojima-A-F'\n",
    "        return(answer_extraction_key)\n",
    "    else: raise ValueError(\"type must be bool or multiplechoice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join strings from a list with underscore\n",
    "def join_strings(list_of_strings):\n",
    "    joined_string = \"\"\n",
    "    for string in list_of_strings:\n",
    "        joined_string += (\"_\" + str(string))\n",
    "    return(joined_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_sprint(dataset_name_splits, api_service_model, cot_trigger_keys, number_examples):\n",
    "    for dataset_name, split in dataset_name_splits:\n",
    "        collection = Collection([dataset_name], verbose=False)\n",
    "        collection = collection.select(split=split, number_samples=number_examples)\n",
    "        answer_extraction_key = get_answer_extraction(collection[dataset_name][split][0][\"type\"], collection[dataset_name][split][0][\"choices\"])\n",
    "        for api_service, model, api_time_interval in api_service_model:\n",
    "\n",
    "            config={\n",
    "                \"cot_trigger_keys\": cot_trigger_keys,\n",
    "                \"answer_extraction_keys\": [answer_extraction_key], # Therefore, among A through C/D/E/F, the answer is'\n",
    "                \"author\" : \"thoughtsource\",\n",
    "                \"api_service\": api_service,\n",
    "                \"engine\": model,\n",
    "                \"temperature\": 0,\n",
    "                \"max_tokens\": 512,\n",
    "                \"api_time_interval\": api_time_interval,\n",
    "                \"verbose\": False,\n",
    "                \"warn\": False,\n",
    "            }\n",
    "        \n",
    "            collection.generate(config=config)\n",
    "            collection.evaluate()\n",
    "            collection.dump(dataset_name + \"_\" + split + \"_\" + str(number_examples) + \"_\" + api_service + \"_\" + model + join_strings(config[\"cot_trigger_keys\"]) + \".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading medmc_qa...\n"
     ]
    }
   ],
   "source": [
    "collection = Collection([\"medmc_qa\"], verbose=False, load_pregenerated_cots=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = collection.select(split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run through every item of the dataset and save the index of the items that have a generated_cot\n",
    "idx = []\n",
    "for i,ex in enumerate(collection[\"medmc_qa\"][\"validation\"]):\n",
    "    if ex[\"generated_cot\"] != []:\n",
    "        idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection[\"medmc_qa\"][\"validation\"] = collection[\"medmc_qa\"][\"validation\"].select(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = collection.select(\"validation\", number_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "| Name     | Train   |   Valid | Test   |\n",
       "|----------|---------|---------|--------|\n",
       "| medmc_qa | -       |     100 | -      |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa', 'open_book_qa', 'pubmed_qa', 'qed', 'strategy_qa', 'svamp', 'worldtree']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run through every item of the dataset and save the index of the items that have a generated_cot\n",
    "control_idx = []\n",
    "for i,ex in enumerate(collection[\"medmc_qa\"][\"validation\"]):\n",
    "    if ex[\"generated_cot\"] != []:\n",
    "        control_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(control_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "| Name     | Train   |   Valid | Test   |\n",
       "|----------|---------|---------|--------|\n",
       "| medmc_qa | -       |    4183 | -      |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa', 'open_book_qa', 'pubmed_qa', 'qed', 'strategy_qa', 'svamp', 'worldtree']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = collection[\"medmc_qa\"][\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sorted = dataset.filter(lambda example: example[\"generated_cot\"] != [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'ref_id', 'question', 'type', 'choices', 'context', 'cot', 'answer', 'generated_cot', 'feedback'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection[\"medmc_qa\"][\"validation\"] = dataset_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "| Name     | Train   |   Valid | Test   |\n",
       "|----------|---------|---------|--------|\n",
       "| medmc_qa | -       |    1000 | -      |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa', 'open_book_qa', 'pubmed_qa', 'qed', 'strategy_qa', 'svamp', 'worldtree']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "collection.dump(\"medmc_qa_filtered_1000.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "random_ids = random.sample(range(1000), 100)\n",
    "random_ids = sorted(random_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(random_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cot.create_thoughtsource_100 import create_medmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading medmc_qa...\n"
     ]
    }
   ],
   "source": [
    "create_medmc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dataset_sorted.to_json(\"medmc_qa_filtered.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.dump(\"medmc_foo_100.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll = Collection.from_json(\"medmc_foo_100.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "| Name     | Train   |   Valid | Test   |\n",
       "|----------|---------|---------|--------|\n",
       "| medmc_qa | -       |     100 | -      |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa', 'open_book_qa', 'pubmed_qa', 'qed', 'strategy_qa', 'svamp', 'worldtree']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating worldtree test...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c66edfa620049b38143b2c99bb7287d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'worldtree': {'test': {'accuracy': {'command-xlarge-nightly': {'None_None_kojima-A-D': 0.63,\n",
       "     'None_kojima-01_kojima-A-D': 0.63,\n",
       "     'None_kojima-02_kojima-A-D': 0.65,\n",
       "     'None_kojima-03_kojima-A-D': 0.59}}}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = Collection.from_json(\"worldtree_test_100_cohere_command-xlarge-nightly.json\")\n",
    "collection.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating worldtree test...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ba0bd7f2a7434ba82e145974fe51d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'worldtree': {'test': {'accuracy': {'text-davinci-003': {'None_kojima-01_kojima-A-D': 0.73}}}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = Collection.from_json(\"worldtree_test_100_openai_text-davinci-003.json\")\n",
    "collection.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating open_book_qa test...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc643e3ea8d64f45860d7c292bdeec81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'open_book_qa': {'test': {'accuracy': {'command-xlarge-nightly': {'None_kojima-01_kojima-A-D': 0.38}}}}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = Collection.from_json(\"open_book_qa_test_100_cohere_command-xlarge-nightly.json\")\n",
    "collection.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating open_book_qa test...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcfa699f63a04340bf20f60b7c6dedfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'open_book_qa': {'test': {'accuracy': {'text-davinci-003': {'None_kojima-01_kojima-A-D': 0.68}}}}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = Collection.from_json(\"open_book_qa_test_100_openai_text-davinci-003.json\")\n",
    "collection.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one example with one prompt took 1.5 minutes\n",
    "# so 100 examples with 4 prompts probably take 1.5 * 100 * 4 = 600 minutes = 10 hours\n",
    "# started at 19h10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading commonsense_qa...\n",
      "| Name           | Train   |   Valid | Test   |\n",
      "|----------------|---------|---------|--------|\n",
      "| commonsense_qa | -       |       1 | -      |\n",
      "\n",
      "Not loaded: ['aqua', 'asdiv', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa', 'medmc_qa', 'open_book_qa', 'pubmed_qa', 'qed', 'strategy_qa', 'svamp', 'worldtree']\n"
     ]
    }
   ],
   "source": [
    "collection = Collection([dataset_name], verbose=False)\n",
    "collection = collection.select(split=split, number_samples=number_examples)\n",
    "print(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration of the input and parameters of the language model \n",
    "config={\n",
    "    \"cot_trigger_keys\": [\n",
    "        None,         # no cot-trigger\n",
    "        'kojima-01',  # Answer: Let's think step by step.\n",
    "        'kojima-02',  # Answer: First,\n",
    "        'kojima-03'], # Answer: Let's think about this logically.\n",
    "    \"answer_extraction_keys\": [answer_extraction_key], # Therefore, among A through C/D/E/F, the answer is'\n",
    "    \"author\" : \"thoughtsource\",\n",
    "    \"api_service\": api_service,\n",
    "    \"engine\": model,\n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"verbose\": False,\n",
    "    \"warn\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        You are about to \u001b[1m call an external API \u001b[0m in total 8 times, which \u001b[1m may produce costs \u001b[0m.\n",
      "        API calls for reasoning chain generation: 1 samples  * 1 instructions  * 4 reasoning chain triggers\n",
      "        API calls for answer extraction: n_samples  1 samples  * 1 instructions  * 4 reasoning chain triggers * 1 answer extraction triggers\n",
      "        Do you want to continue? y/n\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# Generating chains-of-thought and answer extractions\n",
    "collection.generate(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense_qa validation...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'commonsense_qa': {'validation': {'accuracy': {'google/flan-t5-xl': {'None_None_kojima-A-E': 1.0,\n",
       "     'None_kojima-01_kojima-A-E': 1.0,\n",
       "     'None_kojima-02_kojima-A-E': 1.0,\n",
       "     'None_kojima-03_kojima-A-E': 1.0}}}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "collection.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.dump(dataset_name + \"_\" + split + \"_\" + str(number_examples) + \".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'commonsense_qa_validation_1'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name + \"_\" + split + \"_\" + str(number_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading med_qa...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "| Name   | Train   | Valid   |   Test |\n",
       "|--------|---------|---------|--------|\n",
       "| med_qa | -       | -       |      1 |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'medmc_qa', 'open_book_qa', 'pubmed_qa', 'qed', 'strategy_qa', 'svamp', 'worldtree']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly select 10 rows from train split\n",
    "med = Collection([\"med_qa\"], verbose=False, load_pregenerated_cots=\"all\")\n",
    "med_test_1 = med.select(split=\"test\", number_samples=1)\n",
    "med_test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_test_1.dump(\"med_test_1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading med_qa...\n",
      "Loading pubmed_qa...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "| Name      |   Train | Valid   | Test   |\n",
       "|-----------|---------|---------|--------|\n",
       "| med_qa    |     100 | -       | -      |\n",
       "| pubmed_qa |     100 | -       | -      |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'medmc_qa', 'open_book_qa', 'qed', 'strategy_qa', 'svamp', 'worldtree']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that you can also sample from multiple datasets into one collection:\n",
    "collection_medical = Collection([\"med_qa\", \"pubmed_qa\"], verbose=False)\n",
    "collection_medical_100 = collection_medical.select(split=\"train\", number_samples=100)\n",
    "collection_medical_100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f57e102",
   "metadata": {},
   "source": [
    "## 2. Generating reasoning chains and extracting answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class creator(object):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_subset = [(\"commonsense_qa\", \"validation\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonsense = creator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, value in (name_subset):\n",
    "    setattr(commonsense, name, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'commonsense_qa': 'validation'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "commonsense.__dict__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f57e102",
   "metadata": {},
   "source": [
    "#### Using predefined text snippets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f043028",
   "metadata": {},
   "source": [
    "ThoughtSource comes pre-loaded with a large [collection of text snippets ('prompt fragments')](https://github.com/OpenBioLink/ThoughtSource/blob/main/libs/cot/cot/fragments.json) to elicit chain-of-thought reasoning in large language models and to extract answers from chains-of-thought. Let's see how prompt fragments look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-01'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's think step by step.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-02'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Answer: We should think about this step by step.'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-03'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Answer: First,'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-04'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Answer: Before we dive into the answer,'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-05'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Answer: Proof followed by the answer.'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-06'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's think step by step in a realistic way.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-07'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's think step by step using common sense and knowledge.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-08'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's think like a detective step by step.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-09'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's think about this logically.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-10'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's think step by step. First,\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-11'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's think\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-12'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's solve this problem by splitting it into steps.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-13'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Answer: The answer is after the proof.'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-14'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's be realistic and think step by step.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-01'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's derive the differential diagnosis step by step.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-02'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's use step by step inductive reasoning, given the medical nature of the question.\"</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-03'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's differentiate using step by step reasoning like a medical expert.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-04'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's think step by step using deductive reasoning.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-05'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's differentiate using step by step reasoning .\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-06'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's think step by step to arrive at one of the options.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-07'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's break the problem into multiple steps.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-08'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's use step by step deductive reasoning, given the medical nature of the question.\"</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-09'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's think step by step like a doctor.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-10'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's think step by step like a medical expert.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-11'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's summarize the facts step by step.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-12'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's think step by step using inductive reasoning.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-13'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's think step by step using deductive reasoning like a medical expert.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-14'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's be concise and think step by step.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-15'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's differentiate using step by step deductive reasoning like a medical expert.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-16'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's argue step by step.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-17'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's think step by step like a clinician.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-18'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's reflect on each answer option step by step.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-19'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's reason and differentiate options step by step like a medical expert.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-20'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's differentiate using step by step inductive reasoning like a medical expert.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-21'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's think step by step given every option equal consideration.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-22'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's think step by step like a scientist.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-23'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's use step by step inductive reasoning.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-24'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's work by elimination step by step.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-25'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's use step by step deductive reasoning.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-26'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's follow a Bayesian step by step approach.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-27'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's reflect on each option from the least likely to the most likely.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'lievin-28'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's use step by step Bayesian reasoning, given the medical nature of the question.\"</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-01'\u001b[0m, \u001b[32m\"Answer: Let's think step by step.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-02'\u001b[0m, \u001b[32m'Answer: We should think about this step by step.'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-03'\u001b[0m, \u001b[32m'Answer: First,'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-04'\u001b[0m, \u001b[32m'Answer: Before we dive into the answer,'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-05'\u001b[0m, \u001b[32m'Answer: Proof followed by the answer.'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-06'\u001b[0m, \u001b[32m\"Answer: Let's think step by step in a realistic way.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-07'\u001b[0m, \u001b[32m\"Answer: Let's think step by step using common sense and knowledge.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-08'\u001b[0m, \u001b[32m\"Answer: Let's think like a detective step by step.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-09'\u001b[0m, \u001b[32m\"Answer: Let's think about this logically.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-10'\u001b[0m, \u001b[32m\"Answer: Let's think step by step. First,\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-11'\u001b[0m, \u001b[32m\"Answer: Let's think\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-12'\u001b[0m, \u001b[32m\"Answer: Let's solve this problem by splitting it into steps.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-13'\u001b[0m, \u001b[32m'Answer: The answer is after the proof.'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-14'\u001b[0m, \u001b[32m\"Answer: Let's be realistic and think step by step.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'lievin-01'\u001b[0m, \u001b[32m\"Answer: Let's derive the differential diagnosis step by step.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'lievin-02'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m\"Answer: Let's use step by step inductive reasoning, given the medical nature of the question.\"\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'lievin-03'\u001b[0m, \u001b[32m\"Answer: Let's differentiate using step by step reasoning like a medical expert.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'lievin-04'\u001b[0m, \u001b[32m\"Answer: Let's think step by step using deductive reasoning.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'lievin-05'\u001b[0m, \u001b[32m\"Answer: Let's differentiate using step by step reasoning .\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'lievin-06'\u001b[0m, \u001b[32m\"Answer: Let's think step by step to arrive at one of the options.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'lievin-07'\u001b[0m, \u001b[32m\"Answer: Let's break the problem into multiple steps.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'lievin-08'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m\"Answer: Let's use step by step deductive reasoning, given the medical nature of the question.\"\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'lievin-09'\u001b[0m, \u001b[32m\"Answer: Let's think step by step like a doctor.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'lievin-10'\u001b[0m, \u001b[32m\"Answer: Let's think step by step like a medical expert.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'lievin-11'\u001b[0m, \u001b[32m\"Answer: Let's summarize the facts step by step.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'lievin-12'\u001b[0m, \u001b[32m\"Answer: Let's think step by step using inductive reasoning.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'lievin-13'\u001b[0m, \u001b[32m\"Answer: Let's think step by step using deductive reasoning like a medical expert.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'lievin-14'\u001b[0m, \u001b[32m\"Answer: Let's be concise and think step by step.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'lievin-15'\u001b[0m, \u001b[32m\"Answer: Let's differentiate using step by step deductive reasoning like a medical expert.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'lievin-16'\u001b[0m, \u001b[32m\"Answer: Let's argue step by step.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'lievin-17'\u001b[0m, \u001b[32m\"Answer: Let's think step by step like a clinician.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'lievin-18'\u001b[0m, \u001b[32m\"Answer: Let's reflect on each answer option step by step.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'lievin-19'\u001b[0m, \u001b[32m\"Answer: Let's reason and differentiate options step by step like a medical expert.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'lievin-20'\u001b[0m, \u001b[32m\"Answer: Let's differentiate using step by step inductive reasoning like a medical expert.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'lievin-21'\u001b[0m, \u001b[32m\"Answer: Let's think step by step given every option equal consideration.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'lievin-22'\u001b[0m, \u001b[32m\"Answer: Let's think step by step like a scientist.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'lievin-23'\u001b[0m, \u001b[32m\"Answer: Let's use step by step inductive reasoning.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'lievin-24'\u001b[0m, \u001b[32m\"Answer: Let's work by elimination step by step.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'lievin-25'\u001b[0m, \u001b[32m\"Answer: Let's use step by step deductive reasoning.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'lievin-26'\u001b[0m, \u001b[32m\"Answer: Let's follow a Bayesian step by step approach.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'lievin-27'\u001b[0m, \u001b[32m\"Answer: Let's reflect on each option from the least likely to the most likely.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'lievin-28'\u001b[0m, \u001b[32m\"Answer: Let's use step by step Bayesian reasoning, given the medical nature of the question.\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Chain of thought prompts\n",
    "pprint(list(FRAGMENTS[\"cot_triggers\"].items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-01'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Therefore, the answer is'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-02'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Therefore,'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-03'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'The answer is'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-numerals'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Therefore, the answer (arabic numerals) is'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-yes-no'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Therefore, the answer (Yes or No) is'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-A-C'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Therefore, among A through C, the answer is'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-A-D'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Therefore, among A through D, the answer is'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-A-E'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Therefore, among A through E, the answer is'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-A-F'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Therefore, among A through F, the answer is'</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-01'\u001b[0m, \u001b[32m'Therefore, the answer is'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-02'\u001b[0m, \u001b[32m'Therefore,'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-03'\u001b[0m, \u001b[32m'The answer is'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-numerals'\u001b[0m, \u001b[32m'Therefore, the answer \u001b[0m\u001b[32m(\u001b[0m\u001b[32marabic numerals\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-yes-no'\u001b[0m, \u001b[32m'Therefore, the answer \u001b[0m\u001b[32m(\u001b[0m\u001b[32mYes or No\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-A-C'\u001b[0m, \u001b[32m'Therefore, among A through C, the answer is'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-A-D'\u001b[0m, \u001b[32m'Therefore, among A through D, the answer is'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-A-E'\u001b[0m, \u001b[32m'Therefore, among A through E, the answer is'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-A-F'\u001b[0m, \u001b[32m'Therefore, among A through F, the answer is'\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer extraction prompts\n",
    "pprint(list(FRAGMENTS[\"answer_extractions\"].items()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f57e102",
   "metadata": {},
   "source": [
    "#### Configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m \n",
      "    \"instruction_keys\": list(str) - Determines which instruction_keys are used from fragments.json,\n",
      "        the corresponding string will be inserted under \"instruction\" in the fragments. Default: [None] (No instruction)\n",
      "    \"cot_trigger_keys\": list(str) - Determines which cot triggers are used from fragments.json,\n",
      "        the corresponding string will be inserted under \"cot_trigger\" in the fragments. Default: [\"kojima-01\"]\n",
      "    \"answer_extraction_keys\": list(str) - Determines which answer extraction prompts are used from fragments.json,\n",
      "        the corresponding string will be inserted under \"answer\" in the fragments. Default: [\"kojima-01\"]\n",
      "    \"template_cot_generation\": string - is the model input in the text generation step, variables in brackets.\n",
      "        Only variables of this list are allowed: \"instruction\", 'question\", \"answer_choices\", \"cot_trigger\"\n",
      "        Default: {instruction}\\n\\n{question}\\n{answer_choices}\\n\\n{cot_trigger}\n",
      "    \"template_answer_extraction\": string - is the model input in the answer extraction step, variables in brackets.\n",
      "        Only variables of this list are allowed: \"instruction\", 'question\", \"answer_choices\", \"cot_trigger\",\n",
      "        \"cot\", \"answer\"\n",
      "        Default: {instruction}\\n\\n{question}\\n{answer_choices}\\n\\n{cot_trigger}{cot}\\n{answer_extraction}\n",
      "    \"author\" : str - Name of the person responsible for generation, Default: \"\"\n",
      "    \"api_service\" str - Name of the used api service: \"openai\", \"huggingface_hub\" or \"cohere\"\n",
      "        or a mock api service \"mock_api\" for debugging, Default: \"huggingface_hub\"\n",
      "    \"engine\": str -  Name of model used, look at website of api which are\n",
      "        available, e.g. for \"openai\": \"text-davinci-002\", Default: \"google/flan-t5-xl\"\n",
      "    \"temperature\": float - Describes how much randomness is in the generated output,\n",
      "        0.0 means the model will only output the most likely answer, 1.0 means\n",
      "        the model will also output very unlikely answers, defaults to 0\n",
      "    \"max_tokens\": int - Maximum length of output generated by model , Default: 256\n",
      "    \"api_time_interval\": float - Pause between two api calls in seconds, Default: 1.0\n",
      "    \"warn\": bool - Print warnings preventing excessive api usage, Default: True\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# overview of all available configurations\n",
    "from cot.config import Config as config_overview\n",
    "print(f'\\033[94m {config_overview.__doc__[48:]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining your own template (optional)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default chain of thought generation template as shown above is: \"{instruction}\\n\\n{question}\\n{answer_choices}\\n\\n{cot_trigger}\". </br>\n",
    "You could also define your own template with a different structure and even free text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Answer this question:\\n{question}\\n{answer_choices}\\n\\nGive a detailed explanation of your answer.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you define your custom chain of thought generation template, do not forget to also define a custom answer extraction template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Answer this question:\\n{question}\\n{answer_choices}\\n\\nGive a detailed explanation of your answer.{cot}\\n{answer_extraction}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f57e102",
   "metadata": {},
   "source": [
    "### 2.1 Using a Mock-API to create reasoning chains and extract answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration of the input and parameters of the language model \n",
    "config={\n",
    "    # We compare three different prompts for the chain of thought generation:\n",
    "    # \"Answer: Let's think step by step.\" and 'Answer: We should think about this step by step.', and \"Answer: First,\" \n",
    "    \"cot_trigger_keys\": ['kojima-01','kojima-02', 'kojima-03'],\n",
    "\n",
    "    # We use the same answer extraction prompt for all three prompts\n",
    "    # \"Therefore, among A through D, the answer is\"\n",
    "    \"answer_extraction_keys\": ['kojima-A-D'], \n",
    "    \n",
    "    \"author\" : \"your_name\",\n",
    "    \"api_service\": \"mock_api\", # <--- We use a mock API here for demonstration purposes of the tutorial.\n",
    "    \"engine\": \"\", \n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"verbose\": False,\n",
    "    \"warn\": True,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating reasoning chains and extracting answers in one call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating chains-of-thought and answer extractions\n",
    "worldtree_10.generate(config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above was a fake call to the mock API we now **load a prepared dataset** with real model answers for the purpose of the tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a pre-generated example for the purpose of this tutorial\n",
    "worldtree_10 = Collection.from_json(\"worldtree_10.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f57e102",
   "metadata": {},
   "source": [
    "### 2.2 Using your own API to create reasoning chains and extract answers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ThoughtSource can connect to external AI service providers such as the [OpenAI API](https://openai.com/api/) or the [Hugging Face Hub](https://huggingface.co/docs/hub/index). Set your token, 'api_service' and 'engine' parameters accordingly.\n",
    "\n",
    "In this tutorial we will use the Hugging Face Hub, which is for free. You can make an account and then copy your token from the Hugging Face settings page.\n",
    "\n",
    "To use the API you need to set the environment variable `HUGGINGFACEHUB_API_TOKEN` to your API token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"<token>\"   # <--- set token (can be found in your Hugging Face settings page)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"<token>\"  # <--- Set token for which API you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration of the input and parameters of the language model \n",
    "config={\n",
    "    # We compare three different prompts for the chain of thought generation:\n",
    "    # \"Answer: Let's think step by step.\" and 'Answer: We should think about this step by step.', and \"Answer: First,\" \n",
    "    \"cot_trigger_keys\": ['kojima-01','kojima-02', 'kojima-03'],\n",
    "\n",
    "    # We use the same answer extraction prompt for all three prompts\n",
    "    # \"Therefore, among A through D, the answer is\"\n",
    "    \"answer_extraction_keys\": ['kojima-A-D'], \n",
    "    \n",
    "    \"api_service\": \"huggingface_hub\", # <--- Select which API you want to use\n",
    "    \"engine\": \"google/flan-t5-xl\", # <--- Select which model you want to use\n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"verbose\": False,\n",
    "    \"warn\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading worldtree...\n"
     ]
    }
   ],
   "source": [
    "# Loading just one sample from the dataset so it runs faster\n",
    "worldtree = Collection([\"worldtree\"], verbose=False)\n",
    "worldtree_1 = worldtree.select(split=\"train\", number_samples=1) # just selecting 1 sample, change that if you want to run on more"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a **two step process:**\n",
    " - The language model first answers a question with a detailed reasoning chain.\n",
    " - The language model then extracts the answer from its own reasoning chain.\n",
    "\n",
    "The code does it automatically at once, but it helpful to have in mind the underlying two step process. </br>\n",
    "Therefore you need two API calls for each example as shown in the warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        You are about to \u001b[1m call an external API \u001b[0m in total 6 times, which \u001b[1m may produce costs \u001b[0m.\n",
      "        API calls for reasoning chain generation: 1 samples  * 1 instructions  * 3 reasoning chain triggers\n",
      "        API calls for answer extraction: n_samples  1 samples  * 1 instructions  * 3 reasoning chain triggers * 1 answer extraction triggers \n",
      "        Do you want to continue? y/n\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# Calling the external API will approximately take 30 seconds for this example\n",
    "worldtree_1.generate(config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your cannot press 'y' because your coding environment is not interactive, set \"warn\" to false in the config to deactivate the warning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f57e102",
   "metadata": {},
   "source": [
    "## 3. Evaluation of model answers and downloading all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating worldtree train...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'worldtree': {'train': {'accuracy': {'text-davinci-003': {'None_kojima-01_kojima-A-D': 0.7,\n",
       "     'None_kojima-02_kojima-A-D': 0.7,\n",
       "     'None_kojima-03_kojima-A-D': 0.8}}}}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worldtree_10.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the file, which now includes the dataset and your generated reasoning chains, extracted answers and evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the file that includes all generated chains of thought and answer extractions a the evaluation results\n",
    "worldtree_10.dump(\"worldtree_10.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you used your own API, use this code to evaluate and save the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(worldtree_1.evaluate())\n",
    "# worldtree_1.dump(\"worldtree_1.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f57e102",
   "metadata": {},
   "source": [
    "## 4. Inspect the model outputs in the Web Tool"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is the link: **[ThoughtSource Annotator](http://thought.samwald.info:3000/)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just **upload your just downloaded 'worldtree_10.json' file** to the web tool."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccbfa654f25866afe66a1c016d0b518a994fbe20a93c2d8b432dbe114020e2d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
