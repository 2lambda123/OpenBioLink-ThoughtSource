{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 2,
>>>>>>> first version to give overview, not everything working yet
=======
   "execution_count": 1,
>>>>>>> Adapted explanatory notebooks
   "id": "0b410b12-370e-48f5-9a93-9f7821ed847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cot import Collection\n",
    "from cot.generate import TEMPLATES\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f57e102",
   "metadata": {},
   "source": [
    "# Model Chain of Thought (CoT) Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f043028",
   "metadata": {},
   "source": [
    "## Examples of available templates\n",
    "Three groups of templates for CoT generation:\n",
    "1) Instructions\n",
    "2) CoT-Triggers\n",
    "3) Answer-Extractions"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 3,
>>>>>>> first version to give overview, not everything working yet
=======
   "execution_count": 2,
>>>>>>> Adapted explanatory notebooks
   "id": "63ddbbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> Adapted explanatory notebooks
      "{'qa-01': 'Answer the following question through step-by-step reasoning.',\n",
      " 'qa-02': 'Answer the following question through careful, concise step-by-step '\n",
      "          'reasoning.',\n",
      " 'qa-03': 'Answer the following question through careful, concise step-by-step '\n",
      "          'reasoning. Avoid making up wrong statements. If the question does '\n",
      "          'not make sense or cannot be answered, write \"I cannot answer the '\n",
      "          'question\". If you do not have a good answer, write \"I do not have a '\n",
      "          'good answer\". If you are uncertain, write \"I am uncertain about '\n",
      "          'this\".',\n",
      " 'qa-04': 'Answer the following question through careful, concise step-by-step '\n",
      "          'reasoning. Avoid making up wrong statements. Generate sub-questions '\n",
      "          'that are required to answer the original question, answer them '\n",
      "          'until you can answer the original question. If the question does '\n",
      "          'not make sense or cannot be answered, write \"I cannot answer the '\n",
      "          'question\". If you do not have a good answer, write \"I do not have a '\n",
      "          'good answer\". If you are uncertain, write \"I am uncertain about '\n",
      "          'this\".'}\n"
<<<<<<< HEAD
=======
      "'Answer the following question through step-by-step reasoning.'\n"
>>>>>>> first version to give overview, not everything working yet
=======
>>>>>>> Adapted explanatory notebooks
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "pprint(TEMPLATES[\"instructions\"])"
=======
    "pprint(TEMPLATES[\"instructions\"]['qa-01'])"
>>>>>>> first version to give overview, not everything working yet
=======
    "pprint(TEMPLATES[\"instructions\"])"
>>>>>>> Adapted explanatory notebooks
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 4,
>>>>>>> first version to give overview, not everything working yet
=======
   "execution_count": 3,
>>>>>>> Adapted explanatory notebooks
   "id": "63ddbbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> Adapted explanatory notebooks
      "{'kojima-01': \"Answer: Let's think step by step.\",\n",
      " 'kojima-02': 'Answer: We should think about this step by step.',\n",
      " 'kojima-03': 'Answer: First,',\n",
      " 'kojima-04': 'Answer: Before we dive into the answer,',\n",
      " 'kojima-05': 'Answer: Proof followed by the answer.',\n",
      " 'kojima-06': \"Answer: Let's think step by step in a realistic way.\",\n",
      " 'kojima-07': \"Answer: Let's think step by step using common sense and \"\n",
      "              'knowledge.',\n",
      " 'kojima-08': \"Answer: Let's think like a detective step by step.\",\n",
      " 'kojima-09': \"Answer: Let's think about this logically.\",\n",
      " 'kojima-10': \"Answer: Let's think step by step. First,\",\n",
      " 'kojima-11': \"Answer: Let's think\",\n",
      " 'kojima-12': \"Answer: Let's solve this problem by splitting it into steps.\",\n",
      " 'kojima-13': 'Answer: The answer is after the proof.',\n",
      " 'kojima-14': \"Answer: Let's be realistic and think step by step.\",\n",
      " 'lievin-01': 'Answer: Let’s derive the differential diagnosis step by step.',\n",
      " 'lievin-02': 'Answer: Let’s use step by step inductive reasoning, given the '\n",
      "              'medical nature of the question.',\n",
      " 'lievin-03': 'Answer: Let’s differentiate using step by step reasoning like a '\n",
      "              'medical expert.',\n",
      " 'lievin-04': 'Answer: Let’s think step by step using deductive reasoning.',\n",
      " 'lievin-05': 'Answer: Let’s differentiate using step by step reasoning .',\n",
      " 'lievin-06': 'Answer: Let’s think step by step to arrive at one of the '\n",
      "              'options.',\n",
      " 'lievin-07': 'Answer: Let’s break the problem into multiple steps.',\n",
      " 'lievin-08': 'Answer: Let’s use step by step deductive reasoning, given the '\n",
      "              'medical nature of the question.',\n",
      " 'lievin-09': 'Answer: Let’s think step by step like a doctor.',\n",
      " 'lievin-10': 'Answer: Let’s think step by step like a medical expert.',\n",
      " 'lievin-11': 'Answer: Let’s summarize the facts step by step.',\n",
      " 'lievin-12': 'Answer: Let’s think step by step using inductive reasoning.',\n",
      " 'lievin-13': 'Answer: Let’s think step by step using deductive reasoning like '\n",
      "              'a medical expert.',\n",
      " 'lievin-14': 'Answer: Let’s be concise and think step by step.',\n",
      " 'lievin-15': 'Answer: Let’s differentiate using step by step deductive '\n",
      "              'reasoning like a medical expert.',\n",
      " 'lievin-16': 'Answer: Let’s argue step by step.',\n",
      " 'lievin-17': 'Answer: Let’s think step by step like a clinician.',\n",
      " 'lievin-18': 'Answer: Let’s reflect on each answer option step by step.',\n",
      " 'lievin-19': 'Answer: Let’s reason and differentiate options step by step '\n",
      "              'like a medical expert.',\n",
      " 'lievin-20': 'Answer: Let’s differentiate using step by step inductive '\n",
      "              'reasoning like a medical expert.',\n",
      " 'lievin-21': 'Answer: Let’s think step by step given every option equal '\n",
      "              'consideration.',\n",
      " 'lievin-22': 'Answer: Let’s think step by step like a scientist.',\n",
      " 'lievin-23': 'Answer: Let’s use step by step inductive reasoning.',\n",
      " 'lievin-24': 'Answer: Let’s work by elimination step by step.',\n",
      " 'lievin-25': 'Answer: Let’s use step by step deductive reasoning.',\n",
      " 'lievin-26': 'Answer: Let’s follow a Bayesian step by step approach.',\n",
      " 'lievin-27': 'Answer: Let’s reflect on each option from the least likely to '\n",
      "              'the most likely.',\n",
      " 'lievin-28': 'Answer: Let’s use step by step Bayesian reasoning, given the '\n",
      "              'medical nature of the question.'}\n"
<<<<<<< HEAD
=======
      "\"Answer: Let's think step by step.\"\n"
>>>>>>> first version to give overview, not everything working yet
=======
>>>>>>> Adapted explanatory notebooks
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "pprint(TEMPLATES[\"cot-triggers\"])"
=======
    "pprint(TEMPLATES[\"cot-triggers\"]['kojima-01'])"
>>>>>>> first version to give overview, not everything working yet
=======
    "pprint(TEMPLATES[\"cot-triggers\"])"
>>>>>>> Adapted explanatory notebooks
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 5,
>>>>>>> first version to give overview, not everything working yet
=======
   "execution_count": 4,
>>>>>>> Adapted explanatory notebooks
   "id": "63ddbbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> Adapted explanatory notebooks
      "{'kojima-01': 'Therefore, the answer is',\n",
      " 'kojima-02': 'Therefore,',\n",
      " 'kojima-03': 'The answer is',\n",
      " 'kojima-A-C': 'Therefore, among A through C, the answer is',\n",
      " 'kojima-A-D': 'Therefore, among A through D, the answer is',\n",
      " 'kojima-A-E': 'Therefore, among A through E, the answer is',\n",
      " 'kojima-A-F': 'Therefore, among A through F, the answer is',\n",
      " 'kojima-numerals': 'Therefore, the answer (arabic numerals) is',\n",
      " 'kojima-yes-no': 'Therefore, the answer (Yes or No) is'}\n"
<<<<<<< HEAD
=======
      "'Therefore, the answer is'\n"
>>>>>>> first version to give overview, not everything working yet
=======
>>>>>>> Adapted explanatory notebooks
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "pprint(TEMPLATES[\"answer-extractions\"])"
=======
    "pprint(TEMPLATES[\"answer-extractions\"]['kojima-01'])"
>>>>>>> first version to give overview, not everything working yet
=======
    "pprint(TEMPLATES[\"answer-extractions\"])"
>>>>>>> Adapted explanatory notebooks
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Chain of Thought examples"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 6,
>>>>>>> first version to give overview, not everything working yet
=======
   "execution_count": 5,
>>>>>>> Adapted explanatory notebooks
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading worldtree...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "| Name      |   Train | Valid   | Test   |\n",
       "|-----------|---------|---------|--------|\n",
       "| worldtree |     100 | -       | -      |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'open_book_qa', 'qed', 'strategy_qa', 'svamp']"
      ]
     },
<<<<<<< HEAD
<<<<<<< HEAD
     "execution_count": 5,
=======
     "execution_count": 6,
>>>>>>> first version to give overview, not everything working yet
=======
     "execution_count": 5,
>>>>>>> Adapted explanatory notebooks
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset (detailed explanation in generate_samples notebook)\n",
    "collection = Collection([\"worldtree\"], verbose=False)\n",
    "worldtree_100_random = collection.select(split=\"train\", number_samples=100, random_samples=True, seed=0)\n",
    "worldtree_100_random"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 7,
>>>>>>> first version to give overview, not everything working yet
=======
   "execution_count": 6,
>>>>>>> Adapted explanatory notebooks
   "id": "ea911fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also open an existing json\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "# collection = Collection.from_json(\"worldtree_100_dataset.json\")"
=======
    "# collection = Collection.from_json(\"worldtree_random_100_only_dataset.json\")"
>>>>>>> first version to give overview, not everything working yet
=======
    "# collection = Collection.from_json(\"worldtree_100_dataset.json\")"
>>>>>>> Adapted explanatory notebooks
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of config file"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 8,
>>>>>>> first version to give overview, not everything working yet
=======
   "execution_count": 7,
>>>>>>> Adapted explanatory notebooks
   "id": "0444b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "config={\n",
    "    \"idx_range\": None, # Determines which indices the generate_and_extract routine is applied to, Default: None (All items are used)\n",
    "    \"debug\": True, # Determines whether an api is called or a mock is returned, used for debugging, Default: True (api is not used)\n",
    "    \"instruction_keys\": [\"qa-01\"], # Determines which instructions are used from templates.json, Default: None (All used)\n",
    "    \"cot_trigger_keys\": [\"kojima-01\"], # Determines which cot triggers are used from templates.json, Default: None (All are used)\n",
    "    \"answer_extraction_keys\": [\"kojima-01\"], # Determines which answer extraction prompts are used from templates.json, Default: None (All are used)\n",
    "    \"author\" : \"\", # Name of the person responsible for generation, Default: \"\"\n",
    "    \"api_service\": \"openai\", # Name of the API called (\"openai\", \"huggingface_hub\"), Default: \"openai\"\n",
    "    \"engine\": \"text-davinci-002\", # Name of the engine used (for \"huggingface_hub\" use for example \"google/flan-t5-xl\"), Default: \"text-davinci-002\"\n",
    "    \"temperature\": 0, # Name of the person responsible for generation, Default: 0\n",
    "    \"max_tokens\": 128, # Maximum length of output generated by the model, Default: 128\n",
    "    \"api_time_interval\": 1.0, # Pause between two api calls in seconds, Default: 1.0\n",
    "    \"warn\": False,\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "config={\n",
    "    \"debug\": True,\n",
    "    \"instruction_keys\": [None],\n",
    "    \"cot_trigger_keys\": [\n",
    "        'kojima-01'\n",
    "    ],\n",
    "    \"answer_extraction_keys\": [\n",
    "        'kojima-01'\n",
    "    ],\n",
    "    \"author\" : \"simon\",\n",
    "    \"api_service\": \"openai\",\n",
    "    \"engine\": \"text-davinci-002\",\n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"api_time_interval\": 1.0,\n",
    "    \"verbose\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 9,
>>>>>>> first version to give overview, not everything working yet
=======
   "execution_count": 8,
>>>>>>> Adapted explanatory notebooks
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 100, n_instruction_keys: 1, n_cot_trigger_keys: 1, n_answer_extraction_keys: 1\n",
      "You are about to call the openai API which produces costs.\n",
      "Due to your settings you are about to call the openai API in total 200 times.\n",
      "Number API calls for CoT generation: n_samples * n_instruction_keys * n_cot_trigger_keys\n",
      "Number API calls for answer extraction: n_samples * n_instruction_keys * n_cot_trigger_keys * n_answer_extraction_keys\n",
      "Do you want to continue? y/n\n",
      "\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "# Generating chains of thought and answer extractions (Debug mode, not calling model over API)\n",
=======
    "# Generating chains of thought and answer extractions\n",
>>>>>>> first version to give overview, not everything working yet
=======
    "# Generating chains of thought and answer extractions (Debug mode, not calling model over API)\n",
>>>>>>> Adapted explanatory notebooks
    "worldtree_100_random.generate(name=\"worldtree\", config=config)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: worldtree_dataset/worldtree_thoughtsource\n"
=======
   "execution_count": 11,
=======
   "execution_count": 9,
>>>>>>> Adapted explanatory notebooks
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "n_samples: 100, n_instruction_keys: 1, n_cot_trigger_keys: 1, n_answer_extraction_keys: 1\n",
      "You are about to call the openai API which produces costs.\n",
      "Due to your settings you are about to call the openai API in total 200 times.\n",
      "Number API calls for CoT generation: n_samples * n_instruction_keys * n_cot_trigger_keys\n",
      "Number API calls for answer extraction: n_samples * n_instruction_keys * n_cot_trigger_keys * n_answer_extraction_keys\n",
      "Do you want to continue? y/n\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'None'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m      2\u001b[0m config \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconfig.json\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m worldtree_100_random\u001b[39m.\u001b[39;49mgenerate(config\u001b[39m=\u001b[39;49mconfig)\n",
      "File \u001b[0;32m~/CoT/ThoughtSource/libs/cot/cot/dataloader.py:212\u001b[0m, in \u001b[0;36mCollection.generate\u001b[0;34m(self, name, split, config)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache:\n\u001b[0;32m--> 212\u001b[0m         \u001b[39mself\u001b[39m[name] \u001b[39m=\u001b[39m generate_and_extract(\u001b[39mself\u001b[39;49m[name], config\u001b[39m=\u001b[39;49mconfig)\n\u001b[1;32m    213\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     \u001b[39mif\u001b[39;00m split \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/CoT/ThoughtSource/libs/cot/cot/generate.py:124\u001b[0m, in \u001b[0;36mgenerate_and_extract\u001b[0;34m(data, config)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m data\u001b[39m.\u001b[39;49mmap(_generate_and_extract, with_indices\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, fn_kwargs\u001b[39m=\u001b[39;49mconfig)\n",
      "File \u001b[0;32m~/CoT/ThoughtSource/venv/lib/python3.8/site-packages/datasets/dataset_dict.py:438\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[1;32m    437\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 438\u001b[0m     {\n\u001b[1;32m    439\u001b[0m         k: dataset\u001b[39m.\u001b[39mmap(\n\u001b[1;32m    440\u001b[0m             function\u001b[39m=\u001b[39mfunction,\n\u001b[1;32m    441\u001b[0m             with_indices\u001b[39m=\u001b[39mwith_indices,\n\u001b[1;32m    442\u001b[0m             with_rank\u001b[39m=\u001b[39mwith_rank,\n\u001b[1;32m    443\u001b[0m             input_columns\u001b[39m=\u001b[39minput_columns,\n\u001b[1;32m    444\u001b[0m             batched\u001b[39m=\u001b[39mbatched,\n\u001b[1;32m    445\u001b[0m             batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m    446\u001b[0m             drop_last_batch\u001b[39m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    447\u001b[0m             remove_columns\u001b[39m=\u001b[39mremove_columns,\n\u001b[1;32m    448\u001b[0m             keep_in_memory\u001b[39m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    449\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    450\u001b[0m             cache_file_name\u001b[39m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    451\u001b[0m             writer_batch_size\u001b[39m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    452\u001b[0m             features\u001b[39m=\u001b[39mfeatures,\n\u001b[1;32m    453\u001b[0m             disable_nullable\u001b[39m=\u001b[39mdisable_nullable,\n\u001b[1;32m    454\u001b[0m             fn_kwargs\u001b[39m=\u001b[39mfn_kwargs,\n\u001b[1;32m    455\u001b[0m             num_proc\u001b[39m=\u001b[39mnum_proc,\n\u001b[1;32m    456\u001b[0m             desc\u001b[39m=\u001b[39mdesc,\n\u001b[1;32m    457\u001b[0m         )\n\u001b[1;32m    458\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    459\u001b[0m     }\n\u001b[1;32m    460\u001b[0m )\n",
      "File \u001b[0;32m~/CoT/ThoughtSource/venv/lib/python3.8/site-packages/datasets/dataset_dict.py:439\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[1;32m    437\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    438\u001b[0m     {\n\u001b[0;32m--> 439\u001b[0m         k: dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m    440\u001b[0m             function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m    441\u001b[0m             with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m    442\u001b[0m             with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m    443\u001b[0m             input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m    444\u001b[0m             batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m    445\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    446\u001b[0m             drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m    447\u001b[0m             remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m    448\u001b[0m             keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m    449\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m    450\u001b[0m             cache_file_name\u001b[39m=\u001b[39;49mcache_file_names[k],\n\u001b[1;32m    451\u001b[0m             writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m    452\u001b[0m             features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m    453\u001b[0m             disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m    454\u001b[0m             fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m    455\u001b[0m             num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m    456\u001b[0m             desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m    457\u001b[0m         )\n\u001b[1;32m    458\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    459\u001b[0m     }\n\u001b[1;32m    460\u001b[0m )\n",
      "File \u001b[0;32m~/CoT/ThoughtSource/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py:1955\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   1952\u001b[0m disable_tqdm \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[1;32m   1954\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_proc \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 1955\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_single(\n\u001b[1;32m   1956\u001b[0m         function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m   1957\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m   1958\u001b[0m         with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m   1959\u001b[0m         input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m   1960\u001b[0m         batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m   1961\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   1962\u001b[0m         drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m   1963\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m   1964\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m   1965\u001b[0m         load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m   1966\u001b[0m         cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[1;32m   1967\u001b[0m         writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m   1968\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1969\u001b[0m         disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m   1970\u001b[0m         fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m   1971\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[1;32m   1972\u001b[0m         disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm,\n\u001b[1;32m   1973\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m   1974\u001b[0m     )\n\u001b[1;32m   1975\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1977\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
      "File \u001b[0;32m~/CoT/ThoughtSource/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py:520\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    519\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    521\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    522\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    523\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/CoT/ThoughtSource/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py:487\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    481\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    482\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    483\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    484\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    485\u001b[0m }\n\u001b[1;32m    486\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 487\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    488\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    489\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/CoT/ThoughtSource/venv/lib/python3.8/site-packages/datasets/fingerprint.py:458\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m             kwargs[fingerprint_name] \u001b[39m=\u001b[39m update_fingerprint(\n\u001b[1;32m    453\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fingerprint, transform, kwargs_for_fingerprint\n\u001b[1;32m    454\u001b[0m             )\n\u001b[1;32m    456\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    460\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/CoT/ThoughtSource/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py:2320\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   2318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batched:\n\u001b[1;32m   2319\u001b[0m     \u001b[39mfor\u001b[39;00m i, example \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(pbar):\n\u001b[0;32m-> 2320\u001b[0m         example \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(example, i, offset\u001b[39m=\u001b[39;49moffset)\n\u001b[1;32m   2321\u001b[0m         \u001b[39mif\u001b[39;00m update_data:\n\u001b[1;32m   2322\u001b[0m             \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/CoT/ThoughtSource/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py:2220\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   2218\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   2219\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 2220\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   2221\u001b[0m \u001b[39mif\u001b[39;00m update_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2222\u001b[0m     \u001b[39m# Check if the function returns updated examples\u001b[39;00m\n\u001b[1;32m   2223\u001b[0m     update_data \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(processed_inputs, (Mapping, pa\u001b[39m.\u001b[39mTable))\n",
      "File \u001b[0;32m~/CoT/ThoughtSource/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py:1915\u001b[0m, in \u001b[0;36mDataset.map.<locals>.decorate.<locals>.decorated\u001b[0;34m(item, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1911\u001b[0m decorated_item \u001b[39m=\u001b[39m (\n\u001b[1;32m   1912\u001b[0m     Example(item, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batched \u001b[39melse\u001b[39;00m Batch(item, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures)\n\u001b[1;32m   1913\u001b[0m )\n\u001b[1;32m   1914\u001b[0m \u001b[39m# Use the LazyDict internally, while mapping the function\u001b[39;00m\n\u001b[0;32m-> 1915\u001b[0m result \u001b[39m=\u001b[39m f(decorated_item, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1916\u001b[0m \u001b[39m# Return a standard dict\u001b[39;00m\n\u001b[1;32m   1917\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39mdata \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, LazyDict) \u001b[39melse\u001b[39;00m result\n",
      "File \u001b[0;32m~/CoT/ThoughtSource/libs/cot/cot/generate.py:180\u001b[0m, in \u001b[0;36m_generate_and_extract\u001b[0;34m(item, idx, idx_range, author, api_service, engine, temperature, max_tokens, api_time_interval, instruction_keys, cot_trigger_keys, answer_extraction_keys, debug, warn, verbose)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39mfor\u001b[39;00m instruction_key \u001b[39min\u001b[39;00m instruction_keys:\n\u001b[1;32m    178\u001b[0m     \u001b[39mif\u001b[39;00m instruction_key \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m         instruction_promt \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 180\u001b[0m             TEMPLATES[\u001b[39m\"\u001b[39;49m\u001b[39minstructions\u001b[39;49m\u001b[39m\"\u001b[39;49m][instruction_key] \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m prompt\n\u001b[1;32m    181\u001b[0m         )\n\u001b[1;32m    182\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m         instruction_promt \u001b[39m=\u001b[39m prompt\n",
      "\u001b[0;31mKeyError\u001b[0m: 'None'"
>>>>>>> first version to give overview, not everything working yet
=======
      "No config specified, defaulting to: worldtree_dataset/worldtree_thoughtsource\n"
>>>>>>> Adapted explanatory notebooks
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "# the above was a fake call in debug mode. Now loading prepared dataset with real model answers.\n",
    "worldtree_100_random = Collection.from_json(\"worldtree_100_model.json\")"
=======
    "import json\n",
    "config = json.load(open(\"config.json\"))\n",
    "worldtree_100_random.generate(config=config)"
>>>>>>> first version to give overview, not everything working yet
=======
    "# the above was a fake call in debug mode. Now loading prepared dataset with real model answers.\n",
    "worldtree_100_random = Collection.from_json(\"worldtree_100_model.json\")"
>>>>>>> Adapted explanatory notebooks
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508ffb0",
   "metadata": {},
   "source": [
    "#### Question, choices and right answer"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 79,
>>>>>>> first version to give overview, not everything working yet
=======
   "execution_count": 10,
>>>>>>> Adapted explanatory notebooks
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A parent and a child share several characteristics. Both individuals are '\n",
      " 'tall, have curly hair, are good cooks, and have freckles. Which of these '\n",
      " 'characteristics is a learned behavior?')\n",
      "['being tall', 'having curly hair', 'being a good cook', 'having freckles']\n",
      "['being a good cook']\n"
     ]
    }
   ],
   "source": [
    "pprint(worldtree_100_random[\"worldtree\"][\"train\"][0][\"question\"])\n",
    "pprint(worldtree_100_random[\"worldtree\"][\"train\"][0][\"choices\"])\n",
    "pprint(worldtree_100_random[\"worldtree\"][\"train\"][0][\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508ffb0",
   "metadata": {},
   "source": [
    "#### Model generated chain of thought and extracted answer (correct)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 80,
>>>>>>> first version to give overview, not everything working yet
=======
   "execution_count": 11,
>>>>>>> Adapted explanatory notebooks
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' A parent and child share characteristics because they have the same genes. '\n",
      " 'So, being tall, having curly hair, and having freckles are all '\n",
      " 'characteristics that are passed down from the parent to the child. However, '\n",
      " 'being a good cook is a learned behavior and is not passed down from the '\n",
      " 'parent to the child.')\n",
      "' C) being a good cook.'\n"
     ]
    }
   ],
   "source": [
    "pprint(worldtree_100_random[\"worldtree\"][\"train\"][0][\"generated_cot\"][0][\"cot\"])\n",
    "pprint(worldtree_100_random[\"worldtree\"][\"train\"][0][\"generated_cot\"][0]['answers'][0]['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508ffb0",
   "metadata": {},
   "source": [
    "### Save generated CoTs and extracted answers"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 40,
>>>>>>> first version to give overview, not everything working yet
=======
   "execution_count": 12,
>>>>>>> Adapted explanatory notebooks
   "id": "172d4d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving into collection file, appending model output\n",
    "worldtree_100_random.dump(\"worldtree_100_model.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a19cb823e24c98ee05a9cfa4a3a579b5d56d4c1a735f2a12456750b95a1e155e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
