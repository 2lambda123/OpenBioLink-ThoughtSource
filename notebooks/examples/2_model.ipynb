{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b410b12-370e-48f5-9a93-9f7821ed847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cot import Collection\n",
    "from cot.generate import TEMPLATES\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f57e102",
   "metadata": {},
   "source": [
    "# Model Chain of Thought (CoT) Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f043028",
   "metadata": {},
   "source": [
    "## Examples of available templates\n",
    "Three groups of templates for CoT generation:\n",
    "1) Instructions\n",
    "2) CoT-Triggers\n",
    "3) Answer-Extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63ddbbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Answer the following question through step-by-step reasoning.'\n"
     ]
    }
   ],
   "source": [
    "pprint(TEMPLATES[\"instructions\"]['qa-01'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63ddbbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Answer: Let's think step by step.\"\n"
     ]
    }
   ],
   "source": [
    "pprint(TEMPLATES[\"cot-triggers\"]['kojima-01'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63ddbbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Therefore, the answer is'\n"
     ]
    }
   ],
   "source": [
    "pprint(TEMPLATES[\"answer-extractions\"]['kojima-01'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Chain of Thought examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading worldtree...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "| Name      |   Train | Valid   | Test   |\n",
       "|-----------|---------|---------|--------|\n",
       "| worldtree |     100 | -       | -      |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'open_book_qa', 'qed', 'strategy_qa', 'svamp']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset (detailed explanation in generate_samples notebook)\n",
    "collection = Collection([\"worldtree\"], verbose=False)\n",
    "worldtree_100_random = collection.select(split=\"train\", number_samples=100, random_samples=True, seed=0)\n",
    "worldtree_100_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea911fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also open an existing json\n",
    "# collection = Collection.from_json(\"worldtree_random_100_only_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0444b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "config={\n",
    "    \"idx_range\": None, # Determines which indices the generate_and_extract routine is applied to, Default: None (All items are used)\n",
    "    \"debug\": True, # Determines whether an api is called or a mock is returned, used for debugging, Default: True (api is not used)\n",
    "    \"instruction_keys\": [\"qa-01\"], # Determines which instructions are used from templates.json, Default: None (All used)\n",
    "    \"cot_trigger_keys\": [\"kojima-01\"], # Determines which cot triggers are used from templates.json, Default: None (All are used)\n",
    "    \"answer_extraction_keys\": [\"kojima-01\"], # Determines which answer extraction prompts are used from templates.json, Default: None (All are used)\n",
    "    \"author\" : \"\", # Name of the person responsible for generation, Default: \"\"\n",
    "    \"api_service\": \"openai\", # Name of the API called (\"openai\", \"huggingface_hub\"), Default: \"openai\"\n",
    "    \"engine\": \"text-davinci-002\", # Name of the engine used (for \"huggingface_hub\" use for example \"google/flan-t5-xl\"), Default: \"text-davinci-002\"\n",
    "    \"temperature\": 0, # Name of the person responsible for generation, Default: 0\n",
    "    \"max_tokens\": 128, # Maximum length of output generated by the model, Default: 128\n",
    "    \"api_time_interval\": 1.0, # Pause between two api calls in seconds, Default: 1.0\n",
    "    \"warn\": False,\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "config={\n",
    "    \"debug\": True,\n",
    "    \"instruction_keys\": [None],\n",
    "    \"cot_trigger_keys\": [\n",
    "        'kojima-01'\n",
    "    ],\n",
    "    \"answer_extraction_keys\": [\n",
    "        'kojima-01'\n",
    "    ],\n",
    "    \"author\" : \"simon\",\n",
    "    \"api_service\": \"openai\",\n",
    "    \"engine\": \"text-davinci-002\",\n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"api_time_interval\": 1.0,\n",
    "    \"verbose\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 100, n_instruction_keys: 1, n_cot_trigger_keys: 1, n_answer_extraction_keys: 1\n",
      "You are about to call the openai API which produces costs.\n",
      "Due to your settings you are about to call the openai API in total 200 times.\n",
      "Number API calls for CoT generation: n_samples * n_instruction_keys * n_cot_trigger_keys\n",
      "Number API calls for answer extraction: n_samples * n_instruction_keys * n_cot_trigger_keys * n_answer_extraction_keys\n",
      "Do you want to continue? y/n\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generating chains of thought and answer extractions\n",
    "worldtree_100_random.generate(name=\"worldtree\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 100, n_instruction_keys: 1, n_cot_trigger_keys: 1, n_answer_extraction_keys: 1\n",
      "You are about to call the openai API which produces costs.\n",
      "Due to your settings you are about to call the openai API in total 200 times.\n",
      "Number API calls for CoT generation: n_samples * n_instruction_keys * n_cot_trigger_keys\n",
      "Number API calls for answer extraction: n_samples * n_instruction_keys * n_cot_trigger_keys * n_answer_extraction_keys\n",
      "Do you want to continue? y/n\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'None'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m      2\u001b[0m config \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconfig.json\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m worldtree_100_random\u001b[39m.\u001b[39;49mgenerate(config\u001b[39m=\u001b[39;49mconfig)\n",
      "File \u001b[0;32m~/CoT/ThoughtSource/libs/cot/cot/dataloader.py:212\u001b[0m, in \u001b[0;36mCollection.generate\u001b[0;34m(self, name, split, config)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache:\n\u001b[0;32m--> 212\u001b[0m         \u001b[39mself\u001b[39m[name] \u001b[39m=\u001b[39m generate_and_extract(\u001b[39mself\u001b[39;49m[name], config\u001b[39m=\u001b[39;49mconfig)\n\u001b[1;32m    213\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     \u001b[39mif\u001b[39;00m split \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/CoT/ThoughtSource/libs/cot/cot/generate.py:124\u001b[0m, in \u001b[0;36mgenerate_and_extract\u001b[0;34m(data, config)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m data\u001b[39m.\u001b[39;49mmap(_generate_and_extract, with_indices\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, fn_kwargs\u001b[39m=\u001b[39;49mconfig)\n",
      "File \u001b[0;32m~/CoT/ThoughtSource/venv/lib/python3.8/site-packages/datasets/dataset_dict.py:438\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[1;32m    437\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 438\u001b[0m     {\n\u001b[1;32m    439\u001b[0m         k: dataset\u001b[39m.\u001b[39mmap(\n\u001b[1;32m    440\u001b[0m             function\u001b[39m=\u001b[39mfunction,\n\u001b[1;32m    441\u001b[0m             with_indices\u001b[39m=\u001b[39mwith_indices,\n\u001b[1;32m    442\u001b[0m             with_rank\u001b[39m=\u001b[39mwith_rank,\n\u001b[1;32m    443\u001b[0m             input_columns\u001b[39m=\u001b[39minput_columns,\n\u001b[1;32m    444\u001b[0m             batched\u001b[39m=\u001b[39mbatched,\n\u001b[1;32m    445\u001b[0m             batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m    446\u001b[0m             drop_last_batch\u001b[39m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    447\u001b[0m             remove_columns\u001b[39m=\u001b[39mremove_columns,\n\u001b[1;32m    448\u001b[0m             keep_in_memory\u001b[39m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    449\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    450\u001b[0m             cache_file_name\u001b[39m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    451\u001b[0m             writer_batch_size\u001b[39m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    452\u001b[0m             features\u001b[39m=\u001b[39mfeatures,\n\u001b[1;32m    453\u001b[0m             disable_nullable\u001b[39m=\u001b[39mdisable_nullable,\n\u001b[1;32m    454\u001b[0m             fn_kwargs\u001b[39m=\u001b[39mfn_kwargs,\n\u001b[1;32m    455\u001b[0m             num_proc\u001b[39m=\u001b[39mnum_proc,\n\u001b[1;32m    456\u001b[0m             desc\u001b[39m=\u001b[39mdesc,\n\u001b[1;32m    457\u001b[0m         )\n\u001b[1;32m    458\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    459\u001b[0m     }\n\u001b[1;32m    460\u001b[0m )\n",
      "File \u001b[0;32m~/CoT/ThoughtSource/venv/lib/python3.8/site-packages/datasets/dataset_dict.py:439\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[1;32m    437\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    438\u001b[0m     {\n\u001b[0;32m--> 439\u001b[0m         k: dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m    440\u001b[0m             function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m    441\u001b[0m             with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m    442\u001b[0m             with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m    443\u001b[0m             input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m    444\u001b[0m             batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m    445\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    446\u001b[0m             drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m    447\u001b[0m             remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m    448\u001b[0m             keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m    449\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m    450\u001b[0m             cache_file_name\u001b[39m=\u001b[39;49mcache_file_names[k],\n\u001b[1;32m    451\u001b[0m             writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m    452\u001b[0m             features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m    453\u001b[0m             disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m    454\u001b[0m             fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m    455\u001b[0m             num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m    456\u001b[0m             desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m    457\u001b[0m         )\n\u001b[1;32m    458\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    459\u001b[0m     }\n\u001b[1;32m    460\u001b[0m )\n",
      "File \u001b[0;32m~/CoT/ThoughtSource/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py:1955\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   1952\u001b[0m disable_tqdm \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[1;32m   1954\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_proc \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 1955\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_single(\n\u001b[1;32m   1956\u001b[0m         function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m   1957\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m   1958\u001b[0m         with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m   1959\u001b[0m         input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m   1960\u001b[0m         batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m   1961\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   1962\u001b[0m         drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m   1963\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m   1964\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m   1965\u001b[0m         load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m   1966\u001b[0m         cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[1;32m   1967\u001b[0m         writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m   1968\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1969\u001b[0m         disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m   1970\u001b[0m         fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m   1971\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[1;32m   1972\u001b[0m         disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm,\n\u001b[1;32m   1973\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m   1974\u001b[0m     )\n\u001b[1;32m   1975\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1977\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
      "File \u001b[0;32m~/CoT/ThoughtSource/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py:520\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    519\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    521\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    522\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    523\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/CoT/ThoughtSource/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py:487\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    481\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    482\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    483\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    484\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    485\u001b[0m }\n\u001b[1;32m    486\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 487\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    488\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    489\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/CoT/ThoughtSource/venv/lib/python3.8/site-packages/datasets/fingerprint.py:458\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m             kwargs[fingerprint_name] \u001b[39m=\u001b[39m update_fingerprint(\n\u001b[1;32m    453\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fingerprint, transform, kwargs_for_fingerprint\n\u001b[1;32m    454\u001b[0m             )\n\u001b[1;32m    456\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    460\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/CoT/ThoughtSource/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py:2320\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   2318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batched:\n\u001b[1;32m   2319\u001b[0m     \u001b[39mfor\u001b[39;00m i, example \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(pbar):\n\u001b[0;32m-> 2320\u001b[0m         example \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(example, i, offset\u001b[39m=\u001b[39;49moffset)\n\u001b[1;32m   2321\u001b[0m         \u001b[39mif\u001b[39;00m update_data:\n\u001b[1;32m   2322\u001b[0m             \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/CoT/ThoughtSource/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py:2220\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   2218\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   2219\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 2220\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   2221\u001b[0m \u001b[39mif\u001b[39;00m update_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2222\u001b[0m     \u001b[39m# Check if the function returns updated examples\u001b[39;00m\n\u001b[1;32m   2223\u001b[0m     update_data \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(processed_inputs, (Mapping, pa\u001b[39m.\u001b[39mTable))\n",
      "File \u001b[0;32m~/CoT/ThoughtSource/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py:1915\u001b[0m, in \u001b[0;36mDataset.map.<locals>.decorate.<locals>.decorated\u001b[0;34m(item, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1911\u001b[0m decorated_item \u001b[39m=\u001b[39m (\n\u001b[1;32m   1912\u001b[0m     Example(item, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batched \u001b[39melse\u001b[39;00m Batch(item, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures)\n\u001b[1;32m   1913\u001b[0m )\n\u001b[1;32m   1914\u001b[0m \u001b[39m# Use the LazyDict internally, while mapping the function\u001b[39;00m\n\u001b[0;32m-> 1915\u001b[0m result \u001b[39m=\u001b[39m f(decorated_item, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1916\u001b[0m \u001b[39m# Return a standard dict\u001b[39;00m\n\u001b[1;32m   1917\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39mdata \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, LazyDict) \u001b[39melse\u001b[39;00m result\n",
      "File \u001b[0;32m~/CoT/ThoughtSource/libs/cot/cot/generate.py:180\u001b[0m, in \u001b[0;36m_generate_and_extract\u001b[0;34m(item, idx, idx_range, author, api_service, engine, temperature, max_tokens, api_time_interval, instruction_keys, cot_trigger_keys, answer_extraction_keys, debug, warn, verbose)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39mfor\u001b[39;00m instruction_key \u001b[39min\u001b[39;00m instruction_keys:\n\u001b[1;32m    178\u001b[0m     \u001b[39mif\u001b[39;00m instruction_key \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m         instruction_promt \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 180\u001b[0m             TEMPLATES[\u001b[39m\"\u001b[39;49m\u001b[39minstructions\u001b[39;49m\u001b[39m\"\u001b[39;49m][instruction_key] \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m prompt\n\u001b[1;32m    181\u001b[0m         )\n\u001b[1;32m    182\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m         instruction_promt \u001b[39m=\u001b[39m prompt\n",
      "\u001b[0;31mKeyError\u001b[0m: 'None'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "config = json.load(open(\"config.json\"))\n",
    "worldtree_100_random.generate(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508ffb0",
   "metadata": {},
   "source": [
    "#### Question, choices and right answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A parent and a child share several characteristics. Both individuals are '\n",
      " 'tall, have curly hair, are good cooks, and have freckles. Which of these '\n",
      " 'characteristics is a learned behavior?')\n",
      "['being tall', 'having curly hair', 'being a good cook', 'having freckles']\n",
      "['being a good cook']\n"
     ]
    }
   ],
   "source": [
    "pprint(worldtree_100_random[\"worldtree\"][\"train\"][0][\"question\"])\n",
    "pprint(worldtree_100_random[\"worldtree\"][\"train\"][0][\"choices\"])\n",
    "pprint(worldtree_100_random[\"worldtree\"][\"train\"][0][\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508ffb0",
   "metadata": {},
   "source": [
    "#### Model generated chain of thought and extracted answer (correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' A parent and child share characteristics because they have the same genes. '\n",
      " 'So, being tall, having curly hair, and having freckles are all '\n",
      " 'characteristics that are passed down from the parent to the child. However, '\n",
      " 'being a good cook is a learned behavior and is not passed down from the '\n",
      " 'parent to the child.')\n",
      "' C) being a good cook.'\n"
     ]
    }
   ],
   "source": [
    "pprint(worldtree_100_random[\"worldtree\"][\"train\"][0][\"generated_cot\"][0][\"cot\"])\n",
    "pprint(worldtree_100_random[\"worldtree\"][\"train\"][0][\"generated_cot\"][0]['answers'][0]['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508ffb0",
   "metadata": {},
   "source": [
    "### Save generated CoTs and extracted answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "172d4d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving into collection file, appending model output\n",
    "worldtree_100_random.dump(\"worldtree_100_model.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a19cb823e24c98ee05a9cfa4a3a579b5d56d4c1a735f2a12456750b95a1e155e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
