{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b410b12-370e-48f5-9a93-9f7821ed847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cot import Collection\n",
    "from cot.generate import TEMPLATES\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f57e102",
   "metadata": {},
   "source": [
    "# Model Chain of Thought (CoT) Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f043028",
   "metadata": {},
   "source": [
    "## Examples of available templates\n",
    "Three groups of templates for CoT generation:\n",
    "1) Instructions\n",
    "2) cot_triggers\n",
    "3) answer_extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63ddbbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'qa-01': 'Answer the following question through step-by-step reasoning.',\n",
      " 'qa-02': 'Answer the following question through careful, concise step-by-step '\n",
      "          'reasoning.',\n",
      " 'qa-03': 'Answer the following question through careful, concise step-by-step '\n",
      "          'reasoning. Avoid making up wrong statements. If the question does '\n",
      "          'not make sense or cannot be answered, write \"I cannot answer the '\n",
      "          'question\". If you do not have a good answer, write \"I do not have a '\n",
      "          'good answer\". If you are uncertain, write \"I am uncertain about '\n",
      "          'this\".',\n",
      " 'qa-04': 'Answer the following question through careful, concise step-by-step '\n",
      "          'reasoning. Avoid making up wrong statements. Generate sub-questions '\n",
      "          'that are required to answer the original question, answer them '\n",
      "          'until you can answer the original question. If the question does '\n",
      "          'not make sense or cannot be answered, write \"I cannot answer the '\n",
      "          'question\". If you do not have a good answer, write \"I do not have a '\n",
      "          'good answer\". If you are uncertain, write \"I am uncertain about '\n",
      "          'this\".'}\n"
     ]
    }
   ],
   "source": [
    "pprint(TEMPLATES[\"instructions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63ddbbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kojima-01': \"Answer: Let's think step by step.\",\n",
      " 'kojima-02': 'Answer: We should think about this step by step.',\n",
      " 'kojima-03': 'Answer: First,',\n",
      " 'kojima-04': 'Answer: Before we dive into the answer,',\n",
      " 'kojima-05': 'Answer: Proof followed by the answer.',\n",
      " 'kojima-06': \"Answer: Let's think step by step in a realistic way.\",\n",
      " 'kojima-07': \"Answer: Let's think step by step using common sense and \"\n",
      "              'knowledge.',\n",
      " 'kojima-08': \"Answer: Let's think like a detective step by step.\",\n",
      " 'kojima-09': \"Answer: Let's think about this logically.\",\n",
      " 'kojima-10': \"Answer: Let's think step by step. First,\",\n",
      " 'kojima-11': \"Answer: Let's think\",\n",
      " 'kojima-12': \"Answer: Let's solve this problem by splitting it into steps.\",\n",
      " 'kojima-13': 'Answer: The answer is after the proof.',\n",
      " 'kojima-14': \"Answer: Let's be realistic and think step by step.\",\n",
      " 'lievin-01': 'Answer: Let’s derive the differential diagnosis step by step.',\n",
      " 'lievin-02': 'Answer: Let’s use step by step inductive reasoning, given the '\n",
      "              'medical nature of the question.',\n",
      " 'lievin-03': 'Answer: Let’s differentiate using step by step reasoning like a '\n",
      "              'medical expert.',\n",
      " 'lievin-04': 'Answer: Let’s think step by step using deductive reasoning.',\n",
      " 'lievin-05': 'Answer: Let’s differentiate using step by step reasoning .',\n",
      " 'lievin-06': 'Answer: Let’s think step by step to arrive at one of the '\n",
      "              'options.',\n",
      " 'lievin-07': 'Answer: Let’s break the problem into multiple steps.',\n",
      " 'lievin-08': 'Answer: Let’s use step by step deductive reasoning, given the '\n",
      "              'medical nature of the question.',\n",
      " 'lievin-09': 'Answer: Let’s think step by step like a doctor.',\n",
      " 'lievin-10': 'Answer: Let’s think step by step like a medical expert.',\n",
      " 'lievin-11': 'Answer: Let’s summarize the facts step by step.',\n",
      " 'lievin-12': 'Answer: Let’s think step by step using inductive reasoning.',\n",
      " 'lievin-13': 'Answer: Let’s think step by step using deductive reasoning like '\n",
      "              'a medical expert.',\n",
      " 'lievin-14': 'Answer: Let’s be concise and think step by step.',\n",
      " 'lievin-15': 'Answer: Let’s differentiate using step by step deductive '\n",
      "              'reasoning like a medical expert.',\n",
      " 'lievin-16': 'Answer: Let’s argue step by step.',\n",
      " 'lievin-17': 'Answer: Let’s think step by step like a clinician.',\n",
      " 'lievin-18': 'Answer: Let’s reflect on each answer option step by step.',\n",
      " 'lievin-19': 'Answer: Let’s reason and differentiate options step by step '\n",
      "              'like a medical expert.',\n",
      " 'lievin-20': 'Answer: Let’s differentiate using step by step inductive '\n",
      "              'reasoning like a medical expert.',\n",
      " 'lievin-21': 'Answer: Let’s think step by step given every option equal '\n",
      "              'consideration.',\n",
      " 'lievin-22': 'Answer: Let’s think step by step like a scientist.',\n",
      " 'lievin-23': 'Answer: Let’s use step by step inductive reasoning.',\n",
      " 'lievin-24': 'Answer: Let’s work by elimination step by step.',\n",
      " 'lievin-25': 'Answer: Let’s use step by step deductive reasoning.',\n",
      " 'lievin-26': 'Answer: Let’s follow a Bayesian step by step approach.',\n",
      " 'lievin-27': 'Answer: Let’s reflect on each option from the least likely to '\n",
      "              'the most likely.',\n",
      " 'lievin-28': 'Answer: Let’s use step by step Bayesian reasoning, given the '\n",
      "              'medical nature of the question.'}\n"
     ]
    }
   ],
   "source": [
    "pprint(TEMPLATES[\"cot_triggers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63ddbbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kojima-01': 'Therefore, the answer is',\n",
      " 'kojima-02': 'Therefore,',\n",
      " 'kojima-03': 'The answer is',\n",
      " 'kojima-A-C': 'Therefore, among A through C, the answer is',\n",
      " 'kojima-A-D': 'Therefore, among A through D, the answer is',\n",
      " 'kojima-A-E': 'Therefore, among A through E, the answer is',\n",
      " 'kojima-A-F': 'Therefore, among A through F, the answer is',\n",
      " 'kojima-numerals': 'Therefore, the answer (arabic numerals) is',\n",
      " 'kojima-yes-no': 'Therefore, the answer (Yes or No) is'}\n"
     ]
    }
   ],
   "source": [
    "pprint(TEMPLATES[\"answer_extractions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Chain of Thought examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading worldtree...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "| Name      |   Train | Valid   | Test   |\n",
       "|-----------|---------|---------|--------|\n",
       "| worldtree |     100 | -       | -      |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'open_book_qa', 'qed', 'strategy_qa', 'svamp']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset (detailed explanation in generate_samples notebook)\n",
    "collection = Collection([\"worldtree\"], verbose=False)\n",
    "worldtree_100_random = collection.select(split=\"train\", number_samples=100, random_samples=True, seed=0)\n",
    "worldtree_100_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea911fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also open an existing json\n",
    "# collection = Collection.from_json(\"worldtree_100_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0444b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "config={\n",
    "    \"idx_range\": None, # Determines which indices the generate_and_extract routine is applied to, Default: None (All items are used)\n",
    "    \"debug\": True, # Determines whether an api is called or a mock is returned, used for debugging, Default: True (api is not used)\n",
    "    \"instruction_keys\": [\"qa-01\"], # Determines which instructions are used from templates.json, Default: None (All used)\n",
    "    \"cot_trigger_keys\": [\"kojima-01\"], # Determines which cot triggers are used from templates.json, Default: None (All are used)\n",
    "    \"answer_extraction_keys\": [\"kojima-01\"], # Determines which answer extraction prompts are used from templates.json, Default: None (All are used)\n",
    "    \"author\" : \"\", # Name of the person responsible for generation, Default: \"\"\n",
    "    \"api_service\": \"openai\", # Name of the API called (\"openai\", \"huggingface_hub\"), Default: \"openai\"\n",
    "    \"engine\": \"text-davinci-002\", # Name of the engine used (for \"huggingface_hub\" use for example \"google/flan-t5-xl\"), Default: \"text-davinci-002\"\n",
    "    \"temperature\": 0, # Name of the person responsible for generation, Default: 0\n",
    "    \"max_tokens\": 128, # Maximum length of output generated by the model, Default: 128\n",
    "    \"api_time_interval\": 1.0, # Pause between two api calls in seconds, Default: 1.0\n",
    "    \"warn\": False,\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "config={\n",
    "    \"debug\": True,\n",
    "    \"instruction_keys\": [\n",
    "        'qa-01'\n",
    "        ],\n",
    "    \"cot_trigger_keys\": [\n",
    "        'kojima-01'\n",
    "    ],\n",
    "    \"answer_extraction_keys\": [\n",
    "        'kojima-A-D'\n",
    "    ],\n",
    "    \"author\" : \"konstantin\",\n",
    "    \"api_service\": \"huggingface_hub\",\n",
    "    \"engine\": \"google/flan-t5-xl\",\n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"api_time_interval\": 1.0,\n",
    "    \"verbose\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'template_output_generation'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"template_output_generation\"\n",
    "\"template_answer_extraction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            You are about to \u001b[1m call an external API \u001b[0m in total 200 times, which \u001b[1m may produce costs \u001b[0m.\n",
      "            Number API calls for CoT generation: n_samples 100 * n_instruction_keys 1 * n_cot_trigger_keys 1\n",
      "            Number API calls for answer extraction: n_samples 100 * n_instruction_keys 1 * n_cot_trigger_keys 1 * n_answer_extraction_keys 1\n",
      "            Do you want to continue? y/n\n",
      "            \u001b[1m Note: You are in debug mode. When entering 'y', a test run without API calls is made. \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Generating chains of thought and answer extractions (Debug mode, not calling model over API)\n",
    "worldtree_100_random.generate(name=\"worldtree\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: worldtree_dataset/worldtree_thoughtsource\n"
     ]
    }
   ],
   "source": [
    "# the above was a fake call in debug mode. Now loading prepared dataset with real model answers.\n",
    "worldtree_100_random = Collection.from_json(\"worldtree_100_model.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508ffb0",
   "metadata": {},
   "source": [
    "#### Question, choices and right answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A parent and a child share several characteristics. Both individuals are '\n",
      " 'tall, have curly hair, are good cooks, and have freckles. Which of these '\n",
      " 'characteristics is a learned behavior?')\n",
      "['being tall', 'having curly hair', 'being a good cook', 'having freckles']\n",
      "['being a good cook']\n"
     ]
    }
   ],
   "source": [
    "pprint(worldtree_100_random[\"worldtree\"][\"train\"][0][\"question\"])\n",
    "pprint(worldtree_100_random[\"worldtree\"][\"train\"][0][\"choices\"])\n",
    "pprint(worldtree_100_random[\"worldtree\"][\"train\"][0][\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508ffb0",
   "metadata": {},
   "source": [
    "#### Model generated chain of thought and extracted answer (correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Being a good cook is a learned behavior. The parent and the child share '\n",
      " 'several characteristics. So, the answer is C.')\n",
      "'C.'\n"
     ]
    }
   ],
   "source": [
    "pprint(worldtree_100_random[\"worldtree\"][\"train\"][0][\"generated_cot\"][0][\"cot\"])\n",
    "pprint(worldtree_100_random[\"worldtree\"][\"train\"][0][\"generated_cot\"][0]['answers'][0]['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508ffb0",
   "metadata": {},
   "source": [
    "### Save generated CoTs and extracted answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "172d4d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving into collection file, appending model output\n",
    "worldtree_100_random.dump(\"worldtree_100_model.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a19cb823e24c98ee05a9cfa4a3a579b5d56d4c1a735f2a12456750b95a1e155e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
