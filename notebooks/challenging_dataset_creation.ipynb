{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commonsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/robertpraas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from cot import Collection\n",
    "coll = Collection.load_thoughtsource_100(load_pregenerated_cots=True)\n",
    "coll.select_generated_cots(cot_trigger = \"kojima-01\", api_service='openai',model = 'text-davinci-003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1b29d402c3e17cb3b435',\n",
       " 'ref_id': '',\n",
       " 'question': 'Is a pound sterling valuable?',\n",
       " 'type': 'bool',\n",
       " 'choices': ['True', 'False'],\n",
       " 'context': '',\n",
       " 'cot': ['A pound sterling is fiat money.',\n",
       "  'Fiat money is backed by government decree and has no intrinsic value.',\n",
       "  'One pound sterling is worth about 1.24 US dollars by May of 2020.'],\n",
       " 'answer': ['False'],\n",
       " 'generated_cot': [],\n",
       " 'feedback': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll['strategy_qa']['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "| Name           | Train   |   Valid | Test   |\n",
       "|----------------|---------|---------|--------|\n",
       "| commonsense_qa | -       |     100 | -      |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa', 'medmc_qa', 'open_book_qa', 'pubmed_qa', 'qed', 'strategy_qa', 'svamp', 'worldtree']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cot import Collection\n",
    "coll = Collection.load_thoughtsource_100(load_pregenerated_cots=True)\n",
    "coll.select_generated_cots(cot_trigger = \"kojima-01\", api_service='openai',model = 'text-davinci-003')\n",
    "coll.select_generated_cots(answers=\"False\")\n",
    "coll.unload_datasets([\"med_qa\",\"medmc_qa\",\"strategy_qa\",'open_book_qa','worldtree'])\n",
    "coll\n",
    "#coll.dump(\"cs_davinci.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Went through Annotator\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "| Name           | Train   |   Valid | Test   |\n",
       "|----------------|---------|---------|--------|\n",
       "| commonsense_qa | -       |     100 | -      |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa', 'medmc_qa', 'open_book_qa', 'pubmed_qa', 'qed', 'strategy_qa', 'svamp', 'worldtree']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated = Collection.from_json(\"cs_davinci_annotated.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Moved to annotated files folder\"\"\"\n",
    "\n",
    "ann_1 = Collection.from_json(\"annotated_ob_davinci.json\") #open book\n",
    "ann_2 = Collection.from_json(\"annotated_st_davinci.json\") #strategu\n",
    "ann_3 = Collection.from_json(\"annotated_wt_davinci.json\")\n",
    "ann_4 = Collection.from_json(\"cs_davinci_annotated.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_annotated(dataset,split,selection):\n",
    "    i= 0\n",
    "    lst = []\n",
    "    for element in selection[dataset][split]:\n",
    "        if len(element['generated_cot'])>0:\n",
    "            if len(element['generated_cot'][0]['annotations'])>0 :\n",
    "                lst.append(i)\n",
    "        i+=1\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_1 = Collection.from_json(\"annotated_ob_davinci.json\") #open book\n",
    "ann_2 = Collection.from_json(\"annotated_st_davinci.json\") #strategu\n",
    "ann_3 = Collection.from_json(\"annotated_wt_davinci.json\")\n",
    "ann_4 = Collection.from_json(\"cs_davinci_annotated.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_1['open_book_qa']['test'] = ann_1['open_book_qa']['test'].select(select_annotated('open_book_qa','test',ann_1))\n",
    "ann_2['strategy_qa']['train'] = ann_2['strategy_qa']['train'].select(select_annotated('strategy_qa','train',ann_2))\n",
    "ann_3['worldtree']['test'] = ann_3['worldtree']['test'].select(select_annotated('worldtree','test',ann_3))\n",
    "ann_4['commonsense_qa']['validation'] = ann_4['commonsense_qa']['validation'].select(select_annotated('commonsense_qa','validation',ann_4))\n",
    " annotated_set = ann_4.merge(ann_3).merge(ann_2).merge(ann_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_set.dump('annotated_set.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get gpt-4 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cot import Collection\n",
    "col = Collection.from_json(\"../notebooks/thoughtsource_100_openai_chat_gpt-4_None_zhou-01.json\")\n",
    "col.select_generated_cots(api_service='openai_chat',model = 'gpt-4')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'b8c0a4703079cf661d7261a60a1bcbff',\n",
       " 'ref_id': '',\n",
       " 'question': 'Where would you find magazines along side many other printed works?',\n",
       " 'type': 'multiplechoice',\n",
       " 'choices': ['doctor', 'bookstore', 'market', 'train station', 'mortuary'],\n",
       " 'context': '',\n",
       " 'cot': ['If one wants various printed reading material, they can go to a bookstore.',\n",
       "  'All other options are invalid as they do not have wide range of printed reading material.'],\n",
       " 'answer': ['bookstore'],\n",
       " 'generated_cot': [{'id': 'e6c02977-8516-484c-849a-3fe640a1aa3b',\n",
       "   'fragments_version': '0.01',\n",
       "   'instruction': None,\n",
       "   'cot_trigger': 'zhou-01',\n",
       "   'cot_trigger_template': '{instruction}\\n\\n{question}\\n{answer_choices}\\n\\n{cot_trigger}',\n",
       "   'prompt_text': '',\n",
       "   'cot': 'A) doctor - Doctors may have magazines in their waiting rooms, but not many other printed works.\\nB) bookstore - Bookstores have a wide variety of printed works, including magazines, books, newspapers, and more.\\nC) market - Markets may have some magazines, but not a wide variety of printed works.\\nD) train station - Train stations may have some magazines at newsstands, but not many other printed works.\\nE) mortuary - Mortuaries are unlikely to have magazines or other printed works.\\n\\nBased on this analysis, the best answer is B) bookstore.',\n",
       "   'answers': [{'id': 'de2feb10-3235-4d1d-97fa-bf714e87c9d3',\n",
       "     'answer_extraction': 'kojima-A-E',\n",
       "     'answer_extraction_template': '{instruction}\\n\\n{question}\\n{answer_choices}\\n\\n{cot_trigger}{cot}\\n{answer_extraction}',\n",
       "     'answer_extraction_text': '',\n",
       "     'answer': 'B) bookstore.',\n",
       "     'correct_answer': True}],\n",
       "   'author': 'thoughtsource',\n",
       "   'date': '2023/03/17 10:01:02',\n",
       "   'api_service': 'openai_chat',\n",
       "   'model': \"{'name': 'gpt-4', 'temperature': 0, 'max_tokens': 512}\",\n",
       "   'comment': '',\n",
       "   'annotations': []}],\n",
       " 'feedback': []}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col['commonsense_qa']['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at annotated set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "| Name           | Train   | Valid   | Test   |\n",
       "|----------------|---------|---------|--------|\n",
       "| commonsense_qa | -       | 11      | -      |\n",
       "| worldtree      | -       | -       | 6      |\n",
       "| strategy_qa    | 9       | -       | -      |\n",
       "| open_book_qa   | -       | -       | 10     |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa', 'medmc_qa', 'pubmed_qa', 'qed', 'svamp']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cot import Collection\n",
    "ann_col = Collection.from_json(\"../notebooks/internal_documentation/annotated_files/annotated_set.json\")\n",
    "ann_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = ann_col['commonsense_qa']['validation']['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for item in col['commonsense_qa']['validation']:\n",
    "    if item['id'] in id_list:\n",
    "        print(item['generated_cot'][0]['answers'][0]['correct_answer'])\n",
    "\n",
    "# GPT-4 7/11 correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list_2 = ann_col['worldtree']['test']['id']\n",
    "id_list_3 = ann_col['strategy_qa']['train']['id']\n",
    "id_list_4 = ann_col['open_book_qa']['test']['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for item in col['worldtree']['test']:\n",
    "    if item['id'] in id_list_2:\n",
    "        print(item['generated_cot'][0]['answers'][0]['correct_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for item in col['strategy_qa']['train']:\n",
    "    if item['id'] in id_list_3:\n",
    "        print(item['generated_cot'][0]['answers'][0]['correct_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for item in col['open_book_qa']['test']:\n",
    "    if item['id'] in id_list_4:\n",
    "        print(item['generated_cot'][0]['answers'][0]['correct_answer'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Medical dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cot import Collection\n",
    "coll = Collection.load_thoughtsource_100(load_pregenerated_cots=True)\n",
    "coll.select_generated_cots(cot_trigger = \"kojima-01\", api_service='openai',model = 'text-davinci-003')\n",
    "coll.select_generated_cots(answers=\"False\")\n",
    "coll.unload_datasets([\"commonsense_qa\",\"strategy_qa\",'open_book_qa','worldtree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "| Name     | Train   | Valid   | Test   |\n",
       "|----------|---------|---------|--------|\n",
       "| med_qa   | -       | -       | 100    |\n",
       "| medmc_qa | -       | 100     | -      |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'open_book_qa', 'pubmed_qa', 'qed', 'strategy_qa', 'svamp', 'worldtree']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for item in coll['med_qa']['test']:\n",
    "    if len(item['generated_cot'])>0:\n",
    "        i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for item in coll['medmc_qa']['validation']:\n",
    "    if len(item['generated_cot'])>0:\n",
    "        i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll.dump('medical_incorrect.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_1 = Collection.from_json(\"../notebooks/internal_documentation/newly_annotated/annotated_ob_davinci.json\") #open book\n",
    "ann_2 = Collection.from_json(\"../notebooks/internal_documentation/newly_annotated/annotated_st_davinci.json\") #strategu\n",
    "ann_3 = Collection.from_json(\"../notebooks/internal_documentation/newly_annotated/annotated_wt_davinci.json\")\n",
    "ann_4 = Collection.from_json(\"../notebooks/internal_documentation/newly_annotated/cs_davinci_annotated.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_1['open_book_qa']['test'] = ann_1['open_book_qa']['test'].select(select_annotated('open_book_qa','test',ann_1))\n",
    "ann_2['strategy_qa']['train'] = ann_2['strategy_qa']['train'].select(select_annotated('strategy_qa','train',ann_2))\n",
    "ann_3['worldtree']['test'] = ann_3['worldtree']['test'].select(select_annotated('worldtree','test',ann_3))\n",
    "ann_4['commonsense_qa']['validation'] = ann_4['commonsense_qa']['validation'].select(select_annotated('commonsense_qa','validation',ann_4))\n",
    "annotated_set = ann_4.merge(ann_3).merge(ann_2).merge(ann_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "| Name           | Train   | Valid   | Test   |\n",
       "|----------------|---------|---------|--------|\n",
       "| commonsense_qa | -       | 12      | -      |\n",
       "| worldtree      | -       | -       | 7      |\n",
       "| strategy_qa    | 11      | -       | -      |\n",
       "| open_book_qa   | -       | -       | 19     |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa', 'medmc_qa', 'pubmed_qa', 'qed', 'svamp']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
