{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Imports\n",
    "Please follow the installation guide in the [ThoughtSource Readme file](https://github.com/OpenBioLink/ThoughtSource) before using this notebook.\n",
    "\n",
    "After cloning the repo, install the library linking to your local location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e ../libs/cot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from cot import Collection\n",
    "from cot.generate import FRAGMENTS\n",
    "from pprint import pprint\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick intro\n",
    "The ThoughtSource library offers functionality for: \n",
    "* Loading datasets\n",
    "* Creating random sub-samples\n",
    "* Generating novel chain-of-thought reasoning data and answers by connecting to external AI services\n",
    "* Evaluating results\n",
    "\n",
    "Below we will give a quick intro to the libary, followed by more detailed examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading worldtree...\n",
      "None\n",
      "{'accuracy': {'qa-01_kojima-01_kojima-A-D': 0.6}}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 1) Dataset loading and selecting a random sample\n",
    "collection = Collection([\"worldtree\"], verbose=False)\n",
    "collection = collection.select(split=\"train\", number_samples=10)\n",
    "\n",
    "\n",
    "# 2) Language Model generates chains of thought and then extracts answers\n",
    "\n",
    "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"<token>\"   # <--- set token (can be found in your Hugging Face settings page)\n",
    "\n",
    "config={\n",
    "    \"instruction_keys\": ['qa-01'], # \"Answer the following question through step-by-step reasoning.\"\n",
    "    \"cot_trigger_keys\": ['kojima-01'], # \"Answer: Let's think step by step.\"\n",
    "    \"answer_extraction_keys\": ['kojima-A-D'], # \"Therefore, among A through D, the answer is\"\n",
    "    \"api_service\": \"huggingface_hub\",\n",
    "    \"engine\": \"google/flan-t5-xl\",\n",
    "    \"warn\": False,\n",
    "    \"verbose\": False,\n",
    "}\n",
    "pprint(collection.generate(config=config))\n",
    "\n",
    "# 3) Evaluating answers generated by the model\n",
    "pprint(collection.evaluate())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading, sampling and saving a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading worldtree...\n",
      "| Name      |   Train |   Valid |   Test |\n",
      "|-----------|---------|---------|--------|\n",
      "| worldtree |    2207 |     496 |   1664 |\n",
      "\n",
      "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa', 'medmc_qa', 'open_book_qa', 'pubmed_qa', 'qed', 'strategy_qa', 'svamp']\n"
     ]
    }
   ],
   "source": [
    "# load a dataset to sample from \n",
    "collection_worldtree = Collection([\"worldtree\"], verbose=False)\n",
    "print(collection_worldtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "| Name      |   Train | Valid   | Test   |\n",
       "|-----------|---------|---------|--------|\n",
       "| worldtree |     100 | -       | -      |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa', 'medmc_qa', 'open_book_qa', 'pubmed_qa', 'qed', 'strategy_qa', 'svamp']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly select 100 rows from train split\n",
    "collection_worldtree_100 = collection_worldtree.select(split=\"train\", number_samples=100, random_samples=True, seed=0)\n",
    "collection_worldtree_100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that you can also sample from all datasets in a collection like this like this: \n",
    "# collection_train_random_100 = collection.select(split=\"train\", number_samples=100, random_samples=True, seed=0)\n",
    "\n",
    "# Write to JSON file (will be save to the same folder as this notebook)\n",
    "collection_worldtree_100.dump(\"worldtree_100_dataset.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f57e102",
   "metadata": {},
   "source": [
    "## 2. Generating novel reasoning chains and answers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f043028",
   "metadata": {},
   "source": [
    "ThoughtSource comes pre-loaded with a large [collection of text snippets ('prompt fragments')](https://github.com/OpenBioLink/ThoughtSource/blob/main/libs/cot/cot/fragments.json) to elicit chain-of-thought reasoning in large language models and to extract answers from chains-of-thought. Let's see how prompt fragments look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63ddbbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    \"kojima-01\",\n",
      "    \"Answer: Let's think step by step.\"\n",
      "  ],\n",
      "  [\n",
      "    \"kojima-02\",\n",
      "    \"Answer: We should think about this step by step.\"\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Show first two cot_trigger prompts\n",
    "print(json.dumps(list(FRAGMENTS[\"cot_triggers\"].items())[:2], sort_keys=True, indent=2)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating chain-of-thought examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ThoughtSource can connect to external AI service providers such as the [OpenAI API](https://openai.com/api/) or the [Hugging Face Hub](https://huggingface.co/docs/hub/index). Set your token, 'api_service' and 'engine' parameters accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading worldtree...\n"
     ]
    }
   ],
   "source": [
    "# Sample 100 items from the Worldtree v2 dataset\n",
    "collection = Collection([\"worldtree\"], verbose=False)\n",
    "worldtree_100_random = collection.select(split=\"train\", number_samples=100, random_samples=True, seed=0)\n",
    "\n",
    "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"<token>\"  # <--- SET ACCORDINGLY\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"<token>\"  # <--- SET ACCORDINGLY\n",
    "\n",
    "# Configuration for calling AI service. \n",
    "config={\n",
    "    \"idx_range\": \"all\", # Determines which indices the generate_and_extract routine is applied to, Default: \"all\" (All items are used)\n",
    "    \"instruction_keys\": [\"qa-01\"], # Determines which instructions are used from fragments.json, Default: None (no instructions are used)\n",
    "    \"cot_trigger_keys\": [\"kojima-01\"], # Determines which cot triggers are used from fragments.json, Default: [\"kojima-01\"] (only the first trigger is used)\n",
    "    \"answer_extraction_keys\": [\"kojima-A-D\"], # Determines which answer extraction prompts are used from fragments.json, Default: [\"kojima-01\"] (only the first prompt is used)\n",
    "    \"author\" : \"your_name\", # Name of the person responsible for generation, Default: \"\"\n",
    "    \"api_service\": \"mock_api\", # Name of the API called (\"openai\", \"huggingface_hub\", or a mock for testing: \"mock_api\"), Default: \"huggingface_hub\"  # <--- SET ACCORDINGLY\n",
    "    \"engine\": \"\", # Name of the engine used (for \"huggingface_hub\" use for example \"google/flan-t5-xl\"), Default: \"google/flan-t5-xl\"  # <--- SET ACCORDINGLY\n",
    "    \"temperature\": 0, # Name of the person responsible for generation, Default: 0\n",
    "    \"max_tokens\": 512, # Maximum length of output generated by the model, Default: 128\n",
    "    \"api_time_interval\": 1.0, # Pause between two api calls in seconds, Default: 1.0\n",
    "    \"verbose\": False, # Determines whether the progress of the generation is printed, Default: True\n",
    "    \"warn\": True, # Determines whether a warnings that external APIs will be called are printed, Default: True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        You are about to \u001b[1m call an external API \u001b[0m in total 200 times, which \u001b[1m may produce costs \u001b[0m.\n",
      "        Number API calls for CoT generation: n_samples 100 * n_instruction_keys 1 * n_cot_trigger_keys 1\n",
      "        Number API calls for answer extraction: n_samples 100 * n_instruction_keys 1 * n_cot_trigger_keys 1 * n_answer_extraction_keys 1\n",
      "        Do you want to continue? y/n\n",
      "        \u001b[1m Note: You are using a mock api. When entering 'y', a test run without API calls is made. \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Generating chains-of-thought and answer extractions (This is in Mock-API mode, not calling model over API)\n",
    "worldtree_100_random.generate(name=\"worldtree\", config=config) #if you cannot press y, set \"warn\" to false in config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you did not change the config above, the above was a fake call to the mock API -- Now loading a prepared dataset with real model answers.\n",
    "collection = Collection.from_json(\"worldtree_100_generate.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2508ffb0",
   "metadata": {},
   "source": [
    "#### Display a question, answer choices and gold-standard answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Question: The length of a year is equivalent to the time it takes for one'\n",
      "'Answer Options:'\n",
      "['rotation of Earth',\n",
      " 'rotation of the Sun',\n",
      " 'revolution of Earth around the Sun',\n",
      " 'revolution of the Sun around Earth']\n",
      "'Answer: revolution of Earth around the Sun'\n"
     ]
    }
   ],
   "source": [
    "# Extract from prepared dataset\n",
    "pprint(\"Question: \"+ collection[\"worldtree\"][\"train\"][1][\"question\"])\n",
    "pprint(\"Answer Options:\")\n",
    "pprint(collection[\"worldtree\"][\"train\"][1][\"choices\"])\n",
    "pprint(\"Answer: \"+ \"\".join(collection[\"worldtree\"][\"train\"][1][\"answer\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2508ffb0",
   "metadata": {},
   "source": [
    "#### Display model-generated chain-of-thought and extracted answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' Test mock chain of thought.'\n",
      "' Test mock chain of thought.'\n"
     ]
    }
   ],
   "source": [
    "pprint(worldtree_100_random[\"worldtree\"][\"train\"][1][\"generated_cot\"][0][\"cot\"])\n",
    "pprint(worldtree_100_random[\"worldtree\"][\"train\"][1][\"generated_cot\"][0]['answers'][0]['answer'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer generated by the model was correct! To evaluate model answers automatically, ThoughtSource has an in-built evaluate function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f57e102",
   "metadata": {},
   "source": [
    "## 3. Evaluate: Evaluation of model answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading collection with model answers\n",
    "collection = Collection.from_json(\"worldtree_100_generate.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that before the evaluation function is run, the 'correct_answer' boolean field is not set.\n",
    "collection[\"worldtree\"][\"train\"][0]['generated_cot'][0][\"answers\"][0]['correct_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e3e3b0d7774cc7871e45736a3f5e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': {'qa-01_kojima-01_kojima-A-D': 0.86}}\n"
     ]
    }
   ],
   "source": [
    "# Now, let's evaluate the answers, set the 'correct_answer' field by comparing to gold-standard answers, and calculate the accuracy of the model predictions\n",
    "collection.evaluate(\"worldtree\",\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the 'correct_answer' fields are set\n",
    "collection[\"worldtree\"][\"train\"][0]['generated_cot'][0][\"answers\"][0]['correct_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e98cab52d24e809e59bf8533a73eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the file that now also includes data in the 'correct_answer' fields \n",
    "collection.dump(\"worldtree_100_evaluate.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TS2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a45f68e904890b8c321d3b9208e7b6a602b0eb7b894f25513a607ffd68e58ab2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
