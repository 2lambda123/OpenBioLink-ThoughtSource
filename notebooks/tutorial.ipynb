{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Imports\n",
    "Please follow the **installation guide** in the [ThoughtSource Readme file](https://github.com/OpenBioLink/ThoughtSource) before using this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only execute, if you use this notebook in Google Colab:\n",
    "# !pip install -e ../libs/cot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from cot import Collection\n",
    "from cot.generate import FRAGMENTS\n",
    "from rich.pretty import pprint\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick intro\n",
    "The ThoughtSource library offers functionality for: \n",
    "* Loading datasets\n",
    "* Creating random sub-samples\n",
    "* Generating novel chain-of-thought reasoning data and answers by connecting to external AI services\n",
    "* Evaluating results\n",
    "\n",
    "Below we will give a quick intro to the libary, followed by more detailed examples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to use external APIs you need a key. In this tutorial we will use the [Hugging Face API](https://huggingface.co/), which is for free. To use the API you need to set the environment variable `HUGGINGFACEHUB_API_TOKEN` to your API token. You can find your token in your Hugging Face settings page. For now you can set the environment variable in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"<token>\"   # <--- set token (can be found in your Hugging Face settings page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating worldtree train...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9714d19f09544c9e97a7aa5e11414f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'worldtree': {'train': {'accuracy': {'google/flan-t5-xl': {'qa-01_kojima-01_kojima-A-D': 0.6}}}}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08dc3c15bf4e4889b6717fed0e346ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1) Dataset loading and selecting a random sample\n",
    "collection = Collection([\"worldtree\"], verbose=False)\n",
    "collection = collection.select(split=\"train\", number_samples=1)\n",
    "\n",
    "# 2) Language Model generates chains of thought and then extracts answers\n",
    "\n",
    "config={\n",
    "    \"instruction_keys\": ['qa-01'], # \"Answer the following question through step-by-step reasoning.\"\n",
    "    \"cot_trigger_keys\": ['kojima-01'], # \"Answer: Let's think step by step.\"\n",
    "    \"answer_extraction_keys\": ['kojima-A-D'], # \"Therefore, among A through D, the answer is\"\n",
    "    \"api_service\": \"huggingface_hub\",\n",
    "    \"engine\": \"google/flan-t5-xl\",\n",
    "    \"warn\": False,\n",
    "    \"verbose\": False,\n",
    "}\n",
    "collection.generate(config=config)\n",
    "\n",
    "# 3) Evaluating answers generated by the model\n",
    "print(collection.evaluate())\n",
    "\n",
    "# 4) Saving the generated outputs and evaluation results\n",
    "# collection.dump(\"worldtree_10.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading, sampling and saving a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading worldtree...\n",
      "| Name      |   Train |   Valid |   Test |\n",
      "|-----------|---------|---------|--------|\n",
      "| worldtree |    2207 |     496 |   1664 |\n",
      "\n",
      "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa', 'medmc_qa', 'open_book_qa', 'pubmed_qa', 'qed', 'strategy_qa', 'svamp']\n"
     ]
    }
   ],
   "source": [
    "# load a dataset to sample from \n",
    "worldtree = Collection([\"worldtree\"], verbose=False)\n",
    "print(worldtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "| Name      |   Train | Valid   | Test   |\n",
       "|-----------|---------|---------|--------|\n",
       "| worldtree |      10 | -       | -      |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa', 'medmc_qa', 'open_book_qa', 'pubmed_qa', 'qed', 'strategy_qa', 'svamp']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly select 100 rows from train split\n",
    "worldtree_10 = worldtree.select(split=\"train\", number_samples=10, random_samples=True, seed=0)\n",
    "worldtree_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading med_qa...\n",
      "Loading medmc_qa...\n",
      "Loading pubmed_qa...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "| Name      |   Train | Valid   | Test   |\n",
       "|-----------|---------|---------|--------|\n",
       "| med_qa    |     100 | -       | -      |\n",
       "| medmc_qa  |     100 | -       | -      |\n",
       "| pubmed_qa |     100 | -       | -      |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'open_book_qa', 'qed', 'strategy_qa', 'svamp', 'worldtree']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that you could also sample from multiple datasets into one collection like this:\n",
    "collection_medical = Collection([\"med_qa\", \"medmc_qa\", \"pubmed_qa\"], verbose=False)\n",
    "collection_medical_100 = collection_medical.select(split=\"train\", number_samples=100)\n",
    "collection_medical_100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f57e102",
   "metadata": {},
   "source": [
    "## 2. Generating novel reasoning chains and answers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f043028",
   "metadata": {},
   "source": [
    "ThoughtSource comes pre-loaded with a large [collection of text snippets ('prompt fragments')](https://github.com/OpenBioLink/ThoughtSource/blob/main/libs/cot/cot/fragments.json) to elicit chain-of-thought reasoning in large language models and to extract answers from chains-of-thought. Let's see how prompt fragments look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-01'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's think step by step.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-02'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Answer: We should think about this step by step.'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-03'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Answer: First,'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-04'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Answer: Before we dive into the answer,'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-05'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Answer: Proof followed by the answer.'</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-01'\u001b[0m, \u001b[32m\"Answer: Let's think step by step.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-02'\u001b[0m, \u001b[32m'Answer: We should think about this step by step.'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-03'\u001b[0m, \u001b[32m'Answer: First,'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-04'\u001b[0m, \u001b[32m'Answer: Before we dive into the answer,'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-05'\u001b[0m, \u001b[32m'Answer: Proof followed by the answer.'\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Chain of thought prompts\n",
    "pprint(list(FRAGMENTS[\"cot_triggers\"].items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-03'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'The answer is'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-numerals'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Therefore, the answer (arabic numerals) is'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-yes-no'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Therefore, the answer (Yes or No) is'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-A-C'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Therefore, among A through C, the answer is'</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-03'\u001b[0m, \u001b[32m'The answer is'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-numerals'\u001b[0m, \u001b[32m'Therefore, the answer \u001b[0m\u001b[32m(\u001b[0m\u001b[32marabic numerals\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-yes-no'\u001b[0m, \u001b[32m'Therefore, the answer \u001b[0m\u001b[32m(\u001b[0m\u001b[32mYes or No\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-A-C'\u001b[0m, \u001b[32m'Therefore, among A through C, the answer is'\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer extraction prompts\n",
    "pprint(list(FRAGMENTS[\"answer_extractions\"].items())[2:6])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating chain-of-thought examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ThoughtSource can connect to external AI service providers such as the [OpenAI API](https://openai.com/api/) or the [Hugging Face Hub](https://huggingface.co/docs/hub/index). Set your token, 'api_service' and 'engine' parameters accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m\n",
      "    \"instruction_keys\": list(str) - Determines which instruction_keys are used from fragments.json,\n",
      "        the corresponding string will be inserted under \"instruction\" in the fragments. Default: [None] (No instruction)\n",
      "    \"cot_trigger_keys\": list(str) - Determines which cot triggers are used from fragments.json,\n",
      "        the corresponding string will be inserted under \"cot_trigger\" in the fragments. Default: [\"kojima-01\"]\n",
      "    \"answer_extraction_keys\": list(str) - Determines which answer extraction prompts are used from fragments.json,\n",
      "        the corresponding string will be inserted under \"answer\" in the fragments. Default: [\"kojima-01\"]\n",
      "    \"template_cot_generation\": string - is the model input in the text generation step, variables in brackets.\n",
      "        Only variables of this list are allowed: \"instruction\", 'question\", \"answer_choices\", \"cot_trigger\"\n",
      "        Default: {instruction}\n",
      "\n",
      "{question}\n",
      "{answer_choices}\n",
      "\n",
      "{cot_trigger}{cot}\n",
      "{answer_extraction}\n",
      "    \"template_answer_extraction\": string - is the model input in the answer extraction step, variables in brackets.\n",
      "        Only variables of this list are allowed: \"instruction\", 'question\", \"answer_choices\", \"cot_trigger\",\n",
      "        \"cot\", \"answer\"\n",
      "        Default: {instruction}\n",
      "\n",
      "{question}\n",
      "{answer_choices}\n",
      "\n",
      "{cot_trigger}\n",
      "    \"author\" : str - Name of the person responsible for generation, Default: \"\"\n",
      "    \"api_service\" str - Name of the used api service: \"openai\" or \"huggingface_hub\",\n",
      "        or a mock api service \"mock_api\" for debugging, Default: \"huggingface_hub\"\n",
      "    \"engine\": str -  Name of model used, look at website of api which are\n",
      "        available, e.g. for \"openai\": \"text-davinci-002\", Default: \"google/flan-t5-xl\"\n",
      "    \"temperature\": float - Describes how much randomness is in the generated output,\n",
      "        0.0 means the model will only output the most likely answer, 1.0 means\n",
      "        the model will also output very unlikely answers, defaults to 0\n",
      "    \"max_tokens\": int - Maximum length of output generated by model , Default: 128\n",
      "    \"api_time_interval\": float - Pause between two api calls in seconds, Default: 1.0\n",
      "    \"warn\": bool - Print warnings preventing excessive api usage, Default: True\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from cot.config import Config as config_overview\n",
    "print('\\033[94m' + config_overview.__doc__[48:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading worldtree...\n"
     ]
    }
   ],
   "source": [
    "# Sample 100 items from the Worldtree v2 dataset\n",
    "collection = Collection([\"worldtree\"], verbose=False)\n",
    "worldtree_10 = collection.select(split=\"train\", number_samples=10)\n",
    "\n",
    "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"<token>\"  # <--- SET ACCORDINGLY\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"<token>\"  # <--- SET ACCORDINGLY\n",
    "\n",
    "# Configuration for calling AI service. \n",
    "config={\n",
    "    \"instruction_keys\": ['qa-01'], # \"Answer the following question through step-by-step reasoning.\"\n",
    "    \"cot_trigger_keys\": ['kojima-01'], # \"Answer: Let's think step by step.\"\n",
    "    \"answer_extraction_keys\": ['kojima-A-D'], # \"Therefore, among A through D, the answer is\"\n",
    "    \"author\" : \"your_name\",\n",
    "    \"api_service\": \"mock_api\", # <--- SET ACCORDINGLY\n",
    "    \"engine\": \"\", # <--- SET ACCORDINGLY\n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"verbose\": False,\n",
    "    \"warn\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        You are about to \u001b[1m call an external API \u001b[0m in total 20 times, which \u001b[1m may produce costs \u001b[0m.\n",
      "        Number API calls for CoT generation: n_samples 10 * n_instruction_keys 1 * n_cot_trigger_keys 1\n",
      "        Number API calls for answer extraction: n_samples 10 * n_instruction_keys 1 * n_cot_trigger_keys 1 * n_answer_extraction_keys 1\n",
      "        Do you want to continue? y/n\n",
      "        \u001b[1m Note: You are using a mock api. When entering 'y', a test run without API calls is made. \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/tmpa3b68m9k/cache-37ebdcd9e87a1613.arrow\n"
     ]
    }
   ],
   "source": [
    "# Generating chains-of-thought and answer extractions (This is in Mock-API mode, not calling model over API)\n",
    "worldtree_10.generate(config=config) #if you cannot press y, set \"warn\" to false in config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above was a fake call to the mock API\n",
    "For the **purpose of the tutorial** we now load a prepared dataset with real model answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "worldtree_10 = Collection.from_json(\"worldtree_10.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2508ffb0",
   "metadata": {},
   "source": [
    "#### Display a question, answer choices and gold-standard answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Question: Animals may fight, make threatening sounds, and act aggressively '\n",
      " 'toward members of the same species. These behaviors usually occur as the '\n",
      " 'result of')\n",
      "'Answer Options:'\n",
      "['competition', 'conservation', 'decomposition', 'pollution']\n",
      "'Answer: competition'\n"
     ]
    }
   ],
   "source": [
    "# Extract from prepared dataset\n",
    "from pprint import pprint\n",
    "pprint(\"Question: \"+ worldtree_10[\"worldtree\"][\"train\"][1][\"question\"])\n",
    "pprint(\"Answer Options:\")\n",
    "pprint(worldtree_10[\"worldtree\"][\"train\"][1][\"choices\"])\n",
    "pprint(\"Answer: \"+ \"\".join(worldtree_10[\"worldtree\"][\"train\"][1][\"answer\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2508ffb0",
   "metadata": {},
   "source": [
    "#### Display model-generated chain-of-thought and extracted answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Aggressive behaviors are often the result of competition. Competition is the '\n",
      " 'result of animals fighting, making threatening sounds, and acting '\n",
      " 'aggressively toward members of the same species. So, the final answer is A.')\n",
      "'A.'\n"
     ]
    }
   ],
   "source": [
    "pprint(worldtree_10[\"worldtree\"][\"train\"][1][\"generated_cot\"][0][\"cot\"])\n",
    "pprint(worldtree_10[\"worldtree\"][\"train\"][1][\"generated_cot\"][0]['answers'][0]['answer'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer generated by the model was correct! To evaluate model answers automatically, ThoughtSource has an in-built evaluate function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f57e102",
   "metadata": {},
   "source": [
    "## 3. Evaluate: Evaluation of model answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating worldtree train...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f394271bfe25406ebab602471bc1d3a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'worldtree': {'train': {'accuracy': {'google/flan-t5-xl': {'qa-01_kojima-01_kojima-A-D': 0.6}}}}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worldtree_10.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda6b39f13154624a7f7e42b920d2850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the file that now also includes data in the 'correct_answer' fields \n",
    "worldtree_10.dump(\"worldtree_10.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f57e102",
   "metadata": {},
   "source": [
    "## 4. Examine: Use the ThoughtSource Annotation Web Tool"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use our online tool to **see an overview of the models output**. You can also use it to manually annotate the data. Everything is saved to the same json file.\n",
    "\n",
    "Just download the json file and then open it in the **[ThoughtSource Annotator](http://thought.samwald.info:3000/)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9109334bfcd43a12c41a56c4094a65d80af4afb65e4bb5b212660597404ee4d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
