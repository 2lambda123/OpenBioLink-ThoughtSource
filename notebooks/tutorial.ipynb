{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Imports\n",
    "Please follow the **installation guide** in the [ThoughtSource Readme file](https://github.com/OpenBioLink/ThoughtSource) before using this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only execute, if you use this notebook in Google Colab:\n",
    "# !pip install -e ../libs/cot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from cot import Collection\n",
    "from cot.generate import FRAGMENTS\n",
    "from rich.pretty import pprint\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "The ThoughtSource library offers functionality for: \n",
    "1) Loading datasets\n",
    "2) Generating novel chain-of-thought reasoning data and answers\n",
    "3) Evaluating results\n",
    "4) Visualizing results on a Web Application"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading, sampling and saving a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading worldtree...\n",
      "| Name      |   Train |   Valid |   Test |\n",
      "|-----------|---------|---------|--------|\n",
      "| worldtree |    2207 |     496 |   1664 |\n",
      "\n",
      "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa', 'medmc_qa', 'open_book_qa', 'pubmed_qa', 'qed', 'strategy_qa', 'svamp']\n"
     ]
    }
   ],
   "source": [
    "# load a dataset to sample from \n",
    "worldtree = Collection([\"worldtree\"], verbose=False)\n",
    "print(worldtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "| Name      |   Train | Valid   | Test   |\n",
       "|-----------|---------|---------|--------|\n",
       "| worldtree |      10 | -       | -      |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa', 'medmc_qa', 'open_book_qa', 'pubmed_qa', 'qed', 'strategy_qa', 'svamp']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly select 100 rows from train split\n",
    "worldtree_10 = worldtree.select(split=\"train\", number_samples=10, random_samples=True, seed=0)\n",
    "worldtree_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading med_qa...\n",
      "Loading medmc_qa...\n",
      "Loading pubmed_qa...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "| Name      |   Train | Valid   | Test   |\n",
       "|-----------|---------|---------|--------|\n",
       "| med_qa    |     100 | -       | -      |\n",
       "| medmc_qa  |     100 | -       | -      |\n",
       "| pubmed_qa |     100 | -       | -      |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'open_book_qa', 'qed', 'strategy_qa', 'svamp', 'worldtree']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that you could also sample from multiple datasets into one collection like this:\n",
    "collection_medical = Collection([\"med_qa\", \"medmc_qa\", \"pubmed_qa\"], verbose=False)\n",
    "collection_medical_100 = collection_medical.select(split=\"train\", number_samples=100)\n",
    "collection_medical_100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f57e102",
   "metadata": {},
   "source": [
    "## 2. Generating novel reasoning chains and answers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f043028",
   "metadata": {},
   "source": [
    "ThoughtSource comes pre-loaded with a large [collection of text snippets ('prompt fragments')](https://github.com/OpenBioLink/ThoughtSource/blob/main/libs/cot/cot/fragments.json) to elicit chain-of-thought reasoning in large language models and to extract answers from chains-of-thought. Let's see how prompt fragments look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-01'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's think step by step.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-02'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Answer: We should think about this step by step.'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-03'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Answer: First,'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-04'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Answer: Before we dive into the answer,'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-05'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Answer: Proof followed by the answer.'</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-01'\u001b[0m, \u001b[32m\"Answer: Let's think step by step.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-02'\u001b[0m, \u001b[32m'Answer: We should think about this step by step.'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-03'\u001b[0m, \u001b[32m'Answer: First,'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-04'\u001b[0m, \u001b[32m'Answer: Before we dive into the answer,'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-05'\u001b[0m, \u001b[32m'Answer: Proof followed by the answer.'\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Chain of thought prompts\n",
    "pprint(list(FRAGMENTS[\"cot_triggers\"].items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-03'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'The answer is'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-numerals'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Therefore, the answer (arabic numerals) is'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-yes-no'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Therefore, the answer (Yes or No) is'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-A-C'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Therefore, among A through C, the answer is'</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-03'\u001b[0m, \u001b[32m'The answer is'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-numerals'\u001b[0m, \u001b[32m'Therefore, the answer \u001b[0m\u001b[32m(\u001b[0m\u001b[32marabic numerals\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-yes-no'\u001b[0m, \u001b[32m'Therefore, the answer \u001b[0m\u001b[32m(\u001b[0m\u001b[32mYes or No\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-A-C'\u001b[0m, \u001b[32m'Therefore, among A through C, the answer is'\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer extraction prompts\n",
    "pprint(list(FRAGMENTS[\"answer_extractions\"].items())[2:6])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating chain-of-thought examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ThoughtSource can connect to external AI service providers such as the [OpenAI API](https://openai.com/api/) or the [Hugging Face Hub](https://huggingface.co/docs/hub/index). Set your token, 'api_service' and 'engine' parameters accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m\n",
      "    \"instruction_keys\": list(str) - Determines which instruction_keys are used from fragments.json,\n",
      "        the corresponding string will be inserted under \"instruction\" in the fragments. Default: [None] (No instruction)\n",
      "    \"cot_trigger_keys\": list(str) - Determines which cot triggers are used from fragments.json,\n",
      "        the corresponding string will be inserted under \"cot_trigger\" in the fragments. Default: [\"kojima-01\"]\n",
      "    \"answer_extraction_keys\": list(str) - Determines which answer extraction prompts are used from fragments.json,\n",
      "        the corresponding string will be inserted under \"answer\" in the fragments. Default: [\"kojima-01\"]\n",
      "    \"template_cot_generation\": string - is the model input in the text generation step, variables in brackets.\n",
      "        Only variables of this list are allowed: \"instruction\", 'question\", \"answer_choices\", \"cot_trigger\"\n",
      "        Default: {instruction}\\n\\n{question}\\n{answer_choices}\\n\\n{cot_trigger}\n",
      "    \"template_answer_extraction\": string - is the model input in the answer extraction step, variables in brackets.\n",
      "        Only variables of this list are allowed: \"instruction\", 'question\", \"answer_choices\", \"cot_trigger\",\n",
      "        \"cot\", \"answer\"\n",
      "        Default: {instruction}\\n\\n{question}\\n{answer_choices}\\n\\n{cot_trigger}{cot}\\n{answer_extraction}\n",
      "    \"author\" : str - Name of the person responsible for generation, Default: \"\"\n",
      "    \"api_service\" str - Name of the used api service: \"openai\" or \"huggingface_hub\",\n",
      "        or a mock api service \"mock_api\" for debugging, Default: \"huggingface_hub\"\n",
      "    \"engine\": str -  Name of model used, look at website of api which are\n",
      "        available, e.g. for \"openai\": \"text-davinci-002\", Default: \"google/flan-t5-xl\"\n",
      "    \"temperature\": float - Describes how much randomness is in the generated output,\n",
      "        0.0 means the model will only output the most likely answer, 1.0 means\n",
      "        the model will also output very unlikely answers, defaults to 0\n",
      "    \"max_tokens\": int - Maximum length of output generated by model , Default: 128\n",
      "    \"api_time_interval\": float - Pause between two api calls in seconds, Default: 1.0\n",
      "    \"warn\": bool - Print warnings preventing excessive api usage, Default: True\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from cot.config import Config as config_overview\n",
    "print(r'\\033[94m' + config_overview.__doc__[48:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading worldtree...\n"
     ]
    }
   ],
   "source": [
    "# Sample 100 items from the Worldtree v2 dataset\n",
    "collection = Collection([\"worldtree\"], verbose=False)\n",
    "worldtree_10 = collection.select(split=\"train\", number_samples=10)\n",
    "\n",
    "# Configuration for calling AI service. \n",
    "config={\n",
    "    \"instruction_keys\": ['qa-01'], # \"Answer the following question through step-by-step reasoning.\"\n",
    "    \"cot_trigger_keys\": ['kojima-01'], # \"Answer: Let's think step by step.\"\n",
    "    \"answer_extraction_keys\": ['kojima-A-D'], # \"Therefore, among A through D, the answer is\"\n",
    "    \"author\" : \"your_name\",\n",
    "    \"api_service\": \"mock_api\", # <--- We use a mock API here for demonstration purposes of the tutorial.\n",
    "    \"engine\": \"\", # \n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"verbose\": False,\n",
    "    \"warn\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{instruction}\n",
      "\n",
      "{question}\n",
      "{answer_choices}\n",
      "\n",
      "{cot_trigger}\n"
     ]
    }
   ],
   "source": [
    "#In the config, you can also define a custom text template.\n",
    "#The default template for cot-generation is:\n",
    "\n",
    "print(\"{instruction}\\n\\n{question}\\n{answer_choices}\\n\\n{cot_trigger}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer this question:\n",
      "{question}\n",
      "{answer_choices}\n",
      "\n",
      "Give a detailed explanation of your answer.\n"
     ]
    }
   ],
   "source": [
    "# You could also define your own template with a different structure and even free text.\n",
    "print(\"Answer this question:\\n{question}\\n{answer_choices}\\n\\nGive a detailed explanation of your answer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        You are about to \u001b[1m call an external API \u001b[0m in total 20 times, which \u001b[1m may produce costs \u001b[0m.\n",
      "        Number API calls for CoT generation: n_samples 10 * n_instruction_keys 1 * n_cot_trigger_keys 1\n",
      "        Number API calls for answer extraction: n_samples 10 * n_instruction_keys 1 * n_cot_trigger_keys 1 * n_answer_extraction_keys 1\n",
      "        Do you want to continue? y/n\n",
      "        \u001b[1m Note: You are using a mock api. When entering 'y', a test run without API calls is made. \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Generating chains-of-thought and answer extractions (This is in Mock-API mode, not calling model over API)\n",
    "worldtree_10.generate(config=config) #if you cannot press y, set \"warn\" to false in config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above was a fake call to the mock API\n",
    "For the **purpose of the tutorial** we now load a prepared dataset with real model answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "worldtree_10 = Collection.from_json(\"worldtree_10.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2508ffb0",
   "metadata": {},
   "source": [
    "#### Display a question, answer choices and gold-standard answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Question: Animals may fight, make threatening sounds, and act aggressively '\n",
      " 'toward members of the same species. These behaviors usually occur as the '\n",
      " 'result of')\n",
      "'Answer Options: competition,  conservation,  decomposition,  pollution'\n",
      "'Gold Answer: competition'\n"
     ]
    }
   ],
   "source": [
    "# Extract from prepared dataset\n",
    "from pprint import pprint\n",
    "pprint(\"Question: \"+ worldtree_10[\"worldtree\"][\"train\"][1][\"question\"])\n",
    "pprint(\"Answer Options: \" + \",  \".join(worldtree_10[\"worldtree\"][\"train\"][1][\"choices\"]))\n",
    "pprint(\"Gold Answer: \"+ \"\".join(worldtree_10[\"worldtree\"][\"train\"][1][\"answer\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2508ffb0",
   "metadata": {},
   "source": [
    "#### Display model-generated chain-of-thought and extracted answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Aggressive behaviors are often the result of competition. Competition is the '\n",
      " 'result of animals fighting, making threatening sounds, and acting '\n",
      " 'aggressively toward members of the same species. So, the final answer is A.')\n",
      "'A.'\n"
     ]
    }
   ],
   "source": [
    "pprint(worldtree_10[\"worldtree\"][\"train\"][1][\"generated_cot\"][0][\"cot\"])\n",
    "pprint(worldtree_10[\"worldtree\"][\"train\"][1][\"generated_cot\"][0]['answers'][0]['answer'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer generated by the model was correct! To evaluate model answers automatically, ThoughtSource has an in-built evaluate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b2873824f94deeaa3832b2fb7c8a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If you want to see your prompt as full text, you can use:\n",
    "# This is not done by default as the json file gets very large.\n",
    "worldtree_10.full_text_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following question through step-by-step reasoning.\n",
      "\n",
      "Animals may fight, make threatening sounds, and act aggressively toward members of the same species. These behaviors usually occur as the result of\n",
      "A) competition\n",
      "B) conservation\n",
      "C) decomposition\n",
      "D) pollution\n",
      "\n",
      "Answer: Let's think step by step.\n"
     ]
    }
   ],
   "source": [
    "# this is the exact text, which was used by the model to generate the chain of thought\n",
    "print(worldtree_10[\"worldtree\"][\"train\"][1][\"generated_cot\"][0][\"prompt_text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f57e102",
   "metadata": {},
   "source": [
    "## 3. Evaluate: Evaluation of model answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating worldtree train...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38831203e5f47ec8c1ba4abfb5cf0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'worldtree': {'train': {'accuracy': {'google/flan-t5-xl': {'qa-01_kojima-01_kojima-A-D': 0.6}}}}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worldtree_10.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0953f04cb11c4baca2c4405ab43dd8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the file that now also includes data in the 'correct_answer' fields \n",
    "worldtree_10.dump(\"worldtree_10.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f57e102",
   "metadata": {},
   "source": [
    "## 4. Examine: Use the Web Tool to inspect the model outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is the link: **[ThoughtSource Annotator](http://thought.samwald.info:3000/)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just **upload your just downloaded json file** to the web tool.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f57e102",
   "metadata": {},
   "source": [
    "## Next step: Your own data creation with your personal API key."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ThoughtSource can connect to external AI service providers such as the [OpenAI API](https://openai.com/api/) or the [Hugging Face Hub](https://huggingface.co/docs/hub/index). Set your token, 'api_service' and 'engine' parameters accordingly.\n",
    "\n",
    "In this tutorial we will use the Hugging Face Hub, which is for free. To use the API you need to set the environment variable `HUGGINGFACEHUB_API_TOKEN` to your API token. \n",
    "\n",
    "You can find your token in your Hugging Face settings page. You can set the environment variable in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"<token>\"   # <--- set token (can be found in your Hugging Face settings page)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"<token>\"  # <--- Set token for which API you want to use"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The full process of loading, selecting, generating, evaluating and saving in one cell:\n",
    "\n",
    "Calling the external API will approximately take 10 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading worldtree...\n",
      "Evaluating worldtree train...\n",
      "{'worldtree': {'train': {'accuracy': {'google/flan-t5-xl': {'qa-01_kojima-01_kojima-A-D': 1.0}}}}}\n"
     ]
    }
   ],
   "source": [
    "# 1) Dataset loading and selecting a random sample\n",
    "collection = Collection([\"worldtree\"], verbose=False)\n",
    "collection = collection.select(split=\"train\", number_samples=1) # just using 1 sample for demonstration purposes\n",
    "\n",
    "# 2) Language Model generates chains of thought and then extracts answers\n",
    "\n",
    "config={\n",
    "    \"instruction_keys\": ['qa-01'], # \"Answer the following question through step-by-step reasoning.\"\n",
    "    \"cot_trigger_keys\": ['kojima-01'], # \"Answer: Let's think step by step.\"\n",
    "    \"answer_extraction_keys\": ['kojima-A-D'], # \"Therefore, among A through D, the answer is\"\n",
    "    \"api_service\": \"huggingface_hub\",\n",
    "    \"engine\": \"google/flan-t5-xl\",\n",
    "    \"warn\": False,\n",
    "    \"verbose\": False,\n",
    "}\n",
    "collection.generate(config=config)\n",
    "\n",
    "# 3) Evaluating answers generated by the model\n",
    "print(collection.evaluate())\n",
    "\n",
    "# 4) Saving the generated outputs and evaluation results\n",
    "# collection.dump(\"worldtree_1.json\")\n",
    "\n",
    "# 5) Using the the ThoughSource Annotator web tool to inspect your results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9109334bfcd43a12c41a56c4094a65d80af4afb65e4bb5b212660597404ee4d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
