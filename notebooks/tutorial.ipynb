{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Imports\n",
    "Please follow the **installation guide** in the [ThoughtSource Readme file](https://github.com/OpenBioLink/ThoughtSource) before using this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate the autoreload to keep variables up-to-date\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from cot import Collection\n",
    "from cot.generate import FRAGMENTS\n",
    "from rich.pretty import pprint\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "The ThoughtSource library offers functionality for: \n",
    "1) Loading datasets\n",
    "2) Generating novel chain-of-thought reasoning data and answers\n",
    "3) Evaluating results\n",
    "4) Visualizing results on a Web Application"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading, sampling and saving a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading worldtree...\n",
      "| Name      |   Train |   Valid |   Test |\n",
      "|-----------|---------|---------|--------|\n",
      "| worldtree |    2207 |     496 |   1664 |\n",
      "\n",
      "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa', 'medmc_qa', 'open_book_qa', 'pubmed_qa', 'qed', 'strategy_qa', 'svamp']\n"
     ]
    }
   ],
   "source": [
    "# load a dataset to sample from \n",
    "worldtree = Collection([\"worldtree\"], verbose=False)\n",
    "print(worldtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "| Name      |   Train | Valid   | Test   |\n",
       "|-----------|---------|---------|--------|\n",
       "| worldtree |      10 | -       | -      |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa', 'medmc_qa', 'open_book_qa', 'pubmed_qa', 'qed', 'strategy_qa', 'svamp']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly select 10 rows from train split\n",
    "worldtree_10 = worldtree.select(split=\"train\", number_samples=10, random_samples=True, seed=0)\n",
    "worldtree_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading med_qa...\n",
      "Loading pubmed_qa...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "| Name      |   Train | Valid   | Test   |\n",
       "|-----------|---------|---------|--------|\n",
       "| med_qa    |     100 | -       | -      |\n",
       "| pubmed_qa |     100 | -       | -      |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'medmc_qa', 'open_book_qa', 'qed', 'strategy_qa', 'svamp', 'worldtree']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that you could also sample from multiple datasets into one collection like this:\n",
    "collection_medical = Collection([\"med_qa\", \"pubmed_qa\"], verbose=False)\n",
    "collection_medical_100 = collection_medical.select(split=\"train\", number_samples=100)\n",
    "collection_medical_100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f57e102",
   "metadata": {},
   "source": [
    "## 2. Generating reasoning chains and extracting answers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f57e102",
   "metadata": {},
   "source": [
    "#### Using predefined text snippets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f043028",
   "metadata": {},
   "source": [
    "ThoughtSource comes pre-loaded with a large [collection of text snippets ('prompt fragments')](https://github.com/OpenBioLink/ThoughtSource/blob/main/libs/cot/cot/fragments.json) to elicit chain-of-thought reasoning in large language models and to extract answers from chains-of-thought. Let's see how prompt fragments look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-01'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Answer: Let's think step by step.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-02'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Answer: We should think about this step by step.'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-03'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Answer: First,'</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-01'\u001b[0m, \u001b[32m\"Answer: Let's think step by step.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-02'\u001b[0m, \u001b[32m'Answer: We should think about this step by step.'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-03'\u001b[0m, \u001b[32m'Answer: First,'\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Chain of thought prompts\n",
    "pprint(list(FRAGMENTS[\"cot_triggers\"].items())[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-03'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'The answer is'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-numerals'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Therefore, the answer (arabic numerals) is'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-yes-no'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Therefore, the answer (Yes or No) is'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-A-C'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Therefore, among A through C, the answer is'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'kojima-A-D'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Therefore, among A through D, the answer is'</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-03'\u001b[0m, \u001b[32m'The answer is'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-numerals'\u001b[0m, \u001b[32m'Therefore, the answer \u001b[0m\u001b[32m(\u001b[0m\u001b[32marabic numerals\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-yes-no'\u001b[0m, \u001b[32m'Therefore, the answer \u001b[0m\u001b[32m(\u001b[0m\u001b[32mYes or No\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-A-C'\u001b[0m, \u001b[32m'Therefore, among A through C, the answer is'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'kojima-A-D'\u001b[0m, \u001b[32m'Therefore, among A through D, the answer is'\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer extraction prompts\n",
    "pprint(list(FRAGMENTS[\"answer_extractions\"].items())[2:7])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f57e102",
   "metadata": {},
   "source": [
    "#### Configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m \n",
      "    \"instruction_keys\": list(str) - Determines which instruction_keys are used from fragments.json,\n",
      "        the corresponding string will be inserted under \"instruction\" in the fragments. Default: [None] (No instruction)\n",
      "    \"cot_trigger_keys\": list(str) - Determines which cot triggers are used from fragments.json,\n",
      "        the corresponding string will be inserted under \"cot_trigger\" in the fragments. Default: [\"kojima-01\"]\n",
      "    \"answer_extraction_keys\": list(str) - Determines which answer extraction prompts are used from fragments.json,\n",
      "        the corresponding string will be inserted under \"answer\" in the fragments. Default: [\"kojima-01\"]\n",
      "    \"template_cot_generation\": string - is the model input in the text generation step, variables in brackets.\n",
      "        Only variables of this list are allowed: \"instruction\", 'question\", \"answer_choices\", \"cot_trigger\"\n",
      "        Default: {instruction}\n",
      "\n",
      "{question}\n",
      "{answer_choices}\n",
      "\n",
      "{cot_trigger}\n",
      "    \"template_answer_extraction\": string - is the model input in the answer extraction step, variables in brackets.\n",
      "        Only variables of this list are allowed: \"instruction\", 'question\", \"answer_choices\", \"cot_trigger\",\n",
      "        \"cot\", \"answer\"\n",
      "        Default: {instruction}\n",
      "\n",
      "{question}\n",
      "{answer_choices}\n",
      "\n",
      "{cot_trigger}{cot}\n",
      "{answer_extraction}\n",
      "    \"author\" : str - Name of the person responsible for generation, Default: \"\"\n",
      "    \"api_service\" str - Name of the used api service: \"openai\" or \"huggingface_hub\",\n",
      "        or a mock api service \"mock_api\" for debugging, Default: \"huggingface_hub\"\n",
      "    \"engine\": str -  Name of model used, look at website of api which are\n",
      "        available, e.g. for \"openai\": \"text-davinci-002\", Default: \"google/flan-t5-xl\"\n",
      "    \"temperature\": float - Describes how much randomness is in the generated output,\n",
      "        0.0 means the model will only output the most likely answer, 1.0 means\n",
      "        the model will also output very unlikely answers, defaults to 0\n",
      "    \"max_tokens\": int - Maximum length of output generated by model , Default: 128\n",
      "    \"api_time_interval\": float - Pause between two api calls in seconds, Default: 1.0\n",
      "    \"warn\": bool - Print warnings preventing excessive api usage, Default: True\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# overview of all available configurations\n",
    "from cot.config import Config as config_overview\n",
    "print(f'\\033[94m {config_overview.__doc__[48:]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining your own template (optional)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default chain of thought generation template as shown above is: \"{instruction}\\n\\n{question}\\n{answer_choices}\\n\\n{cot_trigger}\". </br>\n",
    "You could also define your own template with a different structure and even free text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Answer this question:\\n{question}\\n{answer_choices}\\n\\nGive a detailed explanation of your answer.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you define your custom chain of thought generation template, do not forget to also define a custom answer extraction template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Answer this question:\\n{question}\\n{answer_choices}\\n\\nGive a detailed explanation of your answer.{cot}\\n{answer_extraction}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f57e102",
   "metadata": {},
   "source": [
    "### 2.1 Using a Mock-API to create reasoning chains and extract answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration of the input and parameters of the language model \n",
    "config={\n",
    "    # We compare three different prompts for the chain of thought generation:\n",
    "    # \"Answer: Let's think step by step.\" and 'Answer: We should think about this step by step.', and \"Answer: First,\" \n",
    "    \"cot_trigger_keys\": ['kojima-01','kojima-02', 'kojima-03'],\n",
    "\n",
    "    # We use the same answer extraction prompt for all three prompts\n",
    "    # \"Therefore, among A through D, the answer is\"\n",
    "    \"answer_extraction_keys\": ['kojima-A-D'], \n",
    "    \n",
    "    \"author\" : \"your_name\",\n",
    "    \"api_service\": \"mock_api\", # <--- We use a mock API here for demonstration purposes of the tutorial.\n",
    "    \"engine\": \"\", \n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"verbose\": False,\n",
    "    \"warn\": True,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating reasoning chains and extracting answers in one call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6410ccdaba40f78f5ebe1deb06eee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Couldn't cast array of type\nstruct<annotation: list<item: null>, answers: list<item: struct<answer: string, answer_extraction: string, answer_extraction_template: string, answer_extraction_text: string, correct_answer: null, id: string>>, api_service: string, author: string, comment: string, cot: string, cot_trigger: string, cot_trigger_template: string, date: string, fragments_version: string, id: string, instruction: null, model: string, prompt_text: string>\nto\n{'id': Value(dtype='string', id=None), 'fragments_version': Value(dtype='string', id=None), 'instruction': Value(dtype='string', id=None), 'cot_trigger': Value(dtype='string', id=None), 'prompt_text': Value(dtype='string', id=None), 'answers': [{'id': Value(dtype='string', id=None), 'answer_extraction': Value(dtype='string', id=None), 'answer_extraction_text': Value(dtype='string', id=None), 'answer': Value(dtype='string', id=None), 'correct_answer': Value(dtype='bool', id=None)}], 'cot': Value(dtype='string', id=None), 'author': Value(dtype='string', id=None), 'date': Value(dtype='string', id=None), 'api_service': Value(dtype='string', id=None), 'model': Value(dtype='string', id=None), 'comment': Value(dtype='string', id=None), 'annotation': [{'author': Value(dtype='string', id=None), 'date': Value(dtype='string', id=None), 'key': Value(dtype='string', id=None), 'value': Value(dtype='string', id=None)}]}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py:2358\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   2357\u001b[0m     \u001b[39mif\u001b[39;00m update_data \u001b[39mand\u001b[39;00m writer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2358\u001b[0m         writer\u001b[39m.\u001b[39;49mfinalize()  \u001b[39m# close_stream=bool(buf_writer is None))  # We only close if we are writing in a file\u001b[39;00m\n\u001b[1;32m   2359\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/arrow_writer.py:537\u001b[0m, in \u001b[0;36mArrowWriter.finalize\u001b[0;34m(self, close_stream)\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhkey_record \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 537\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_examples_on_file()\n\u001b[1;32m    538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpa_writer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/arrow_writer.py:414\u001b[0m, in \u001b[0;36mArrowWriter.write_examples_on_file\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m     batch_examples[col] \u001b[39m=\u001b[39m [row[\u001b[39m0\u001b[39m][col] \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_examples]\n\u001b[0;32m--> 414\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_batch(batch_examples\u001b[39m=\u001b[39;49mbatch_examples)\n\u001b[1;32m    415\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_examples \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/arrow_writer.py:507\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[0;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[1;32m    506\u001b[0m typed_sequence \u001b[39m=\u001b[39m OptimizedTypedSequence(batch_examples[col], \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mcol_type, try_type\u001b[39m=\u001b[39mcol_try_type, col\u001b[39m=\u001b[39mcol)\n\u001b[0;32m--> 507\u001b[0m arrays\u001b[39m.\u001b[39mappend(pa\u001b[39m.\u001b[39;49marray(typed_sequence))\n\u001b[1;32m    508\u001b[0m inferred_features[col] \u001b[39m=\u001b[39m typed_sequence\u001b[39m.\u001b[39mget_inferred_type()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyarrow/array.pxi:230\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyarrow/array.pxi:110\u001b[0m, in \u001b[0;36mpyarrow.lib._handle_arrow_array_protocol\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/arrow_writer.py:202\u001b[0m, in \u001b[0;36mTypedSequence.__arrow_array__\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[39m# We use cast_array_to_feature to support casting to custom types like Audio and Image\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[39m# Also, when trying type \"string\", we don't want to convert integers or floats to \"string\".\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[39m# We only do it if trying_type is False - since this is what the user asks for.\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     out \u001b[39m=\u001b[39m cast_array_to_feature(out, \u001b[39mtype\u001b[39;49m, allow_number_to_str\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrying_type)\n\u001b[1;32m    203\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/table.py:1674\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[0;34m(array, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1673\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1674\u001b[0m     \u001b[39mreturn\u001b[39;00m func(array, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/table.py:1787\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[0;34m(array, feature, allow_number_to_str)\u001b[0m\n\u001b[1;32m   1786\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(feature, \u001b[39mlist\u001b[39m):\n\u001b[0;32m-> 1787\u001b[0m     \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mListArray\u001b[39m.\u001b[39mfrom_arrays(array\u001b[39m.\u001b[39moffsets, _c(array\u001b[39m.\u001b[39;49mvalues, feature[\u001b[39m0\u001b[39;49m]))\n\u001b[1;32m   1788\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(feature, Sequence):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/table.py:1674\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[0;34m(array, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1673\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1674\u001b[0m     \u001b[39mreturn\u001b[39;00m func(array, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/table.py:1809\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[0;34m(array, feature, allow_number_to_str)\u001b[0m\n\u001b[1;32m   1808\u001b[0m     \u001b[39mreturn\u001b[39;00m array_cast(array, feature(), allow_number_to_str\u001b[39m=\u001b[39mallow_number_to_str)\n\u001b[0;32m-> 1809\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt cast array of type\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00marray\u001b[39m.\u001b[39mtype\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mto\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfeature\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Couldn't cast array of type\nstruct<annotation: list<item: null>, answers: list<item: struct<answer: string, answer_extraction: string, answer_extraction_template: string, answer_extraction_text: string, correct_answer: null, id: string>>, api_service: string, author: string, comment: string, cot: string, cot_trigger: string, cot_trigger_template: string, date: string, fragments_version: string, id: string, instruction: null, model: string, prompt_text: string>\nto\n{'id': Value(dtype='string', id=None), 'fragments_version': Value(dtype='string', id=None), 'instruction': Value(dtype='string', id=None), 'cot_trigger': Value(dtype='string', id=None), 'prompt_text': Value(dtype='string', id=None), 'answers': [{'id': Value(dtype='string', id=None), 'answer_extraction': Value(dtype='string', id=None), 'answer_extraction_text': Value(dtype='string', id=None), 'answer': Value(dtype='string', id=None), 'correct_answer': Value(dtype='bool', id=None)}], 'cot': Value(dtype='string', id=None), 'author': Value(dtype='string', id=None), 'date': Value(dtype='string', id=None), 'api_service': Value(dtype='string', id=None), 'model': Value(dtype='string', id=None), 'comment': Value(dtype='string', id=None), 'annotation': [{'author': Value(dtype='string', id=None), 'date': Value(dtype='string', id=None), 'key': Value(dtype='string', id=None), 'value': Value(dtype='string', id=None)}]}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/robertpraas/Desktop/ThoughtSource/notebooks/tutorial.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robertpraas/Desktop/ThoughtSource/notebooks/tutorial.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Generating chains-of-thought and answer extractions\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/robertpraas/Desktop/ThoughtSource/notebooks/tutorial.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m worldtree_10\u001b[39m.\u001b[39;49mgenerate(config\u001b[39m=\u001b[39;49mconfig)\n",
      "File \u001b[0;32m~/Desktop/ThoughtSource/libs/cot/cot/dataloader.py:227\u001b[0m, in \u001b[0;36mCollection.generate\u001b[0;34m(self, name, split, config)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m     \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache:\n\u001b[0;32m--> 227\u001b[0m         \u001b[39mself\u001b[39m[name] \u001b[39m=\u001b[39m generate_and_extract(\u001b[39mself\u001b[39;49m[name], config\u001b[39m=\u001b[39;49mconfig)\n\u001b[1;32m    228\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m split \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/ThoughtSource/libs/cot/cot/generate.py:52\u001b[0m, in \u001b[0;36mgenerate_and_extract\u001b[0;34m(data, config)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39m# The config is transformed into a dataclass object, where all testing is done\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39m# But it will be transformed back to a dictionary for the function 'map'\u001b[39;00m\n\u001b[1;32m     50\u001b[0m config_as_dataclass \u001b[39m=\u001b[39m Config(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig)\n\u001b[0;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m data\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m     53\u001b[0m     _generate_and_extract,\n\u001b[1;32m     54\u001b[0m     with_indices\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     55\u001b[0m     fn_kwargs\u001b[39m=\u001b[39;49masdict(config_as_dataclass),\n\u001b[1;32m     56\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m     57\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/dataset_dict.py:438\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[1;32m    437\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 438\u001b[0m     {\n\u001b[1;32m    439\u001b[0m         k: dataset\u001b[39m.\u001b[39mmap(\n\u001b[1;32m    440\u001b[0m             function\u001b[39m=\u001b[39mfunction,\n\u001b[1;32m    441\u001b[0m             with_indices\u001b[39m=\u001b[39mwith_indices,\n\u001b[1;32m    442\u001b[0m             with_rank\u001b[39m=\u001b[39mwith_rank,\n\u001b[1;32m    443\u001b[0m             input_columns\u001b[39m=\u001b[39minput_columns,\n\u001b[1;32m    444\u001b[0m             batched\u001b[39m=\u001b[39mbatched,\n\u001b[1;32m    445\u001b[0m             batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m    446\u001b[0m             drop_last_batch\u001b[39m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    447\u001b[0m             remove_columns\u001b[39m=\u001b[39mremove_columns,\n\u001b[1;32m    448\u001b[0m             keep_in_memory\u001b[39m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    449\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    450\u001b[0m             cache_file_name\u001b[39m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    451\u001b[0m             writer_batch_size\u001b[39m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    452\u001b[0m             features\u001b[39m=\u001b[39mfeatures,\n\u001b[1;32m    453\u001b[0m             disable_nullable\u001b[39m=\u001b[39mdisable_nullable,\n\u001b[1;32m    454\u001b[0m             fn_kwargs\u001b[39m=\u001b[39mfn_kwargs,\n\u001b[1;32m    455\u001b[0m             num_proc\u001b[39m=\u001b[39mnum_proc,\n\u001b[1;32m    456\u001b[0m             desc\u001b[39m=\u001b[39mdesc,\n\u001b[1;32m    457\u001b[0m         )\n\u001b[1;32m    458\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    459\u001b[0m     }\n\u001b[1;32m    460\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/dataset_dict.py:439\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[1;32m    437\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    438\u001b[0m     {\n\u001b[0;32m--> 439\u001b[0m         k: dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m    440\u001b[0m             function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m    441\u001b[0m             with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m    442\u001b[0m             with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m    443\u001b[0m             input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m    444\u001b[0m             batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m    445\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    446\u001b[0m             drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m    447\u001b[0m             remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m    448\u001b[0m             keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m    449\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m    450\u001b[0m             cache_file_name\u001b[39m=\u001b[39;49mcache_file_names[k],\n\u001b[1;32m    451\u001b[0m             writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m    452\u001b[0m             features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m    453\u001b[0m             disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m    454\u001b[0m             fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m    455\u001b[0m             num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m    456\u001b[0m             desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m    457\u001b[0m         )\n\u001b[1;32m    458\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    459\u001b[0m     }\n\u001b[1;32m    460\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py:1955\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   1952\u001b[0m disable_tqdm \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[1;32m   1954\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_proc \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 1955\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_single(\n\u001b[1;32m   1956\u001b[0m         function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m   1957\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m   1958\u001b[0m         with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m   1959\u001b[0m         input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m   1960\u001b[0m         batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m   1961\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   1962\u001b[0m         drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m   1963\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m   1964\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m   1965\u001b[0m         load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m   1966\u001b[0m         cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[1;32m   1967\u001b[0m         writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m   1968\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1969\u001b[0m         disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m   1970\u001b[0m         fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m   1971\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[1;32m   1972\u001b[0m         disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm,\n\u001b[1;32m   1973\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m   1974\u001b[0m     )\n\u001b[1;32m   1975\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1977\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py:520\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    519\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    521\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    522\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    523\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py:487\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    481\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    482\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    483\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    484\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    485\u001b[0m }\n\u001b[1;32m    486\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 487\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    488\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    489\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/fingerprint.py:458\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m             kwargs[fingerprint_name] \u001b[39m=\u001b[39m update_fingerprint(\n\u001b[1;32m    453\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fingerprint, transform, kwargs_for_fingerprint\n\u001b[1;32m    454\u001b[0m             )\n\u001b[1;32m    456\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    460\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py:2362\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   2360\u001b[0m \u001b[39mif\u001b[39;00m update_data:\n\u001b[1;32m   2361\u001b[0m     \u001b[39mif\u001b[39;00m writer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2362\u001b[0m         writer\u001b[39m.\u001b[39;49mfinalize()\n\u001b[1;32m   2363\u001b[0m     \u001b[39mif\u001b[39;00m tmp_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2364\u001b[0m         tmp_file\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/arrow_writer.py:537\u001b[0m, in \u001b[0;36mArrowWriter.finalize\u001b[0;34m(self, close_stream)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[39m# Re-intializing to empty list for next batch\u001b[39;00m\n\u001b[1;32m    536\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhkey_record \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 537\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_examples_on_file()\n\u001b[1;32m    538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpa_writer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    539\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/arrow_writer.py:414\u001b[0m, in \u001b[0;36mArrowWriter.write_examples_on_file\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m cols:\n\u001b[1;32m    412\u001b[0m     \u001b[39m# Since current_examples contains (example, key) tuples\u001b[39;00m\n\u001b[1;32m    413\u001b[0m     batch_examples[col] \u001b[39m=\u001b[39m [row[\u001b[39m0\u001b[39m][col] \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_examples]\n\u001b[0;32m--> 414\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_batch(batch_examples\u001b[39m=\u001b[39;49mbatch_examples)\n\u001b[1;32m    415\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_examples \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/arrow_writer.py:507\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[0;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[1;32m    505\u001b[0m     col_try_type \u001b[39m=\u001b[39m try_features[col] \u001b[39mif\u001b[39;00m try_features \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m col \u001b[39min\u001b[39;00m try_features \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    506\u001b[0m     typed_sequence \u001b[39m=\u001b[39m OptimizedTypedSequence(batch_examples[col], \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mcol_type, try_type\u001b[39m=\u001b[39mcol_try_type, col\u001b[39m=\u001b[39mcol)\n\u001b[0;32m--> 507\u001b[0m     arrays\u001b[39m.\u001b[39mappend(pa\u001b[39m.\u001b[39;49marray(typed_sequence))\n\u001b[1;32m    508\u001b[0m     inferred_features[col] \u001b[39m=\u001b[39m typed_sequence\u001b[39m.\u001b[39mget_inferred_type()\n\u001b[1;32m    509\u001b[0m schema \u001b[39m=\u001b[39m inferred_features\u001b[39m.\u001b[39marrow_schema \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpa_writer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyarrow/array.pxi:230\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyarrow/array.pxi:110\u001b[0m, in \u001b[0;36mpyarrow.lib._handle_arrow_array_protocol\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/arrow_writer.py:202\u001b[0m, in \u001b[0;36mTypedSequence.__arrow_array__\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[39m# otherwise we can finally use the user's type\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m         \u001b[39m# We use cast_array_to_feature to support casting to custom types like Audio and Image\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         \u001b[39m# Also, when trying type \"string\", we don't want to convert integers or floats to \"string\".\u001b[39;00m\n\u001b[1;32m    201\u001b[0m         \u001b[39m# We only do it if trying_type is False - since this is what the user asks for.\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m         out \u001b[39m=\u001b[39m cast_array_to_feature(out, \u001b[39mtype\u001b[39;49m, allow_number_to_str\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrying_type)\n\u001b[1;32m    203\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n\u001b[1;32m    204\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, pa\u001b[39m.\u001b[39mlib\u001b[39m.\u001b[39mArrowInvalid) \u001b[39mas\u001b[39;00m e:  \u001b[39m# handle type errors and overflows\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/table.py:1674\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[0;34m(array, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1672\u001b[0m     \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mchunked_array([func(chunk, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m array\u001b[39m.\u001b[39mchunks])\n\u001b[1;32m   1673\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1674\u001b[0m     \u001b[39mreturn\u001b[39;00m func(array, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/table.py:1787\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[0;34m(array, feature, allow_number_to_str)\u001b[0m\n\u001b[1;32m   1784\u001b[0m \u001b[39melif\u001b[39;00m pa\u001b[39m.\u001b[39mtypes\u001b[39m.\u001b[39mis_list(array\u001b[39m.\u001b[39mtype):\n\u001b[1;32m   1785\u001b[0m     \u001b[39m# feature must be either [subfeature] or Sequence(subfeature)\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(feature, \u001b[39mlist\u001b[39m):\n\u001b[0;32m-> 1787\u001b[0m         \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mListArray\u001b[39m.\u001b[39mfrom_arrays(array\u001b[39m.\u001b[39moffsets, _c(array\u001b[39m.\u001b[39;49mvalues, feature[\u001b[39m0\u001b[39;49m]))\n\u001b[1;32m   1788\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(feature, Sequence):\n\u001b[1;32m   1789\u001b[0m         \u001b[39mif\u001b[39;00m feature\u001b[39m.\u001b[39mlength \u001b[39m>\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/table.py:1674\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[0;34m(array, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1672\u001b[0m     \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mchunked_array([func(chunk, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m array\u001b[39m.\u001b[39mchunks])\n\u001b[1;32m   1673\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1674\u001b[0m     \u001b[39mreturn\u001b[39;00m func(array, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/table.py:1809\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[0;34m(array, feature, allow_number_to_str)\u001b[0m\n\u001b[1;32m   1807\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(feature, (Sequence, \u001b[39mdict\u001b[39m, \u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m   1808\u001b[0m     \u001b[39mreturn\u001b[39;00m array_cast(array, feature(), allow_number_to_str\u001b[39m=\u001b[39mallow_number_to_str)\n\u001b[0;32m-> 1809\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt cast array of type\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00marray\u001b[39m.\u001b[39mtype\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mto\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfeature\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Couldn't cast array of type\nstruct<annotation: list<item: null>, answers: list<item: struct<answer: string, answer_extraction: string, answer_extraction_template: string, answer_extraction_text: string, correct_answer: null, id: string>>, api_service: string, author: string, comment: string, cot: string, cot_trigger: string, cot_trigger_template: string, date: string, fragments_version: string, id: string, instruction: null, model: string, prompt_text: string>\nto\n{'id': Value(dtype='string', id=None), 'fragments_version': Value(dtype='string', id=None), 'instruction': Value(dtype='string', id=None), 'cot_trigger': Value(dtype='string', id=None), 'prompt_text': Value(dtype='string', id=None), 'answers': [{'id': Value(dtype='string', id=None), 'answer_extraction': Value(dtype='string', id=None), 'answer_extraction_text': Value(dtype='string', id=None), 'answer': Value(dtype='string', id=None), 'correct_answer': Value(dtype='bool', id=None)}], 'cot': Value(dtype='string', id=None), 'author': Value(dtype='string', id=None), 'date': Value(dtype='string', id=None), 'api_service': Value(dtype='string', id=None), 'model': Value(dtype='string', id=None), 'comment': Value(dtype='string', id=None), 'annotation': [{'author': Value(dtype='string', id=None), 'date': Value(dtype='string', id=None), 'key': Value(dtype='string', id=None), 'value': Value(dtype='string', id=None)}]}"
     ]
    }
   ],
   "source": [
    "# Generating chains-of-thought and answer extractions\n",
    "worldtree_10.generate(config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above was a fake call to the mock API we now **load a prepared dataset** with real model answers for the purpose of the tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'answer_extraction_template'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/robertpraas/Desktop/ThoughtSource/notebooks/tutorial.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robertpraas/Desktop/ThoughtSource/notebooks/tutorial.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# loading a pre-generated example for the purpose of this tutorial\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/robertpraas/Desktop/ThoughtSource/notebooks/tutorial.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m worldtree_10 \u001b[39m=\u001b[39m Collection\u001b[39m.\u001b[39;49mfrom_json(\u001b[39m\"\u001b[39;49m\u001b[39mworldtree_10.json\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/ThoughtSource/libs/cot/cot/dataloader.py:220\u001b[0m, in \u001b[0;36mCollection.from_json\u001b[0;34m(path_or_json, download_mode, source)\u001b[0m\n\u001b[1;32m    218\u001b[0m         dic \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame\u001b[39m.\u001b[39mfrom_records(content[dataset_name][split])\u001b[39m.\u001b[39mto_dict(\u001b[39m\"\u001b[39m\u001b[39mseries\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    219\u001b[0m         dic \u001b[39m=\u001b[39m {k: \u001b[39mlist\u001b[39m(v) \u001b[39mfor\u001b[39;00m (k, v) \u001b[39min\u001b[39;00m dic\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m--> 220\u001b[0m         dataset_dict[split_name] \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39;49mDataset\u001b[39m.\u001b[39;49mfrom_dict(dic, info\u001b[39m.\u001b[39;49mfeatures, info, split)\n\u001b[1;32m    221\u001b[0m     collection[dataset_name] \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39mDatasetDict(dataset_dict)\n\u001b[1;32m    222\u001b[0m \u001b[39mreturn\u001b[39;00m collection\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py:817\u001b[0m, in \u001b[0;36mDataset.from_dict\u001b[0;34m(cls, mapping, features, info, split)\u001b[0m\n\u001b[1;32m    815\u001b[0m info\u001b[39m.\u001b[39mfeatures \u001b[39m=\u001b[39m features\n\u001b[1;32m    816\u001b[0m \u001b[39mif\u001b[39;00m features \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 817\u001b[0m     mapping \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39;49mencode_batch(mapping)\n\u001b[1;32m    818\u001b[0m mapping \u001b[39m=\u001b[39m {\n\u001b[1;32m    819\u001b[0m     col: OptimizedTypedSequence(data, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mfeatures[col] \u001b[39mif\u001b[39;00m features \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, col\u001b[39m=\u001b[39mcol)\n\u001b[1;32m    820\u001b[0m     \u001b[39mfor\u001b[39;00m col, data \u001b[39min\u001b[39;00m mapping\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    821\u001b[0m }\n\u001b[1;32m    822\u001b[0m pa_table \u001b[39m=\u001b[39m InMemoryTable\u001b[39m.\u001b[39mfrom_pydict(mapping\u001b[39m=\u001b[39mmapping)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/features/features.py:1373\u001b[0m, in \u001b[0;36mFeatures.encode_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[39mfor\u001b[39;00m key, column \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   1372\u001b[0m     column \u001b[39m=\u001b[39m cast_to_python_objects(column)\n\u001b[0;32m-> 1373\u001b[0m     encoded_batch[key] \u001b[39m=\u001b[39m [encode_nested_example(\u001b[39mself\u001b[39m[key], obj) \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m column]\n\u001b[1;32m   1374\u001b[0m \u001b[39mreturn\u001b[39;00m encoded_batch\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/features/features.py:1373\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[39mfor\u001b[39;00m key, column \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   1372\u001b[0m     column \u001b[39m=\u001b[39m cast_to_python_objects(column)\n\u001b[0;32m-> 1373\u001b[0m     encoded_batch[key] \u001b[39m=\u001b[39m [encode_nested_example(\u001b[39mself\u001b[39;49m[key], obj) \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m column]\n\u001b[1;32m   1374\u001b[0m \u001b[39mreturn\u001b[39;00m encoded_batch\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/features/features.py:1017\u001b[0m, in \u001b[0;36mencode_nested_example\u001b[0;34m(schema, obj)\u001b[0m\n\u001b[1;32m   1015\u001b[0m         \u001b[39mif\u001b[39;00m _check_non_null_non_empty_recursive(first_elmt, sub_schema):\n\u001b[1;32m   1016\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1017\u001b[0m     \u001b[39mif\u001b[39;00m encode_nested_example(sub_schema, first_elmt) \u001b[39m!=\u001b[39m first_elmt:\n\u001b[1;32m   1018\u001b[0m         \u001b[39mreturn\u001b[39;00m [encode_nested_example(sub_schema, o) \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m obj]\n\u001b[1;32m   1019\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(obj)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/features/features.py:1007\u001b[0m, in \u001b[0;36mencode_nested_example\u001b[0;34m(schema, obj)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[39m# Nested structures: we allow dict, list/tuples, sequences\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, \u001b[39mdict\u001b[39m):\n\u001b[0;32m-> 1007\u001b[0m     \u001b[39mreturn\u001b[39;00m {k: encode_nested_example(sub_schema, sub_obj) \u001b[39mfor\u001b[39;00m k, (sub_schema, sub_obj) \u001b[39min\u001b[39;00m zip_dict(schema, obj)}\n\u001b[1;32m   1008\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m   1009\u001b[0m     sub_schema \u001b[39m=\u001b[39m schema[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/features/features.py:1007\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[39m# Nested structures: we allow dict, list/tuples, sequences\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, \u001b[39mdict\u001b[39m):\n\u001b[0;32m-> 1007\u001b[0m     \u001b[39mreturn\u001b[39;00m {k: encode_nested_example(sub_schema, sub_obj) \u001b[39mfor\u001b[39;00m k, (sub_schema, sub_obj) \u001b[39min\u001b[39;00m zip_dict(schema, obj)}\n\u001b[1;32m   1008\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m   1009\u001b[0m     sub_schema \u001b[39m=\u001b[39m schema[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/features/features.py:1017\u001b[0m, in \u001b[0;36mencode_nested_example\u001b[0;34m(schema, obj)\u001b[0m\n\u001b[1;32m   1015\u001b[0m         \u001b[39mif\u001b[39;00m _check_non_null_non_empty_recursive(first_elmt, sub_schema):\n\u001b[1;32m   1016\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1017\u001b[0m     \u001b[39mif\u001b[39;00m encode_nested_example(sub_schema, first_elmt) \u001b[39m!=\u001b[39m first_elmt:\n\u001b[1;32m   1018\u001b[0m         \u001b[39mreturn\u001b[39;00m [encode_nested_example(sub_schema, o) \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m obj]\n\u001b[1;32m   1019\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(obj)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/features/features.py:1007\u001b[0m, in \u001b[0;36mencode_nested_example\u001b[0;34m(schema, obj)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[39m# Nested structures: we allow dict, list/tuples, sequences\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, \u001b[39mdict\u001b[39m):\n\u001b[0;32m-> 1007\u001b[0m     \u001b[39mreturn\u001b[39;00m {k: encode_nested_example(sub_schema, sub_obj) \u001b[39mfor\u001b[39;00m k, (sub_schema, sub_obj) \u001b[39min\u001b[39;00m zip_dict(schema, obj)}\n\u001b[1;32m   1008\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m   1009\u001b[0m     sub_schema \u001b[39m=\u001b[39m schema[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/features/features.py:1007\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[39m# Nested structures: we allow dict, list/tuples, sequences\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, \u001b[39mdict\u001b[39m):\n\u001b[0;32m-> 1007\u001b[0m     \u001b[39mreturn\u001b[39;00m {k: encode_nested_example(sub_schema, sub_obj) \u001b[39mfor\u001b[39;00m k, (sub_schema, sub_obj) \u001b[39min\u001b[39;00m zip_dict(schema, obj)}\n\u001b[1;32m   1008\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m   1009\u001b[0m     sub_schema \u001b[39m=\u001b[39m schema[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/utils/py_utils.py:207\u001b[0m, in \u001b[0;36mzip_dict\u001b[0;34m(*dicts)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39m\"\"\"Iterate over items of dictionaries grouped by their keys.\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m unique_values(itertools\u001b[39m.\u001b[39mchain(\u001b[39m*\u001b[39mdicts)):  \u001b[39m# set merge all keys\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     \u001b[39m# Will raise KeyError if the dict don't have the same keys\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m     \u001b[39myield\u001b[39;00m key, \u001b[39mtuple\u001b[39;49m(d[key] \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m dicts)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/utils/py_utils.py:207\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39m\"\"\"Iterate over items of dictionaries grouped by their keys.\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m unique_values(itertools\u001b[39m.\u001b[39mchain(\u001b[39m*\u001b[39mdicts)):  \u001b[39m# set merge all keys\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     \u001b[39m# Will raise KeyError if the dict don't have the same keys\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m     \u001b[39myield\u001b[39;00m key, \u001b[39mtuple\u001b[39m(d[key] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m dicts)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'answer_extraction_template'"
     ]
    }
   ],
   "source": [
    "# loading a pre-generated example for the purpose of this tutorial\n",
    "worldtree_10 = Collection.from_json(\"worldtree_10.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f57e102",
   "metadata": {},
   "source": [
    "### 2.2 Using your own API to create reasoning chains and extract answers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ThoughtSource can connect to external AI service providers such as the [OpenAI API](https://openai.com/api/) or the [Hugging Face Hub](https://huggingface.co/docs/hub/index). Set your token, 'api_service' and 'engine' parameters accordingly.\n",
    "\n",
    "In this tutorial we will use the Hugging Face Hub, which is for free. You can make an account and then copy your token from the Hugging Face settings page.\n",
    "\n",
    "To use the API you need to set the environment variable `HUGGINGFACEHUB_API_TOKEN` to your API token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"<token>\"   # <--- set token (can be found in your Hugging Face settings page)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"<token>\"  # <--- Set token for which API you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration of the input and parameters of the language model \n",
    "config={\n",
    "    # We compare three different prompts for the chain of thought generation:\n",
    "    # \"Answer: Let's think step by step.\" and 'Answer: We should think about this step by step.', and \"Answer: First,\" \n",
    "    \"cot_trigger_keys\": ['kojima-01','kojima-02', 'kojima-03'],\n",
    "\n",
    "    # We use the same answer extraction prompt for all three prompts\n",
    "    # \"Therefore, among A through D, the answer is\"\n",
    "    \"answer_extraction_keys\": ['kojima-A-D'], \n",
    "    \n",
    "    \"api_service\": \"huggingface_hub\", # <--- Select which API you want to use\n",
    "    \"engine\": \"google/flan-t5-xl\", # <--- Select which model you want to use\n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"verbose\": False,\n",
    "    \"warn\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading worldtree...\n"
     ]
    }
   ],
   "source": [
    "# Loading just one sample from the dataset so it runs faster\n",
    "worldtree = Collection([\"worldtree\"], verbose=False)\n",
    "worldtree_1 = worldtree.select(split=\"train\", number_samples=1) # just selecting 1 sample, change that if you want to run on more"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a **two step process:**\n",
    " - The language model first answers a question with a detailed reasoning chain.\n",
    " - The language model then extracts the answer from its own reasoning chain.\n",
    "\n",
    "The code does it automatically at once, but it helpful to have in mind the underlying two step process. </br>\n",
    "Therefore you need two API calls for each example as shown in the warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        You are about to \u001b[1m call an external API \u001b[0m in total 6 times, which \u001b[1m may produce costs \u001b[0m.\n",
      "        API calls for reasoning chain generation: 1 samples  * 1 instructions  * 3 reasoning chain triggers\n",
      "        API calls for answer extraction: n_samples  1 samples  * 1 instructions  * 3 reasoning chain triggers * 1 answer extraction triggers \n",
      "        Do you want to continue? y/n\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# Calling the external API will approximately take 30 seconds for this example\n",
    "worldtree_1.generate(config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your cannot press 'y' because your coding environment is not interactive, set \"warn\" to false in the config to deactivate the warning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f57e102",
   "metadata": {},
   "source": [
    "## 3. Evaluation of model answers and downloading all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating worldtree train...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'worldtree': {'train': {'accuracy': {'text-davinci-003': {'None_kojima-01_kojima-A-D': 0.7,\n",
       "     'None_kojima-02_kojima-A-D': 0.7,\n",
       "     'None_kojima-03_kojima-A-D': 0.8}}}}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worldtree_10.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the file, which now includes the dataset and your generated reasoning chains, extracted answers and evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the file that includes all generated chains of thought and answer extractions a the evaluation results\n",
    "worldtree_10.dump(\"worldtree_10.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you used your own API, use this code to evaluate and save the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(worldtree_1.evaluate())\n",
    "# worldtree_1.dump(\"worldtree_1.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f57e102",
   "metadata": {},
   "source": [
    "## 4. Inspect the model outputs in the Web Tool"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is the link: **[ThoughtSource Annotator](http://thought.samwald.info:3000/)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just **upload your just downloaded 'worldtree_10.json' file** to the web tool.\n",
    "\n",
    "</br>\n",
    "</br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
