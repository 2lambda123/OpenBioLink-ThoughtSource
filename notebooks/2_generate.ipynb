{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b410b12-370e-48f5-9a93-9f7821ed847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cot import Collection\n",
    "from cot.generate import FRAGMENTS\n",
    "from pprint import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f57e102",
   "metadata": {},
   "source": [
    "# Model Chain of Thought (CoT) Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f043028",
   "metadata": {},
   "source": [
    "## Examples of available fragments\n",
    "Three groups of templates for CoT generation:\n",
    "1) Instructions\n",
    "2) CoT-Triggers\n",
    "3) Answer-Extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63ddbbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'qa-01': 'Answer the following question through step-by-step reasoning.',\n",
      " 'qa-02': 'Answer the following question through careful, concise step-by-step '\n",
      "          'reasoning.',\n",
      " 'qa-03': 'Answer the following question through careful, concise step-by-step '\n",
      "          'reasoning. Avoid making up wrong statements. If the question does '\n",
      "          'not make sense or cannot be answered, write \"I cannot answer the '\n",
      "          'question\". If you do not have a good answer, write \"I do not have a '\n",
      "          'good answer\". If you are uncertain, write \"I am uncertain about '\n",
      "          'this\".',\n",
      " 'qa-04': 'Answer the following question through careful, concise step-by-step '\n",
      "          'reasoning. Avoid making up wrong statements. Generate sub-questions '\n",
      "          'that are required to answer the original question, answer them '\n",
      "          'until you can answer the original question. If the question does '\n",
      "          'not make sense or cannot be answered, write \"I cannot answer the '\n",
      "          'question\". If you do not have a good answer, write \"I do not have a '\n",
      "          'good answer\". If you are uncertain, write \"I am uncertain about '\n",
      "          'this\".'}\n"
     ]
    }
   ],
   "source": [
    "pprint(FRAGMENTS[\"instructions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63ddbbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"kojima-01\": \"Answer: Let's think step by step.\",\n",
      "  \"kojima-02\": \"Answer: We should think about this step by step.\",\n",
      "  \"kojima-03\": \"Answer: First,\",\n",
      "  \"kojima-04\": \"Answer: Before we dive into the answer,\",\n",
      "  \"kojima-05\": \"Answer: Proof followed by the answer.\",\n",
      "  \"kojima-06\": \"Answer: Let's think step by step in a realistic way.\",\n",
      "  \"kojima-07\": \"Answer: Let's think step by step using common sense and knowledge.\",\n",
      "  \"kojima-08\": \"Answer: Let's think like a detective step by step.\",\n",
      "  \"kojima-09\": \"Answer: Let's think about this logically.\",\n",
      "  \"kojima-10\": \"Answer: Let's think step by step. First,\",\n",
      "  \"kojima-11\": \"Answer: Let's think\",\n",
      "  \"kojima-12\": \"Answer: Let's solve this problem by splitting it into steps.\",\n",
      "  \"kojima-13\": \"Answer: The answer is after the proof.\",\n",
      "  \"kojima-14\": \"Answer: Let's be realistic and think step by step.\",\n",
      "  \"lievin-01\": \"Answer: Let's derive the differential diagnosis step by step.\",\n",
      "  \"lievin-02\": \"Answer: Let's use step by step inductive reasoning, given the medical nature of the question.\",\n",
      "  \"lievin-03\": \"Answer: Let's differentiate using step by step reasoning like a medical expert.\",\n",
      "  \"lievin-04\": \"Answer: Let's think step by step using deductive reasoning.\",\n",
      "  \"lievin-05\": \"Answer: Let's differentiate using step by step reasoning .\",\n",
      "  \"lievin-06\": \"Answer: Let's think step by step to arrive at one of the options.\",\n",
      "  \"lievin-07\": \"Answer: Let's break the problem into multiple steps.\",\n",
      "  \"lievin-08\": \"Answer: Let's use step by step deductive reasoning, given the medical nature of the question.\",\n",
      "  \"lievin-09\": \"Answer: Let's think step by step like a doctor.\",\n",
      "  \"lievin-10\": \"Answer: Let's think step by step like a medical expert.\",\n",
      "  \"lievin-11\": \"Answer: Let's summarize the facts step by step.\",\n",
      "  \"lievin-12\": \"Answer: Let's think step by step using inductive reasoning.\",\n",
      "  \"lievin-13\": \"Answer: Let's think step by step using deductive reasoning like a medical expert.\",\n",
      "  \"lievin-14\": \"Answer: Let's be concise and think step by step.\",\n",
      "  \"lievin-15\": \"Answer: Let's differentiate using step by step deductive reasoning like a medical expert.\",\n",
      "  \"lievin-16\": \"Answer: Let's argue step by step.\",\n",
      "  \"lievin-17\": \"Answer: Let's think step by step like a clinician.\",\n",
      "  \"lievin-18\": \"Answer: Let's reflect on each answer option step by step.\",\n",
      "  \"lievin-19\": \"Answer: Let's reason and differentiate options step by step like a medical expert.\",\n",
      "  \"lievin-20\": \"Answer: Let's differentiate using step by step inductive reasoning like a medical expert.\",\n",
      "  \"lievin-21\": \"Answer: Let's think step by step given every option equal consideration.\",\n",
      "  \"lievin-22\": \"Answer: Let's think step by step like a scientist.\",\n",
      "  \"lievin-23\": \"Answer: Let's use step by step inductive reasoning.\",\n",
      "  \"lievin-24\": \"Answer: Let's work by elimination step by step.\",\n",
      "  \"lievin-25\": \"Answer: Let's use step by step deductive reasoning.\",\n",
      "  \"lievin-26\": \"Answer: Let's follow a Bayesian step by step approach.\",\n",
      "  \"lievin-27\": \"Answer: Let's reflect on each option from the least likely to the most likely.\",\n",
      "  \"lievin-28\": \"Answer: Let's use step by step Bayesian reasoning, given the medical nature of the question.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(FRAGMENTS[\"cot_triggers\"], sort_keys=True, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63ddbbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kojima-01': 'Therefore, the answer is',\n",
      " 'kojima-02': 'Therefore,',\n",
      " 'kojima-03': 'The answer is',\n",
      " 'kojima-A-C': 'Therefore, among A through C, the answer is',\n",
      " 'kojima-A-D': 'Therefore, among A through D, the answer is',\n",
      " 'kojima-A-E': 'Therefore, among A through E, the answer is',\n",
      " 'kojima-A-F': 'Therefore, among A through F, the answer is',\n",
      " 'kojima-numerals': 'Therefore, the answer (arabic numerals) is',\n",
      " 'kojima-yes-no': 'Therefore, the answer (Yes or No) is'}\n"
     ]
    }
   ],
   "source": [
    "pprint(FRAGMENTS[\"answer_extractions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Chain of Thought examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading worldtree...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "| Name      |   Train | Valid   | Test   |\n",
       "|-----------|---------|---------|--------|\n",
       "| worldtree |     100 | -       | -      |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa', 'medmc_qa', 'open_book_qa', 'pubmed_qa', 'qed', 'strategy_qa', 'svamp']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset (detailed explanation in generate_samples notebook)\n",
    "collection = Collection([\"worldtree\"], verbose=False)\n",
    "worldtree_100_random = collection.select(split=\"train\", number_samples=100, random_samples=True, seed=0)\n",
    "worldtree_100_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea911fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also open an existing json\n",
    "# collection = Collection.from_json(\"worldtree_100_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0444b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    \"idx_range\": \"all\", # Determines which indices the generate_and_extract routine is applied to, Default: \"all\" (All items are used)\n",
    "    \"instruction_keys\": [\"qa-01\"], # Determines which instructions are used from fragments.json, Default: None (no instructions are used)\n",
    "    \"cot_trigger_keys\": [\"kojima-01\"], # Determines which cot triggers are used from fragments.json, Default: [\"kojima-01\"] (only the first trigger is used)\n",
    "    \"answer_extraction_keys\": [\"kojima-A-D\"], # Determines which answer extraction prompts are used from fragments.json, Default: [\"kojima-01\"] (only the first prompt is used)\n",
    "    \"author\" : \"konstantin\", # Name of the person responsible for generation, Default: \"\"\n",
    "    \"api_service\": \"mock_api\", # Name of the API called (\"openai\", \"huggingface_hub\", or a mock for testing: \"mock_api\"), Default: \"huggingface_hub\"\n",
    "    \"engine\": \"\", # Name of the engine used (for \"huggingface_hub\" use for example \"google/flan-t5-xl\"), Default: \"google/flan-t5-xl\"\n",
    "    \"temperature\": 0, # Name of the person responsible for generation, Default: 0\n",
    "    \"max_tokens\": 512, # Maximum length of output generated by the model, Default: 128\n",
    "    \"api_time_interval\": 1.0, # Pause between two api calls in seconds, Default: 1.0\n",
    "    \"verbose\": False, # Determines whether the progress of the generation is printed, Default: True\n",
    "    \"warn\": True, # Determines whether a warnings that external APIs will be called are printed, Default: True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        You are about to \u001b[1m call an external API \u001b[0m in total 200 times, which \u001b[1m may produce costs \u001b[0m.\n",
      "        Number API calls for CoT generation: n_samples 100 * n_instruction_keys 1 * n_cot_trigger_keys 1\n",
      "        Number API calls for answer extraction: n_samples 100 * n_instruction_keys 1 * n_cot_trigger_keys 1 * n_answer_extraction_keys 1\n",
      "        Do you want to continue? y/n\n",
      "        \u001b[1m Note: You are using a mock api. When entering 'y', a test run without API calls is made. \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Generating chains of thought and answer extractions (Debug mode, not calling model over API)\n",
    "worldtree_100_random.generate(name=\"worldtree\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above was a fake call in debug mode. Now loading prepared dataset with real model answers.\n",
    "worldtree_100_random = Collection.from_json(\"worldtree_100_generate.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508ffb0",
   "metadata": {},
   "source": [
    "#### Question, choices and right answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A parent and a child share several characteristics. Both individuals are '\n",
      " 'tall, have curly hair, are good cooks, and have freckles. Which of these '\n",
      " 'characteristics is a learned behavior?')\n",
      "['being tall', 'having curly hair', 'being a good cook', 'having freckles']\n",
      "['being a good cook']\n"
     ]
    }
   ],
   "source": [
    "pprint(worldtree_100_random[\"worldtree\"][\"train\"][0][\"question\"])\n",
    "pprint(worldtree_100_random[\"worldtree\"][\"train\"][0][\"choices\"])\n",
    "pprint(worldtree_100_random[\"worldtree\"][\"train\"][0][\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508ffb0",
   "metadata": {},
   "source": [
    "#### Model generated chain of thought and extracted answer (correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Being a good cook is a learned behavior. The parent and the child share '\n",
      " 'several characteristics. So, the answer is C.')\n",
      "'C.'\n"
     ]
    }
   ],
   "source": [
    "pprint(worldtree_100_random[\"worldtree\"][\"train\"][0][\"generated_cot\"][0][\"cot\"])\n",
    "pprint(worldtree_100_random[\"worldtree\"][\"train\"][0][\"generated_cot\"][0]['answers'][0]['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508ffb0",
   "metadata": {},
   "source": [
    "### Save generated CoTs and extracted answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "172d4d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a88051d60446c7a015fe834f4430a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Saving into collection file, appending model output\n",
    "worldtree_100_random.dump(\"worldtree_100_generate.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a19cb823e24c98ee05a9cfa4a3a579b5d56d4c1a735f2a12456750b95a1e155e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
