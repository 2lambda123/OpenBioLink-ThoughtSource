{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cot import Collection\n",
    "med_qa = Collection.load_thoughtsource_100(names=['med_qa'],load_pregenerated_cots=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "| Name   | Train   | Valid   |   Test |\n",
       "|--------|---------|---------|--------|\n",
       "| med_qa | -       | -       |    100 |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa_open', 'medmc_qa', '_init_', 'mmlu_clinical_knowledge', 'mmlu_college_biology', 'mmlu_college_medicine', 'mmlu_medical_genetics', 'mmlu_professional_medicine', '_init_', 'mmlu_anatomy', 'open_book_qa', 'pubmed_qa', 'qed', 'strategy_qa', 'svamp', 'worldtree']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    \"instruction_keys\": None,\n",
    "    \"cot_trigger_keys\": [\"zhou-01\"],\n",
    "    \"answer_extraction_keys\": 'auto-kojima', \n",
    "    \"author\" : \"thoughtsource\",\n",
    "    \"api_service\": \"openai_chat\",\n",
    "    \"api_time_interval\": 1,\n",
    "    \"engine\": \"gpt-3.5-turbo\", \n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"verbose\": False,\n",
    "    \"warn\": False,\n",
    "}\n",
    "med_qa.generate(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4eca722203420fa2d28f332796ceaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe90e6cf80c4df9b782e4f33a86d797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde4c3a3a82b443ea998d13aca758c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "med_qa.dump(\"med_qa_zhou.json\")\n",
    "med_qa.evaluate()\n",
    "med_qa.dump(\"med_qa_zhou_eval.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a5d52754644c4395e24d4213badbc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'med_qa': {'test': {'accuracy': {'gpt-3.5-turbo': {'None_zhou-01_kojima-A-E': 0.58}}}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_qa.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating med_qa...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c420a6f464545f8ade61162d48c87bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID cc9ca491264f0eb1cc2357d0f2e4cee9 in your message.).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cfd846d808a4ee28a7113da28369830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebaecb3bd1fa4fcdb9d284f6d43caefc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aeaa4500a254d41b86ad1292c8cd00b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b715e84218ca430fb6e429fe6944dc05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'med_qa': {'test': {'accuracy': {'gpt-4': {'None_zhou-01_kojima-A-E': 0.81}}}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\n",
    "    \"instruction_keys\": None,\n",
    "    \"cot_trigger_keys\": [\"zhou-01\"],\n",
    "    \"answer_extraction_keys\": 'auto-kojima', \n",
    "    \"author\" : \"thoughtsource\",\n",
    "    \"api_service\": \"openai_chat\",\n",
    "    \"api_time_interval\": 1,\n",
    "    \"engine\": \"gpt-4\", \n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"verbose\": False,\n",
    "    \"warn\": False,\n",
    "}\n",
    "med_qa.generate(config=config)\n",
    "\n",
    "med_qa.dump(\"med_qa_zhou_gpt4.json\")\n",
    "med_qa.evaluate()\n",
    "med_qa.dump(\"med_qa_zhou_eval_gpt4.json\")\n",
    "\n",
    "med_qa.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_qa_correct = med_qa\n",
    "med_qa_false = Collection.from_json(\"/Users/robertpraas/Desktop/ThoughtSource/notebooks/reflection_messages/med_qa_zhou_eval_gpt4.json\")\n",
    "med_qa_correct.select_generated_cots(answer=True)\n",
    "med_qa_correct = med_qa_correct.filter(lambda x: len(x[\"generated_cot\"])==1)\n",
    "\n",
    "med_qa_false.select_generated_cots(answer=False)\n",
    "med_qa_false = med_qa_false.filter(lambda x: len(x[\"generated_cot\"])==1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "| Name   | Train   | Valid   |   Test |\n",
       "|--------|---------|---------|--------|\n",
       "| med_qa | -       | -       |     19 |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa_open', 'medmc_qa', '_init_', 'mmlu_clinical_knowledge', 'mmlu_college_biology', 'mmlu_college_medicine', 'mmlu_medical_genetics', 'mmlu_professional_medicine', '_init_', 'mmlu_anatomy', 'open_book_qa', 'pubmed_qa', 'qed', 'strategy_qa', 'svamp', 'worldtree']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_qa_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "| Name   | Train   | Valid   |   Test |\n",
       "|--------|---------|---------|--------|\n",
       "| med_qa | -       | -       |     81 |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa_open', 'medmc_qa', '_init_', 'mmlu_clinical_knowledge', 'mmlu_college_biology', 'mmlu_college_medicine', 'mmlu_medical_genetics', 'mmlu_professional_medicine', '_init_', 'mmlu_anatomy', 'open_book_qa', 'pubmed_qa', 'qed', 'strategy_qa', 'svamp', 'worldtree']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_qa_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'cherry', 'banana']\n",
      "['banana', 'apple', 'cherry']\n",
      "['cherry', 'apple', 'banana']\n",
      "['apple', 'banana', 'cherry']\n",
      "['apple', 'banana', 'cherry']\n",
      "['banana', 'apple', 'cherry']\n",
      "['banana', 'apple', 'cherry']\n",
      "['banana', 'cherry', 'apple']\n",
      "['cherry', 'apple', 'banana']\n",
      "['cherry', 'banana', 'apple']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "mylist = [\"apple\", \"banana\", \"cherry\"]\n",
    "\n",
    "for i in range(10):\n",
    "    random.shuffle(mylist)\n",
    "    print(mylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_before = Collection.from_json(\"/Users/robertpraas/Desktop/ThoughtSource/notebooks/internal_documentation/paper_2/filtered_ts_100_gpt-4_None_correct_zhou-01_false.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "| Name           | Train   | Valid   | Test   |\n",
       "|----------------|---------|---------|--------|\n",
       "| commonsense_qa | -       | 7       | -      |\n",
       "| med_qa         | -       | -       | 6      |\n",
       "| medmc_qa       | -       | 8       | -      |\n",
       "| open_book_qa   | -       | -       | 2      |\n",
       "| strategy_qa    | 3       | -       | -      |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa_open', '_init_', 'mmlu_clinical_knowledge', 'mmlu_college_biology', 'mmlu_college_medicine', 'mmlu_medical_genetics', 'mmlu_professional_medicine', '_init_', 'mmlu_anatomy', 'pubmed_qa', 'qed', 'svamp', 'worldtree']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_4_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_before_false = Collection.from_json(\"/Users/robertpraas/Desktop/ThoughtSource/notebooks/internal_documentation/paper_2/filtered_ts_100_gpt-4_None_correct_zhou-01_correct.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/robertpraas/Desktop/ThoughtSource/libs/cot/cot/datasets/thoughtsource/thoughtsource_100.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/robertpraas/Desktop/ThoughtSource/notebooks/reflection_messages/data_helper_notebook.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robertpraas/Desktop/ThoughtSource/notebooks/reflection_messages/data_helper_notebook.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcot\u001b[39;00m \u001b[39mimport\u001b[39;00m Collection\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/robertpraas/Desktop/ThoughtSource/notebooks/reflection_messages/data_helper_notebook.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m medmc \u001b[39m=\u001b[39m Collection\u001b[39m.\u001b[39;49mload_thoughtsource_100(names \u001b[39m=\u001b[39;49m [\u001b[39m'\u001b[39;49m\u001b[39mmedmc_qa\u001b[39;49m\u001b[39m'\u001b[39;49m],load_pregenerated_cots\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robertpraas/Desktop/ThoughtSource/notebooks/reflection_messages/data_helper_notebook.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m openbook \u001b[39m=\u001b[39m Collection\u001b[39m.\u001b[39mload_thoughtsource_100(names \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mopen_book_qa\u001b[39m\u001b[39m'\u001b[39m],load_pregenerated_cots\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robertpraas/Desktop/ThoughtSource/notebooks/reflection_messages/data_helper_notebook.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m worldtree \u001b[39m=\u001b[39m Collection\u001b[39m.\u001b[39mload_thoughtsource_100(names \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mworldtree\u001b[39m\u001b[39m'\u001b[39m],load_pregenerated_cots\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/ThoughtSource/libs/cot/cot/dataloader.py:447\u001b[0m, in \u001b[0;36mCollection.load_thoughtsource_100\u001b[0;34m(names, load_pregenerated_cots)\u001b[0m\n\u001b[1;32m    445\u001b[0m path_to_biodatasets \u001b[39m=\u001b[39m (pathlib\u001b[39m.\u001b[39mPath(\u001b[39m__file__\u001b[39m)\u001b[39m.\u001b[39mparent\u001b[39m.\u001b[39mabsolute() \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdatasets\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mresolve()\n\u001b[1;32m    446\u001b[0m path_to_thoughtsource_100 \u001b[39m=\u001b[39m path_to_biodatasets \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mthoughtsource\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mthoughtsource_100.json\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 447\u001b[0m collection \u001b[39m=\u001b[39m Collection\u001b[39m.\u001b[39;49mfrom_json(\u001b[39mstr\u001b[39;49m(path_to_thoughtsource_100))\n\u001b[1;32m    448\u001b[0m \u001b[39m# drop all names that are not in the list\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m names \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/ThoughtSource/libs/cot/cot/dataloader.py:403\u001b[0m, in \u001b[0;36mCollection.from_json\u001b[0;34m(path_or_json, download_mode, source)\u001b[0m\n\u001b[1;32m    401\u001b[0m                 content \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(infile)\n\u001b[1;32m    402\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    404\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_json, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    405\u001b[0m     content \u001b[39m=\u001b[39m path_or_json\n",
      "File \u001b[0;32m~/Desktop/ThoughtSource/libs/cot/cot/dataloader.py:394\u001b[0m, in \u001b[0;36mCollection.from_json\u001b[0;34m(path_or_json, download_mode, source)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_json, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    393\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 394\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(path_or_json, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m infile:\n\u001b[1;32m    395\u001b[0m             content \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(infile)\n\u001b[1;32m    396\u001b[0m     \u001b[39m# if file is not found and path does not end with .json, try to load it with .json\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/robertpraas/Desktop/ThoughtSource/libs/cot/cot/datasets/thoughtsource/thoughtsource_100.json'"
     ]
    }
   ],
   "source": [
    "from cot import Collection\n",
    "medmc = Collection.load_thoughtsource_100(names = ['medmc_qa'],load_pregenerated_cots=False)\n",
    "openbook = Collection.load_thoughtsource_100(names = ['open_book_qa'],load_pregenerated_cots=False)\n",
    "worldtree = Collection.load_thoughtsource_100(names = ['worldtree'],load_pregenerated_cots=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating medmc_qa...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31afe05b5dec42bd99b785f2717957c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 65dec95714c094c243b02b6eb677f7c7 in your message.).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7d8debf5f8549ccfdb050ea6262c6c76 in your message.).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 76f096a06668ec5a39ea6905a07ecbff in your message.).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e09df2992634c22afac42a790762a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e7a12e0c8b48aaac58b8b7c4ed23f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b80fb3f4a2943cdb9b106e608422878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating open_book_qa...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27861d51a620428cbee4405f80827607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a0c60585d03ab1f548ad6fca2e1055c2 in your message.).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 31e25d1babdf36b404cbd8ef66e2499e in your message.).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8cbf43de865d0e6e7d27bcf9f638b097 in your message.).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ccfa27a62034655ba8a3362a9441b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e68666b016451ba8f5b9521a4c1e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611acf5f6ccd4b05a83d16f1b74c9ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating worldtree...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a051b1615674dfd9fe7802f9cd59286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f2411163a5a479cb121e6bde8dc6bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bfc91852df64c3a991542e82c85cd4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd22b4f5cebd47c0a4fdf1b77f3f779c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config={\n",
    "    \"instruction_keys\": None,\n",
    "    \"cot_trigger_keys\": [\"zhou-01\"],\n",
    "    \"answer_extraction_keys\": 'auto-kojima', \n",
    "    \"author\" : \"thoughtsource\",\n",
    "    \"api_service\": \"openai_chat\",\n",
    "    \"api_time_interval\": 1,\n",
    "    \"engine\": \"gpt-3.5-turbo\", \n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"verbose\": False,\n",
    "    \"warn\": False,\n",
    "}\n",
    "medmc.generate(config=config)\n",
    "medmc.dump(\"medmc_zhou_chatgpt.json\")\n",
    "medmc.evaluate()\n",
    "medmc.dump(\"medmc_zhou_eval_chatgpt.json\")\n",
    "\n",
    "openbook.generate(config=config)\n",
    "openbook.dump(\"openbook_zhou_chatgpt.json\")\n",
    "openbook.evaluate()\n",
    "openbook.dump(\"openbook_zhou_eval_chatgpt.json\")\n",
    "\n",
    "worldtree.generate(config=config)\n",
    "worldtree.dump(\"worldtree_zhou_chatgpt.json\")\n",
    "worldtree.evaluate()\n",
    "worldtree.dump(\"worldtree_zhou_eval_chatgpt.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadcd9f7dbdc4448ab80b177b61a50f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'medmc_qa': {'validation': {'accuracy': {'gpt-3.5-turbo': {'None_zhou-01_kojima-A-D': 0.53}}}}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medmc.evaluate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6a85bf0507437287a62a8c7509ebd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'worldtree': {'test': {'accuracy': {'gpt-3.5-turbo': {'None_zhou-01_kojima-A-D': 0.94}}}}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worldtree.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7950b58f1aef45ef9f5bbdd374f60bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'open_book_qa': {'test': {'accuracy': {'gpt-3.5-turbo': {'None_zhou-01_kojima-A-D': 0.72}}}}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "openbook.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cot import Collection\n",
    "medqa = Collection.from_json(\"med_qa_zhou.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medqa = medqa.select(split=\"test\", number_samples=1, random_samples=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating med_qa...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304d096419854e368f1ee16ab60c4610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e7946cc1dffd625dd570bc6de89f1748 in your message.).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b41d0f2f4fa244d4dc620d9c5a33ad3e in your message.).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 70ab28fd0431a57142da3cc4e65afa30 in your message.).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6a60eefc4f0ff0a18ea49183c0e21698 in your message.).\n"
     ]
    }
   ],
   "source": [
    "config={\n",
    "    \"instruction_keys\": None,\n",
    "    \"cot_trigger_keys\": [\"zhou-refl\"],\n",
    "    \"answer_extraction_keys\": 'auto-kojima', \n",
    "    \"author\" : \"thoughtsource\",\n",
    "    \"api_service\": \"openai_chat\",\n",
    "    \"api_time_interval\": 1,\n",
    "    \"engine\": \"gpt-3.5-turbo\", \n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"verbose\": False,\n",
    "    \"warn\": False,\n",
    "}\n",
    "medqa.generate(config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae4af514bf844deb9b9a9bed7e01fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "medqa.dump(\"med_qa_zhou_refl_chatgpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bccb16110b843a8a55b6539d978677e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'med_qa': {'test': {'accuracy': {'gpt-3.5-turbo': {'None_zhou-01_kojima-A-E': 0.58,\n",
       "     'None_zhou-refl_kojima-A-E': 0.52}}}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medqa.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating med_qa...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7345ed3485948198427b737257ef939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config={\n",
    "    \"instruction_keys\": None,\n",
    "    \"cot_trigger_keys\": [\"zhou-refl\"],\n",
    "    \"answer_extraction_keys\": 'auto-kojima', \n",
    "    \"author\" : \"thoughtsource\",\n",
    "    \"api_service\": \"openai_chat\",\n",
    "    \"api_time_interval\": 1,\n",
    "    \"engine\": \"gpt-4\", \n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"verbose\": False,\n",
    "    \"warn\": False,\n",
    "}\n",
    "medqa.generate(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e97b7a424a43bb8922d521ff6ed208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f32d9eb3abc42578250fbbd33db4bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b53b7d9f26246fbb61af1286e75372c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'med_qa': {'test': {'accuracy': {'gpt-3.5-turbo': {'None_zhou-01_kojima-A-E': 0.58,\n",
       "     'None_zhou-refl_kojima-A-E': 0.52},\n",
       "    'gpt-4': {'None_zhou-refl_kojima-A-E': 0.8}}}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medqa.evaluate()\n",
    "medqa.dump(\"gpt_4_zhou_refl\")\n",
    "medqa.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260b4c8fc1e84637975c51029005a49e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'med_qa': {'test': {'accuracy': {'gpt-4': {'None_zhou-01_kojima-A-E': 0.81}}}}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Collection.from_json(\"med_qa_zhou_gpt4.json\")\n",
    "test.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_qa_false = Collection.from_json(\"/Users/robertpraas/Desktop/ThoughtSource/notebooks/reflection_messages/med_qa_zhou_eval_gpt4.json\")\n",
    "med_qa_false.select_generated_cots(answer=False)\n",
    "med_qa_false = med_qa_false.filter(lambda x: len(x[\"generated_cot\"])==1)\n",
    "med_qa_false.dump(\"medqa_false_gpt_4\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run TS-400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "| Name   | Train   | Valid   |   Test |\n",
       "|--------|---------|---------|--------|\n",
       "| med_qa | -       | -       |    400 |\n",
       "\n",
       "Not loaded: ['aqua', 'asdiv', 'commonsense_qa', 'entailment_bank', 'gsm8k', 'mawps', 'med_qa_open', 'medmc_qa', '_init_', 'mmlu_clinical_knowledge', 'mmlu_college_biology', 'mmlu_college_medicine', 'mmlu_medical_genetics', 'mmlu_professional_medicine', '_init_', 'mmlu_anatomy', 'open_book_qa', 'pubmed_qa', 'qed', 'strategy_qa', 'svamp', 'worldtree']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cot import Collection\n",
    "med_qa = Collection.from_json(\"med_qa_400.json\")\n",
    "\n",
    "med_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    \"instruction_keys\": None,\n",
    "    \"cot_trigger_keys\": [\"zhou-01\"],\n",
    "    \"answer_extraction_keys\": 'auto-kojima', \n",
    "    \"author\" : \"thoughtsource\",\n",
    "    \"api_service\": \"openai_chat\",\n",
    "    \"api_time_interval\": 1,\n",
    "    \"engine\": \"gpt-3.5-turbo\", \n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"verbose\": False,\n",
    "    \"warn\": False,\n",
    "}\n",
    "med_qa.generate(config=config)\n",
    "med_qa.dump(\"med_qa_400_turbo\")\n",
    "med_qa.evaluate()\n",
    "med_qa.dump(\"med_qa_400_turbo_eval\")\n",
    "med_qa.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'med_qa_test_3',\n",
       " 'ref_id': '',\n",
       " 'question': 'A 39-year-old woman is brought to the emergency department because of fevers, chills, and left lower quadrant pain. Her temperature is 39.1°C (102.3°F), pulse is 126/min, respirations are 28/min, and blood pressure is 80/50 mm Hg. There is blood oozing around the site of a peripheral intravenous line. Pelvic examination shows mucopurulent discharge from the cervical os and left adnexal tenderness. Laboratory studies show:\\nPlatelet count 14,200/mm3\\nFibrinogen 83 mg/mL (N = 200–430 mg/dL)\\nD-dimer 965 ng/mL (N < 500 ng/mL)\\nWhen phenol is applied to a sample of the patient\\'s blood at 90°C, a phosphorylated N-acetylglucosamine dimer with 6 fatty acids attached to a polysaccharide side chain is identified. A blood culture is most likely to show which of the following?\"',\n",
       " 'type': 'multiplechoice',\n",
       " 'choices': ['Coagulase-positive, gram-positive cocci forming mauve-colored colonies on methicillin-containing agar',\n",
       "  'Encapsulated, gram-negative coccobacilli forming grey-colored colonies on charcoal blood agar',\n",
       "  'Spore-forming, gram-positive bacilli forming yellow colonies on casein agar',\n",
       "  'Lactose-fermenting, gram-negative rods forming pink colonies on MacConkey agar',\n",
       "  'Gamma-hemolytic, gram-positive cocci forming green colonies on vancomycin agar'],\n",
       " 'context': '',\n",
       " 'cot': [],\n",
       " 'answer': ['Lactose-fermenting, gram-negative rods forming pink colonies on MacConkey agar'],\n",
       " 'generated_cot': [],\n",
       " 'feedback': []}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_qa['med_qa']['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'med_qa_test_3',\n",
       " 'ref_id': '',\n",
       " 'question': 'A 39-year-old woman is brought to the emergency department because of fevers, chills, and left lower quadrant pain. Her temperature is 39.1°C (102.3°F), pulse is 126/min, respirations are 28/min, and blood pressure is 80/50 mm Hg. There is blood oozing around the site of a peripheral intravenous line. Pelvic examination shows mucopurulent discharge from the cervical os and left adnexal tenderness. Laboratory studies show:\\nPlatelet count 14,200/mm3\\nFibrinogen 83 mg/mL (N = 200–430 mg/dL)\\nD-dimer 965 ng/mL (N < 500 ng/mL)\\nWhen phenol is applied to a sample of the patient\\'s blood at 90°C, a phosphorylated N-acetylglucosamine dimer with 6 fatty acids attached to a polysaccharide side chain is identified. A blood culture is most likely to show which of the following?\"',\n",
       " 'type': 'multiplechoice',\n",
       " 'choices': ['Coagulase-positive, gram-positive cocci forming mauve-colored colonies on methicillin-containing agar',\n",
       "  'Encapsulated, gram-negative coccobacilli forming grey-colored colonies on charcoal blood agar',\n",
       "  'Spore-forming, gram-positive bacilli forming yellow colonies on casein agar',\n",
       "  'Lactose-fermenting, gram-negative rods forming pink colonies on MacConkey agar',\n",
       "  'Gamma-hemolytic, gram-positive cocci forming green colonies on vancomycin agar'],\n",
       " 'context': '',\n",
       " 'cot': [],\n",
       " 'answer': ['Lactose-fermenting, gram-negative rods forming pink colonies on MacConkey agar'],\n",
       " 'generated_cot': [],\n",
       " 'feedback': []}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_qa['med_qa']['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating med_qa...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759ec8fccd8c46839e9d4153771543e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(API-)Error in item 0: blocked output: please adjust your prompt and try again, as this generation may be a potential violation of our Usage Guidelines (https://docs.cohere.ai/usage-guidelines/).\n",
      "Retrying with additional time of 10 seconds.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m# generate cohere\u001b[39;00m\n\u001b[1;32m      7\u001b[0m config\u001b[39m=\u001b[39m{\n\u001b[1;32m      8\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minstruction_keys\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcot_trigger_keys\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39mzhou-01\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m }\n\u001b[0;32m---> 20\u001b[0m med_qa\u001b[39m.\u001b[39;49mgenerate(config\u001b[39m=\u001b[39;49mconfig)\n\u001b[1;32m     22\u001b[0m med_qa\u001b[39m.\u001b[39mdump(\u001b[39m\"\u001b[39m\u001b[39mmed_qa_400_zhou_cohere.json\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m med_qa\u001b[39m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/ThoughtSource/libs/cot/cot/dataloader.py:548\u001b[0m, in \u001b[0;36mCollection.generate\u001b[0;34m(self, name, split, config)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGenerating \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    547\u001b[0m         \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache[name]:\n\u001b[0;32m--> 548\u001b[0m             \u001b[39mself\u001b[39m[name][split] \u001b[39m=\u001b[39m generate_and_extract(\u001b[39mself\u001b[39;49m[name][split], config\u001b[39m=\u001b[39;49mconfig)\n\u001b[1;32m    549\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    550\u001b[0m     \u001b[39mif\u001b[39;00m split \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/ThoughtSource/libs/cot/cot/generate.py:63\u001b[0m, in \u001b[0;36mgenerate_and_extract\u001b[0;34m(data, config)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39m# The config is transformed into a dataclass object, where all testing is done\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39m# But it will be transformed back to a dictionary for the function 'map'\u001b[39;00m\n\u001b[1;32m     61\u001b[0m config_as_dataclass \u001b[39m=\u001b[39m Config(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39madaptive_config)\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m data\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m     64\u001b[0m     _generate_and_extract,\n\u001b[1;32m     65\u001b[0m     with_indices\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     66\u001b[0m     fn_kwargs\u001b[39m=\u001b[39;49masdict(config_as_dataclass),\n\u001b[1;32m     67\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m     68\u001b[0m     load_from_cache_file\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     69\u001b[0m )\n",
      "File \u001b[0;32m~/ThoughtSource/.conda/lib/python3.9/site-packages/datasets/arrow_dataset.py:1955\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   1952\u001b[0m disable_tqdm \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[1;32m   1954\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_proc \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 1955\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_single(\n\u001b[1;32m   1956\u001b[0m         function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m   1957\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m   1958\u001b[0m         with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m   1959\u001b[0m         input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m   1960\u001b[0m         batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m   1961\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   1962\u001b[0m         drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m   1963\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m   1964\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m   1965\u001b[0m         load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m   1966\u001b[0m         cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[1;32m   1967\u001b[0m         writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m   1968\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1969\u001b[0m         disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m   1970\u001b[0m         fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m   1971\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[1;32m   1972\u001b[0m         disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm,\n\u001b[1;32m   1973\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m   1974\u001b[0m     )\n\u001b[1;32m   1975\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1977\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
      "File \u001b[0;32m~/ThoughtSource/.conda/lib/python3.9/site-packages/datasets/arrow_dataset.py:520\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    519\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    521\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    522\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    523\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/ThoughtSource/.conda/lib/python3.9/site-packages/datasets/arrow_dataset.py:487\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    481\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    482\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    483\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    484\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    485\u001b[0m }\n\u001b[1;32m    486\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 487\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    488\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    489\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/ThoughtSource/.conda/lib/python3.9/site-packages/datasets/fingerprint.py:458\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m             kwargs[fingerprint_name] \u001b[39m=\u001b[39m update_fingerprint(\n\u001b[1;32m    453\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fingerprint, transform, kwargs_for_fingerprint\n\u001b[1;32m    454\u001b[0m             )\n\u001b[1;32m    456\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    460\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/ThoughtSource/.conda/lib/python3.9/site-packages/datasets/arrow_dataset.py:2320\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   2318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batched:\n\u001b[1;32m   2319\u001b[0m     \u001b[39mfor\u001b[39;00m i, example \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(pbar):\n\u001b[0;32m-> 2320\u001b[0m         example \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(example, i, offset\u001b[39m=\u001b[39;49moffset)\n\u001b[1;32m   2321\u001b[0m         \u001b[39mif\u001b[39;00m update_data:\n\u001b[1;32m   2322\u001b[0m             \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/ThoughtSource/.conda/lib/python3.9/site-packages/datasets/arrow_dataset.py:2220\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   2218\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   2219\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 2220\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   2221\u001b[0m \u001b[39mif\u001b[39;00m update_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2222\u001b[0m     \u001b[39m# Check if the function returns updated examples\u001b[39;00m\n\u001b[1;32m   2223\u001b[0m     update_data \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(processed_inputs, (Mapping, pa\u001b[39m.\u001b[39mTable))\n",
      "File \u001b[0;32m~/ThoughtSource/.conda/lib/python3.9/site-packages/datasets/arrow_dataset.py:1915\u001b[0m, in \u001b[0;36mDataset.map.<locals>.decorate.<locals>.decorated\u001b[0;34m(item, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1911\u001b[0m decorated_item \u001b[39m=\u001b[39m (\n\u001b[1;32m   1912\u001b[0m     Example(item, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batched \u001b[39melse\u001b[39;00m Batch(item, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures)\n\u001b[1;32m   1913\u001b[0m )\n\u001b[1;32m   1914\u001b[0m \u001b[39m# Use the LazyDict internally, while mapping the function\u001b[39;00m\n\u001b[0;32m-> 1915\u001b[0m result \u001b[39m=\u001b[39m f(decorated_item, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1916\u001b[0m \u001b[39m# Return a standard dict\u001b[39;00m\n\u001b[1;32m   1917\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39mdata \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, LazyDict) \u001b[39melse\u001b[39;00m result\n",
      "File \u001b[0;32m~/ThoughtSource/libs/cot/cot/generate.py:162\u001b[0m, in \u001b[0;36m_generate_and_extract\u001b[0;34m(item, idx, idx_range, author, api_service, engine, temperature, max_tokens, api_time_interval, instruction_keys, cot_trigger_keys, template_cot_generation, answer_extraction_keys, template_answer_extraction, warn, verbose)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m-----------------COT TRIGGER TEXT-----------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    160\u001b[0m     \u001b[39mprint\u001b[39m(generate_cot_prompt)\n\u001b[0;32m--> 162\u001b[0m cot \u001b[39m=\u001b[39m query_model(\n\u001b[1;32m    163\u001b[0m     generate_cot_prompt,\n\u001b[1;32m    164\u001b[0m     api_service,\n\u001b[1;32m    165\u001b[0m     engine,\n\u001b[1;32m    166\u001b[0m     temperature,\n\u001b[1;32m    167\u001b[0m     max_tokens,\n\u001b[1;32m    168\u001b[0m     api_time_interval,\n\u001b[1;32m    169\u001b[0m )\n\u001b[1;32m    170\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m    171\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m------------------GENERATED COT-------------------\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/ThoughtSource/libs/cot/cot/generate.py:501\u001b[0m, in \u001b[0;36mquery_model\u001b[0;34m(input, api_service, engine, temperature, max_tokens, api_time_interval)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[39m# return (\"This is a \" + 20 * \"long \" + \"Mock CoT.\\n\")*20\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \n\u001b[1;32m    497\u001b[0m \u001b[39m# langchain package implementation\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    499\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m \u001b[39mimport\u001b[39;00m LLMChain, Prompt\n\u001b[0;32m--> 501\u001b[0m     time\u001b[39m.\u001b[39;49msleep(api_time_interval)\n\u001b[1;32m    502\u001b[0m     template \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m{prompt}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    503\u001b[0m     prompt \u001b[39m=\u001b[39m Prompt(template\u001b[39m=\u001b[39mtemplate, input_variables\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from cot import Collection\n",
    "med_qa = Collection.from_json(\"med_qa_400.json\")\n",
    "#med_qa = med_qa.select(split=\"test\", number_samples=5, random_samples=True, seed=0)\n",
    "\n",
    "med_qa\n",
    "# generate cohere\n",
    "config={\n",
    "    \"instruction_keys\": None,\n",
    "    \"cot_trigger_keys\": [\"zhou-01\"],\n",
    "    \"answer_extraction_keys\": 'auto-kojima', \n",
    "    \"author\" : \"thoughtsource\",\n",
    "    \"api_service\": \"cohere\",\n",
    "    \"api_time_interval\": 1,\n",
    "    \"engine\": \"command-xlarge-nightly\", \n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"verbose\": False,\n",
    "    \"warn\": False,\n",
    "}\n",
    "med_qa.generate(config=config)\n",
    "\n",
    "med_qa.dump(\"med_qa_400_zhou_cohere.json\")\n",
    "med_qa.evaluate()\n",
    "med_qa.dump(\"med_qa_400_zhou_cohere.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cot import Collection\n",
    "med_qa = Collection.from_json(\"med_qa_400.json\")\n",
    "config={\n",
    "    \"instruction_keys\": None,\n",
    "    \"cot_trigger_keys\": [\"zhou-01\"],\n",
    "    \"answer_extraction_keys\": 'auto-kojima', \n",
    "    \"author\" : \"thoughtsource\",\n",
    "    \"api_service\": \"openai_chat\",\n",
    "    \"api_time_interval\": 1,\n",
    "    \"engine\": \"gpt-4\", \n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"verbose\": False,\n",
    "    \"warn\": False,\n",
    "}\n",
    "med_qa.generate(config=config)\n",
    "\n",
    "med_qa.dump(\"med_qa_zhou_gpt4.json\")\n",
    "med_qa.evaluate()\n",
    "med_qa.dump(\"med_qa_zhou_eval_gpt4.json\")\n",
    "\n",
    "med_qa.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if merging CoTs works"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Collection.from_json(\"./med_qa_400_turbo_eval.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'med_qa_test_3',\n",
       " 'ref_id': '',\n",
       " 'question': 'A 39-year-old woman is brought to the emergency department because of fevers, chills, and left lower quadrant pain. Her temperature is 39.1°C (102.3°F), pulse is 126/min, respirations are 28/min, and blood pressure is 80/50 mm Hg. There is blood oozing around the site of a peripheral intravenous line. Pelvic examination shows mucopurulent discharge from the cervical os and left adnexal tenderness. Laboratory studies show:\\nPlatelet count 14,200/mm3\\nFibrinogen 83 mg/mL (N = 200–430 mg/dL)\\nD-dimer 965 ng/mL (N < 500 ng/mL)\\nWhen phenol is applied to a sample of the patient\\'s blood at 90°C, a phosphorylated N-acetylglucosamine dimer with 6 fatty acids attached to a polysaccharide side chain is identified. A blood culture is most likely to show which of the following?\"',\n",
       " 'type': 'multiplechoice',\n",
       " 'choices': ['Coagulase-positive, gram-positive cocci forming mauve-colored colonies on methicillin-containing agar',\n",
       "  'Encapsulated, gram-negative coccobacilli forming grey-colored colonies on charcoal blood agar',\n",
       "  'Spore-forming, gram-positive bacilli forming yellow colonies on casein agar',\n",
       "  'Lactose-fermenting, gram-negative rods forming pink colonies on MacConkey agar',\n",
       "  'Gamma-hemolytic, gram-positive cocci forming green colonies on vancomycin agar'],\n",
       " 'context': '',\n",
       " 'cot': [],\n",
       " 'answer': ['Lactose-fermenting, gram-negative rods forming pink colonies on MacConkey agar'],\n",
       " 'generated_cot': [{'id': '96bd6bab-1492-4492-a25a-74206cbd4b11',\n",
       "   'fragments_version': '0.01',\n",
       "   'instruction': None,\n",
       "   'cot_trigger': 'zhou-01',\n",
       "   'cot_trigger_template': '{instruction}\\n\\n{question}\\n{answer_choices}\\n\\n{cot_trigger}',\n",
       "   'prompt_text': '',\n",
       "   'cot': \"First, we can use the information given about the patient's symptoms and laboratory results to narrow down the possible organisms causing her infection. The presence of mucopurulent discharge and left adnexal tenderness suggests a pelvic inflammatory disease (PID), which can be caused by a variety of bacteria including Neisseria gonorrhoeae and Chlamydia trachomatis. The low platelet count and elevated D-dimer suggest disseminated intravascular coagulation (DIC), which can be caused by sepsis from a variety of organisms.\\n\\nNext, we can use the information about the phenol test to further narrow down the possibilities. This test is used to identify the presence of lipopolysaccharides (LPS) on the surface of gram-negative bacteria. The specific structure identified in this case (a phosphorylated N-acetylglucosamine dimer with 6 fatty acids attached to a polysaccharide side chain) is characteristic of the LPS found in gram-negative bacteria of the Enterobacteriaceae family, such as Escherichia coli and Klebsiella pneumoniae.\\n\\nBased on this information, we can eliminate options A, C, and E, as they describe gram-positive bacteria. Option B describes encapsulated gram-negative coccobacilli, which could potentially cause sepsis and DIC, but it does not fit with the information from the phenol test. Option D describes lactose-fermenting gram-negative rods, which could include E. coli and Klebsiella pneumoniae, and is the most likely answer given the information provided.\\n\\nTherefore, the correct answer is D) Lactose-fermenting, gram-negative rods forming pink colonies on MacConkey agar.\",\n",
       "   'answers': [{'id': 'abf4aa11-4576-47dc-b7ea-a284f0908006',\n",
       "     'answer_extraction': 'kojima-A-E',\n",
       "     'answer_extraction_template': '{instruction}\\n\\n{question}\\n{answer_choices}\\n\\n{cot_trigger}{cot}\\n{answer_extraction}',\n",
       "     'answer_extraction_text': '',\n",
       "     'answer': 'D) Lactose-fermenting, gram-negative rods forming pink colonies on MacConkey agar.',\n",
       "     'answer_from_choices': 'D',\n",
       "     'correct_answer': True}],\n",
       "   'author': 'thoughtsource',\n",
       "   'date': '2023/06/01 21:54:43',\n",
       "   'api_service': 'openai_chat',\n",
       "   'model': \"{'name': 'gpt-3.5-turbo', 'temperature': 0, 'max_tokens': 512}\",\n",
       "   'comment': '',\n",
       "   'annotations': []}],\n",
       " 'feedback': []}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['med_qa']['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
